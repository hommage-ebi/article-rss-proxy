<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Thu, 29 May 2025 03:22:39 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-29</title><link>https://arxiv.org/2025-05-29</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22058v1"&gt;The experimental determination of exchange mass terms in surface states on both terminations of MnBi4Te7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22207v1"&gt;Modulation of Polarization and Metallicity in Janus Sliding Ferroelectrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22462v1"&gt;X-ray View of Light-Induced Spin Reorientation in TmFeO$_{3}$: Direct Observation of a 90$^\circ$ Néel Vector Rotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22488v1"&gt;Raman Optical Activity Induced by Ferroaxial Order in $\textrm{NiTiO}_3$&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22628v1"&gt;Lattice Compatibility and Energy Barriers in Intercalation Compounds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22077v1"&gt;Influence of thickness on magnetic properties of RF-sputtered amorphous CoNbZr thin films&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22183v1"&gt;Anomalous Hall Effect in Thick Co$_3$Sn$_2$S$_2$ Weyl Semimetal Crystals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22198v1"&gt;Solvated electrons in polar liquids as epsilon-near-zero materials tunable in the terahertz frequency range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22324v1"&gt;Indirect Magnetoelectric Coupling via Skew Scattering by Orbital Angular Momentum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22544v1"&gt;Nonlinear time-reversal symmetry breaking in kagome spin ice HoAgGe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21922v1"&gt;Dual-Polarization SHG Interferometry for Imaging Antiparallel Domains and Stacking Angles of 2D Heterocrystals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21924v1"&gt;Modified Transfer Matrix Method for the Extraction of Material Properties via Terahertz Time-Domain Spectroscopy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22559v1"&gt;Observation of Coherent Ferrons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22268v1"&gt;Thermophysical Properties and Phase Behavior of CO2 with Impurities: Insight from Molecular Simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21888v1"&gt;Dissecting Exciton-Polariton Transport in Organic Molecular Crystals: Emerging Conductivity Assisted by Intermolecular Vibrational Coupling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22239v1"&gt;Finite-size effects of the excess entropy computed from integrating the radial distribution function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22623v1"&gt;A theory for diffusion-controlled reactions within nonequilibrium steady-states&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22576v1"&gt;ExoPhoto: A Database of Temperature-Dependent Photodissociation Cross Sections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22215v1"&gt;Solving Boolean Satisfiability Problems Using A Hypergraph-based Probabilistic Computer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22223v1"&gt;Bayesian Learning in Structural Dynamics: A Comprehensive Review and Emerging Trends&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22385v1"&gt;Does the Forced Van der Pol Oscillator Exhibit Irregular Behavior?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22514v1"&gt;Closing the Quantum-Classical Scaling Gap in Approximate Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22386v1"&gt;Ensemble Modeling of the Solar Wind Flow with Boundary Conditions Governed by Synchronic Photospheric Magnetograms. I. Multi-point Validation in the Inner Heliosphere&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21988v1"&gt;Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22087v1"&gt;Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22147v1"&gt;Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22148v1"&gt;What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22244v1"&gt;A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22288v1"&gt;Compression versus Accuracy: A Hierarchy of Lifted Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21893v1"&gt;SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21895v1"&gt;Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21904v1"&gt;CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21908v1"&gt;Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21918v1"&gt;Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21926v1"&gt;Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21954v1"&gt;UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21955v1"&gt;Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21969v1"&gt;DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21996v1"&gt;Learning World Models for Interactive Video Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22003v1"&gt;Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22006v1"&gt;Efficiently Enhancing General Agents With Hierarchical-categorical Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22021v1"&gt;GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22038v1"&gt;Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22042v1"&gt;Estimating the Effects of Sample Training Orders for Large Language Models without Retraining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22050v1"&gt;Reinforced Reasoning for Embodied Planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22074v1"&gt;The Resurrection of the ReLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22086v1"&gt;iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22112v1"&gt;Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22116v1"&gt;Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22128v1"&gt;Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22141v1"&gt;FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22184v1"&gt;Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22199v1"&gt;Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22200v1"&gt;Investigating Mechanisms for In-Context Vision Language Binding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22202v1"&gt;Let's Predict Sentence by Sentence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22224v1"&gt;Solver-Free Decision-Focused Learning for Linear Optimization Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22280v1"&gt;Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22287v1"&gt;New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22291v1"&gt;Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22306v1"&gt;Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22338v1"&gt;Text2Grad: Reinforcement Learning from Natural Language Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22343v1"&gt;Empowering Intelligent Low-altitude Economy with Large AI Model Deployment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22353v1"&gt;VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22358v1"&gt;Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22370v1"&gt;SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22384v1"&gt;Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22389v1"&gt;Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22441v1"&gt;Can NeRFs See without Cameras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22442v1"&gt;SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22445v1"&gt;NFR: Neural Feature-Guided Non-Rigid Shape Registration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22513v1"&gt;Strengthening Proportionality in Temporal Voting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22521v1"&gt;Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22543v1"&gt;Scaling-up Perceptual Video Quality Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22566v1"&gt;Universal Visuo-Tactile Video Understanding for Embodied Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22572v1"&gt;Fusion Steering: Prompt-Specific Activation Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22581v1"&gt;Tell me Habibi, is it Real or Fake?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22601v1"&gt;Machine Unlearning under Overparameterization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22635v1"&gt;Learning Composable Chains-of-Thought&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22660v1"&gt;Maximizing Confidence Alone Improves Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21887v1"&gt;SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21906v1"&gt;Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21923v1"&gt;FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21928v1"&gt;Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21938v1"&gt;Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21956v1"&gt;Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21966v1"&gt;MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21985v1"&gt;Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22027v1"&gt;Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22029v1"&gt;Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22067v1"&gt;From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22093v1"&gt;From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22106v1"&gt;AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22108v1"&gt;Inclusive, Differentially Private Federated Learning for Clinical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22125v1"&gt;Sentiment Simulation using Generative AI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22137v1"&gt;Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22165v1"&gt;Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22174v1"&gt;Online Fair Division for Personalized $2$-Value Instances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22179v1"&gt;Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22203v1"&gt;Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22232v1"&gt;Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22271v1"&gt;Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22290v1"&gt;Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22303v1"&gt;Voice CMS: updating the knowledge base of a digital assistant through conversation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22310v1"&gt;From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22311v1"&gt;From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22312v1"&gt;Skywork Open Reasoner 1 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22334v1"&gt;Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22356v1"&gt;Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22387v1"&gt;DAM: Domain-Aware Module for Multi-Domain Dataset Condensation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22411v1"&gt;Mitigating Overthinking in Large Reasoning Models via Manifold Steering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22425v1"&gt;Scaling Reasoning without Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22453v1"&gt;Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22457v1"&gt;Fostering Video Reasoning via Next-Event Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22467v1"&gt;Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22477v1"&gt;Human-Centered Human-AI Collaboration (HCHAC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22483v1"&gt;A Closer Look at Multimodal Representation Collapse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22491v1"&gt;On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22492v1"&gt;Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22503v1"&gt;From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22525v1"&gt;Thinking with Generated Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22531v1"&gt;Training RL Agents for Multi-Objective Network Defense Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22533v1"&gt;TabularQGAN: A Quantum Generative Model for Tabular Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22552v1"&gt;ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22564v1"&gt;PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22591v1"&gt;Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22597v1"&gt;HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22602v1"&gt;One Rank at a Time: Cascading Error Dynamics in Sequential Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22608v1"&gt;Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22613v1"&gt;RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22626v1"&gt;SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22642v1"&gt;FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22649v1"&gt;Pre-training for Recommendation Unlearning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22657v1"&gt;3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22193v1"&gt;Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22633v1"&gt;Spatial Knowledge Graph-Guided Multimodal Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22104v1"&gt;Efficient Dynamic Shielding for Parametric Safety Specifications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22438v1"&gt;Synonymous Variational Inference for Perceptual Image Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21944v1"&gt;Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21978v1"&gt;Two-Stage Feature Generation with Transformer and Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21987v1"&gt;ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22014v1"&gt;Learning in Compact Spaces with Approximately Normalized Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22028v1"&gt;Weakly-Supervised Contrastive Learning for Imprecise Class Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22041v1"&gt;Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22049v1"&gt;Differentiable Generalized Sliced Wasserstein Plans&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22081v1"&gt;Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22114v1"&gt;BiMi Sheets: Infosheets for bias mitigation methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22151v1"&gt;Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22158v1"&gt;The informativeness of the gradient revisited&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22196v1"&gt;An Augmentation-Aware Theory for Self-Supervised Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22235v1"&gt;Optimal kernel regression bounds under energy-bounded noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22254v1"&gt;A Unified Online-Offline Framework for Co-Branding Campaign Recommendations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22308v1"&gt;Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22316v1"&gt;Rethinking BPS: A Utility-Based Evaluation Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22322v1"&gt;A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22355v1"&gt;Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22359v1"&gt;Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22362v1"&gt;Directed Homophily-Aware Graph Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22381v1"&gt;A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22422v1"&gt;STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22440v1"&gt;Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22450v1"&gt;Position: All Current Generative Fidelity and Diversity Metrics are Flawed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22473v1"&gt;Pure Exploration with Infinite Answers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22474v1"&gt;Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22475v1"&gt;Non-Asymptotic Analysis of (Sticky) Track-and-Stop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22494v1"&gt;ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22504v1"&gt;Geometric GNNs for Charged Particle Tracking at GlueX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22506v1"&gt;Sparsification and Reconstruction from the Perspective of Representation Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22524v1"&gt;Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22549v1"&gt;DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22573v1"&gt;FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22578v1"&gt;Benignity of loss landscape with weight decay requires both large overparametrization and initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22637v1"&gt;Understanding (Un)Reliability of Steering Vectors in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22641v1"&gt;Spectral Survival Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22650v1"&gt;On Learning Verifiers for Chain-of-Thought Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21892v1"&gt;Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21910v1"&gt;Taming Transformer Without Using Learning Rate Warmup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21930v1"&gt;Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21942v1"&gt;Continual Learning Beyond Experience Rehearsal and Full Model Surrogates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21959v1"&gt;EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22008v1"&gt;Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22048v1"&gt;Learning Curves of Stochastic Gradient Descent in Kernel Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22090v1"&gt;High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22094v1"&gt;ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22099v1"&gt;On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22107v1"&gt;Curse of High Dimensionality Issue in Transformer for Long-context Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22135v1"&gt;RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22152v1"&gt;Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22238v1"&gt;Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22243v1"&gt;UDuo: Universal Dual Optimization Framework for Online Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22252v1"&gt;B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22255v1"&gt;Train Sparse Autoencoders Efficiently by Utilizing Features Correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22257v1"&gt;Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22275v1"&gt;Full Domain Analysis in Fluid Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22296v1"&gt;360-LLaMA-Factory: Plug &amp; Play Sequence Parallelism for Long Post-Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22318v1"&gt;If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22326v1"&gt;Individualised Counterfactual Examples Using Conformal Prediction Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22332v1"&gt;Credal Prediction based on Relative Likelihood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22342v1"&gt;Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22361v1"&gt;Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22364v1"&gt;Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22454v1"&gt;Depth-Based Matrix Classification for the HHL Quantum Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22481v1"&gt;Hypothesis Testing in Imaging Inverse Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22486v1"&gt;Understanding Adversarial Training with Energy-based Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22502v1"&gt;Assessing Quantum Advantage for Gaussian Process Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22509v1"&gt;Accelerating Optimization via Differentiable Stopping Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22518v1"&gt;IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22527v1"&gt;Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22535v1"&gt;RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22538v1"&gt;Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22541v1"&gt;A Human-Centric Approach to Explainable AI for Personalized Education&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22554v1"&gt;Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22638v1"&gt;SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22662v1"&gt;AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21925v1"&gt;RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22258v1"&gt;LiDAR Based Semantic Perception for Forklifts in Outdoor Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22609v1"&gt;Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22622v1"&gt;Principled Out-of-Distribution Generalization via Simplicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22651v1"&gt;Sherlock: Self-Correcting Reasoning in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21932v1"&gt;Higher-Order Group Synchronization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21889v1"&gt;EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21936v1"&gt;RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21937v1"&gt;Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21940v1"&gt;RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21941v1"&gt;Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21967v1"&gt;Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21979v1"&gt;Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21997v1"&gt;Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21999v1"&gt;Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22017v1"&gt;CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22018v1"&gt;Improving Continual Pre-training Through Seamless Data Packing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22061v1"&gt;Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22095v1"&gt;Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22101v1"&gt;MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22113v1"&gt;THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22118v1"&gt;Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22120v1"&gt;LoKI: Low-damage Knowledge Implanting of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22131v1"&gt;EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22156v1"&gt;InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22157v1"&gt;Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22169v1"&gt;ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22176v1"&gt;TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22236v1"&gt;A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22273v1"&gt;Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22293v1"&gt;Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22298v1"&gt;Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22354v1"&gt;LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22548v1"&gt;Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22582v1"&gt;Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22586v1"&gt;Precise In-Parameter Concept Erasure in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22618v1"&gt;Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22630v1"&gt;Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22653v1"&gt;The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22661v1"&gt;GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22150v1"&gt;Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22222v1"&gt;Look &amp; Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22251v1"&gt;Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22323v1"&gt;Advancing Expert Specialization for Better MoE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22327v1"&gt;NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22517v1"&gt;Multi-MLLM Knowledge Distillation for Out-of-Context News Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22563v1"&gt;Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22627v1"&gt;Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22645v1"&gt;Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22037v1"&gt;Jailbreak Distillation: Renewable Safety Benchmarking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22054v1"&gt;Voice Adaptation for Swiss German&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22088v1"&gt;Visual Cues Support Robust Turn-taking Prediction in Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22231v1"&gt;Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22487v1"&gt;Effective Context in Neural Speech Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-29</guid><pubDate>Thu, 29 May 2025 12:16:40 +0900</pubDate></item><item><title>UI-Evol: Automatic Knowledge Evolving for Computer Use Agents</title><link>http://arxiv.org/abs/2505.21964v1</link><description>外部知識は、最近のコンピュータ利用エージェントの開発において重要な役割を果たしてきました。私たちは、重要な知識-実行ギャップを特定しました。それは、検索された知識が効果的な現実世界のタスク実行に結びつかないことが多いということです。私たちの分析では、90％の正答率の知識であっても、実行成功率はわずか41％であることが示されています。このギャップを埋めるために、自律的なGUI知識進化のためのプラグアンドプレイモジュールであるUI-Evolを提案します。UI-Evolは、実際のエージェントと環境の相互作用から忠実な客観的行動シーケンスを抽出するRetrace Stageと、これらのシーケンスを外部参照と比較して既存の知識を洗練するCritique Stageの2つの段階で構成されています。最先端のエージェントS2を用いて、OSWorldベンチマークで包括的な実験を行いました。その結果、UI-Evolはタスクパフォーマンスを大幅に向上させるだけでなく、コンピュータ利用エージェントにおける行動標準偏差が高いという、これまで見過ごされてきた問題にも対処し、コンピュータ利用タスクにおける優れたパフォーマンスと、エージェントの信頼性を大幅に向上させることが実証されました。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21964v1</guid><pubDate>Wed, 28 May 2025 04:32:05 +0000</pubDate></item><item><title>WebDancer: Towards Autonomous Information Seeking Agency</title><link>http://arxiv.org/abs/2505.22648v1</link><description>複雑な現実世界の問題に対処するには、詳細な情報収集と多段階の推論が必要です。Deep Researchに代表されるエージェントシステムの最近の進歩は、自律的な多段階研究の可能性を強調しています。本研究では、データ中心かつトレーニング段階の視点から、エンドツーエンドのエージェント型情報収集エージェントを構築するための包括的なパラダイムを提示します。我々のアプローチは、(1)閲覧データ構築、(2)軌跡サンプリング、(3)効果的なコールドスタートのための教師ありファインチューニング、(4)汎化性能向上のための強化学習という4つの主要な段階で構成されています。このフレームワークを、ReActに基づくWebエージェントであるWebDancerに実装しました。GAIAおよびWebWalkerQAという困難な情報収集ベンチマークでの実証的評価により、WebDancerの優れた性能が実証され、かなりの成果を達成し、我々のトレーニングパラダイムの有効性が強調されました。エージェントトレーニングのさらなる分析は、より有能なエージェントモデルを開発するための貴重な洞察と、実行可能で体系的な経路を提供します。コードとデモは、https://github.com/Alibaba-NLP/WebAgent で公開されます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22648v1</guid><pubDate>Wed, 28 May 2025 17:57:07 +0000</pubDate></item><item><title>EvolveSearch: An Iterative Self-Evolving Search Agent</title><link>http://arxiv.org/abs/2505.22501v1</link><description>大規模言語モデル（LLM）の急速な進歩は、検索エンジンやウェブブラウザなどのツールとの統合を通じて、エージェント的な情報探索能力の状況を一変させました。しかし、LLMのウェブ検索能力を向上させるための現在の主流なアプローチは、重大な課題に直面しています。教師ありファインチューニングは、オープン検索ドメインでのデータ生成に苦労し、強化学習（RL）はすぐに収束し、データ利用効率が制限されます。これらの問題に対処するために、我々はEvolveSearchという、SFTとRLを組み合わせた新しい反復的な自己進化フレームワークを提案します。これは、外部の人手によるアノテーション付き推論データなしに、エージェント的なウェブ検索能力を向上させます。7つのマルチホップ質問応答（MHQA）ベンチマークに関する広範な実験は、EvolveSearchが反復処理を通じて一貫してパフォーマンスを向上させ、最終的に7つのベンチマーク全体で現在の最先端技術を平均4.7％上回る改善を達成し、オープンウェブ検索ドメインにおける自己進化エージェント能力への扉を開くことを示しています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22501v1</guid><pubDate>Wed, 28 May 2025 15:50:48 +0000</pubDate></item><item><title>RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning</title><link>http://arxiv.org/abs/2505.22430v1</link><description>信頼できる検索拡張生成（RAG）システムを導入するには、堅牢な評価が不可欠です。しかし、現在のLLMベースの評価フレームワークは、主にリソースを大量に消費するモデルに複雑な多段階プロンプトを直接与えることに依存しており、モデルの推論能力を活用しきれておらず、計算コストが大幅に増大しています。本論文では、忠実性と正確性の評価をルールに基づいた推論タスクとして定式化する、新しいエンドツーエンドのフレームワークであるRAG-Zeval（RAG-Zero Evaluator）を提案します。我々のアプローチは、強化学習によって評価者を訓練し、コンパクトなモデルが詳細な説明とともに包括的で健全な評価をワンパスで生成できるようにします。正確な点ごとの報酬信号を得るという課題に対処するために、絶対的なスコアではなく、選好判断を使用するランキングベースの結果報酬メカニズムを導入します。この目的のために、人間のアノテーションなしで品質管理された応答を生成することにより、ランキング参照を合成します。実験により、RAG-Zevalの優れた性能が実証され、人間の判断との最も強い相関関係を達成し、10〜100倍以上のパラメータを持つLLMに依存するベースラインを上回っています。我々のアプローチは、応答評価において優れた解釈可能性も示しています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22430v1</guid><pubDate>Wed, 28 May 2025 14:55:33 +0000</pubDate></item><item><title>Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition</title><link>http://arxiv.org/abs/2505.22375v1</link><description>本研究では、柔軟な高速・低速思考能力を備え、Ascendニューラルプロセッシングユニット（NPU）上で開発された効率的な大規模言語モデル（LLM）推論エンジンであるPangu Embeddedを紹介します。Pangu Embeddedは、既存の推論に最適化されたLLMに蔓延する、大きな計算コストと推論レイテンシの課題に対処します。その構築のために、2段階のトレーニングフレームワークを提案します。第1段階では、モデルは反復蒸留プロセスを通じてファインチューニングされ、反復間のモデルマージを組み込むことで、補完的な知識を効果的に集約します。これに続いて、Ascendクラスタ上での強化学習が行われ、遅延耐性のあるスケジューラによって最適化されます。このスケジューラは、古い同期並列処理と優先順位付けされたデータキューを組み合わせます。RLプロセスは、Multi-source Adaptive Reward System（MARS）によって誘導されます。MARSは、数学、コーディング、および一般的な問題解決タスクのために、決定論的メトリクスと軽量LLM評価器を使用して、動的なタスク固有の報酬信号を生成します。第2段階では、デュアルシステムフレームワークを導入し、Pangu Embeddedに、ルーチンクエリのための「高速」モードと、複雑な推論のためのより深い「低速」モードを付与します。このフレームワークは、ユーザー制御のための手動モード切り替えと、レイテンシと推論の深さのバランスを取るために計算リソースを動的に割り当てる、複雑さを認識した自動モード選択メカニズムの両方を提供します。AIME 2024、GPQA、LiveCodeBenchなどのベンチマークでの実験結果は、7BパラメータのPangu Embeddedが、Qwen3-8BやGLM4-9Bなどの同様のサイズのモデルを上回ることを示しています。単一の統一されたモデルアーキテクチャ内で、迅速な応答と最先端の推論品質を提供し、強力でありながら実用的に展開可能なLLM推論エンジンの開発に向けた有望な方向性を示しています。

&lt;img src="https://arxiv.org/html/2505.22375v1/x1.png"/&gt;&lt;p&gt;Huawei pangutech@huawei.com, Pangu Team&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22375v1</guid><pubDate>Wed, 28 May 2025 14:03:02 +0000</pubDate></item><item><title>BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain</title><link>http://arxiv.org/abs/2505.22240v1</link><description>生物医学的推論では、しばしば、薬剤、疾患、タンパク質などのエンティティ間の相互接続された関係を辿る必要があります。大規模言語モデル（LLM）の重要性が高まっているにもかかわらず、既存のベンチマークは、生物医学領域におけるマルチホップ推論、特に一対多および多対多の関係を含むクエリを評価する能力を欠いています。このギャップは、生物医学的マルチホップ推論の重要な課題を未開拓のままにしています。これに対処するため、構造化された生物医学的知識グラフにおけるマルチホップ、マルチアンサー推論を評価するために設計された新しいベンチマークであるBioHopRを紹介します。包括的なPrimeKGから構築されたBioHopRには、現実世界の生物医学的複雑さを反映した1ホップおよび2ホップの推論タスクが含まれています。最先端モデルの評価では、推論に特化した独自のモデルであるO3-miniが、1ホップタスクで37.93％、2ホップタスクで14.57％の精度を達成し、GPT4Oなどの独自のモデルや、HuatuoGPT-o1-70BやLlama-3.3-70Bなどのオープンソースの生物医学モデルを上回っています。しかし、すべてのモデルでマルチホップのパフォーマンスが大幅に低下しており、生物医学領域における暗黙的な推論ステップを解決することの難しさを浮き彫りにしています。生物医学領域におけるマルチホップ推論のベンチマークの欠如に対処することで、BioHopRは推論能力を評価するための新しい基準を確立し、独自のモデルとオープンソースモデルの間の重要なギャップを強調すると同時に、生物医学LLMにおける将来の進歩への道を開きます。

&lt;img src="https://arxiv.org/html/2505.22240v1/x1.png"/&gt;&lt;p&gt;UCL King’s College London University of Glasgow&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22240v1</guid><pubDate>Wed, 28 May 2025 11:19:01 +0000</pubDate></item><item><title>Reverse Preference Optimization for Complex Instruction Following</title><link>http://arxiv.org/abs/2505.22172v1</link><description>指示追従（IF）は、大規模言語モデル（LLM）にとって重要な能力です。しかし、複数の制約を持つ複雑な指示を処理することは依然として困難です。従来の手法では、通常、満たす制約の数に基づいて好みのペアを選択するため、選択された例が一部の制約に従わない場合や、拒否された例が選択された例よりも特定の点で優れている場合など、ノイズが発生します。複数の好みに合わせるという課題に対処するため、Reverse Preference Optimization（RPO）と呼ばれるシンプルながら効果的な手法を提案します。これは、指示内の制約を動的に反転させることで、選択された応答が完璧であることを保証し、好みのペアのノイズを軽減し、完璧な応答を収集するための広範なサンプリングとフィルタリングの負担を軽減します。さらに、反転は選択された応答と拒否された応答の間のギャップを拡大し、最適化の方向を明確にし、ノイズに対する耐性を高めます。RPOを2つのマルチターンIFベンチマークであるSysbenchとMulti-IFで評価し、DPOベースラインと比較して、それぞれ平均4.6ポイントと2.5ポイントの改善（Llama-3.1 8Bの場合）を示しました。さらに、RPOはモデルサイズ（8Bから70Bパラメータ）にわたって効果的にスケールし、70B RPOモデルはGPT-4oを上回りました。

&lt;img src="https://arxiv.org/html/2505.22172v1/x1.png"/&gt;&lt;p&gt;China, Feiteng Fang, Nanjing University, Ting-En Lin, Tongyi Lab State Key Laboratory for Novel Software Technology, Xiang Huang, Yuchuan Wu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22172v1</guid><pubDate>Wed, 28 May 2025 09:44:27 +0000</pubDate></item><item><title>ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation</title><link>http://arxiv.org/abs/2505.22076v1</link><description>大規模言語モデル（LLM）を指示に従うように訓練することで、未知のタスクに取り組む能力が大幅に向上しました。しかし、強力な汎化能力にもかかわらず、指示に従うLLMは、ドメイン知識を必要とするタスクを扱う際に困難に直面します。本研究では、計算論証（CA）の分野に特化した指示ファインチューニングを紹介します。その目的は、LLMが汎化能力を維持しながら、あらゆる未知のCAタスクに効果的に取り組めるようにすることです。既存のCA研究をレビューし、この目的のために105個のCAタスクに対する自然言語指示を作成しました。これに基づいて、LLMが様々なCAタスクを解決する能力を包括的に評価できる、CA特有のLLMベンチマークを開発しました。自己指示プロセスを適応させ、CAに特化した指示に従うLLMを訓練するために、52,000件のCA関連指示を合成しました。実験の結果、CAに特化した指示ファインチューニングは、既知および未知のCAタスクの両方においてLLMを大幅に強化することが示唆されました。同時に、SuperNIベンチマークの一般的なNLPタスクにおけるパフォーマンスは安定しています。

&lt;img src="https://arxiv.org/html/2505.22076v1/x1.png"/&gt;&lt;p&gt;Maja Stahl Leibniz University Hannover &amp;Timon Ziegenbein Leibniz University Hannover Joonsuk Park University of Richmond &amp;Henning Wachsmuth Leibniz University Hannover&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22076v1</guid><pubDate>Wed, 28 May 2025 07:58:29 +0000</pubDate></item><item><title>Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning</title><link>http://arxiv.org/abs/2505.21958v1</link><description>ドメイン特化型のインストラクションチューニングは、医療質問応答など、特定のアプリケーションにおける大規模言語モデル（LLM）の性能を向上させるための事実上の標準となっています。インストラクションチューニングのデータセットには冗長なデータや低品質なデータが含まれている可能性があるため、データ効率を最大化するためにデータ選択（DS）が通常必要となります。一般的なドメインでの成功にもかかわらず、現在のDS手法は、ドメイン特化型のインストラクションチューニングに必要なデータをうまく選択できないことがよくあります。主な理由の一つは、知識の衝突、つまりLLMの事前学習された知識とインストラクションデータのコンテキスト知識との間の不一致の影響を無視していることです。これはLLMの既存の能力を損ない、ハルシネーションにつながる可能性があります。そこで、LLMの実際のニーズを満たすドメイン特化型のインストラクションチューニングデータを選択するための、シンプルかつ効果的な知識認識型データ選択（KDS）フレームワークを提案します。KDSの中核は、コンテキスト-メモリ知識のアライメントとメモリ内知識の一貫性という2つの側面から知識の衝突を定量的に測定するための、2つの知識認識型メトリックを活用することです。知識の衝突が大きいデータをフィルタリングし、高品質で多様なデータをサンプリングすることで、KDSはLLMの能力を効果的に刺激し、より優れたドメイン特化型のパフォーマンスを達成できます。医療ドメインをテストベッドとして、広範な実験を行い、KDSが他のベースラインを上回り、すべてのLLM間で有意かつ一貫したパフォーマンス向上をもたらすことを経験的に証明します。さらに、KDSはモデルの汎化性能を効果的に向上させ、ハルシネーションの問題を軽減します。

&lt;img src="https://arxiv.org/html/2505.21958v1/x1.png"/&gt;&lt;p&gt;Bo Du, Fei Liao, Juhua Liu, Liang Ding, Qihuang Zhong, and Dacheng Tao&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21958v1</guid><pubDate>Wed, 28 May 2025 04:18:24 +0000</pubDate></item><item><title>PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning</title><link>http://arxiv.org/abs/2505.22085v1</link><description>ルパート-ポリアク平均や指数移動平均（EMA）などの平均化手法は、広く普及しているADAMオプティマイザのような確率的勾配降下法（SGD）の最適化手順を加速させる強力なアプローチです。しかし、考慮する特定の最適化問題によっては、最小の最適化誤差を達成するために、平均化の種類とパラメータを調整する必要があります。本研究では、並列平均化ADAM（PADAM）と呼ぶ平均化手法を提案します。これは、ADAMの異なる平均化されたバリアントを並列に計算し、トレーニングプロセス中に最小の最適化誤差を持つバリアントを動的に選択します。このアプローチの中心的な特徴は、各平均化された軌跡が同じ基礎となるADAM軌跡、つまり同じ基礎となる勾配に依存するため、この手順が通常のADAMオプティマイザよりも多くの勾配評価を必要としないことです。提案されたPADAMオプティマイザを13の確率的最適化および深層ニューラルネットワーク（DNN）学習問題でテストし、標準的なSGD、モーメンタムSGD、EMAの有無にかかわらずAdam、ADAMWなど、文献で知られているオプティマイザとのパフォーマンスを比較します。特に、科学的機械学習からの境界値偏微分方程式問題に対する物理情報ニューラルネットワーク、深層ガラーキン、深層後退確率微分方程式、深層コルモゴロフ近似、および最適制御および最適停止問題に対するDNN近似に、比較対象のオプティマイザを適用します。ほとんどすべての考慮された例において、PADAMは、時には他のものと並んで、時には排他的に、本質的に最小の最適化誤差を達成します。したがって、本研究は、科学的機械学習問題に対してPADAMを検討することを強く示唆し、DNNのトレーニングにおける適応的平均化手順に関するさらなる研究を促進します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22085v1</guid><pubDate>Wed, 28 May 2025 08:07:34 +0000</pubDate></item><item><title>Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations</title><link>http://arxiv.org/abs/2505.21994v1</link><description>発散不安定性のため、ほぼ非圧縮性の均質弾性方程式に対する低次の適合有限要素法は、ラメ定数λ→∞、または等価的にポアソン比ν→1/2となるにつれて精度が低下します。この現象は、ロッキングまたは非ロバスト性として知られており、広範な調査にもかかわらず、完全には解明されていません。本論文では、根本的に異なる機械学習駆動型のアプローチに基づくロバストな手法を提案します。近年開発された物理情報ニューラルネットワーク（PINNs）を活用し、ほぼ非圧縮性材料を支配する線形弾性方程式の数値解法に取り組みます。本手法の核となるアイデアは、与えられた方程式を適切に分解して係数の極端な不均衡を緩和し、同時に順問題と逆問題を解くことで、分解された系の解と関連する外部条件を復元することです。定数、変数、およびパラメータ化されたラメ定数を含む様々な数値実験を通して、提案された方法論の効率を示します。

&lt;img src="https://arxiv.org/html/2505.21994v1/extracted/6487500/Images/lamb10.png"/&gt;&lt;p&gt;Josef Dick, Kassem Mustapha and
 Sanghyeon Park, Seungchan Ko&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21994v1</guid><pubDate>Wed, 28 May 2025 05:52:03 +0000</pubDate></item><item><title>CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs</title><link>http://arxiv.org/abs/2505.22469v1</link><description>現代のマルチプロセッサシステムオンチップ（MPSoC）における効率的な熱および電力管理には、正確な電力消費量推定が不可欠です。最先端のアプローチの一つであるAlternative Blind Power Identification（ABPI）は、理論的には定常状態温度への依存性を排除し、従来のアプローチの大きな欠点に対処します。しかし、ABPIの性能は、実際のハードウェア実装では検証されていませんでした。本研究では、NVIDIA Jetson Xavier AGXプラットフォームを使用した商用ハードウェア上で、ABPIの最初の経験的検証を行います。その結果、ABPIは計算効率が高く、定常状態温度に依存しないものの、現実のシナリオではかなりの精度不足を示すことが明らかになりました。これらの制限を克服するために、Custom Physics-Informed Neural Networks（CPINN）をABPIの基礎となる熱モデルと統合する新しいアプローチを導入します。本アプローチでは、物理原理とデータ駆動型学習を調和させる特殊な損失関数を採用し、推定精度と計算コストのバランスを取るために多目的遺伝的アルゴリズム最適化によって補完します。実験的検証では、CPINN-ABPIは、ABPIと比較して、平均絶対誤差（MAE）でCPUを84.7％、GPUを73.9％削減し、加重平均絶対パーセント誤差（WMAPE）は47％〜81％から〜12％に改善されます。この方法は、195.3μsの推論時間でリアルタイム性能を維持し、異種SoC全体で同様の85％〜99％の精度向上を実現します。

&lt;img src="https://arxiv.org/html/2505.22469v1/extracted/6489827/Images/EXPOverFlow.png"/&gt;&lt;p&gt;Abdel-Hameed A. Badawy1, Ahmad Patooghy2, Mehdi Elahi2, Mohamed R. Elshamy1&lt;/p&gt;&lt;p&gt;1Klipsch School of ECE, New Mexico State University, Las Cruces, NM 88003, United States
2Computer Systems Technology, North Carolina A&amp;T State University, Greensboro, NC, United States&lt;/p&gt;</description><guid isPermaLink="false">2505.22469v1</guid><pubDate>Wed, 28 May 2025 15:22:15 +0000</pubDate></item><item><title>Geometric Hyena Networks for Large-scale Equivariant Learning</title><link>http://arxiv.org/abs/2505.22560v1</link><description>生物学的、化学的、物理的システムをモデル化する際、同変性を維持しながらグローバルな幾何学的コンテキストを処理することは非常に重要です。しかし、同変性とグローバルなコンテキストを大規模に扱うには計算コストがかかるため、これは困難です。同変自己注意などの標準的な手法は二次関数的な複雑さを抱えており、距離ベースのメッセージパッシングなどの局所的な手法はグローバルな情報を犠牲にします。近年成功を収めている状態空間モデルと長畳み込みモデルに触発され、幾何学的システム向けの最初の同変長畳み込みモデルであるGeometric Hyenaを導入します。Geometric Hyenaは、回転と並進に対する同変性を維持しながら、二次関数未満の複雑さでグローバルな幾何学的コンテキストを捉えます。大規模なRNA分子の全原子特性予測と完全なタンパク質分子動力学で評価した結果、Geometric Hyenaは既存の同変モデルよりも優れた性能を発揮し、同変自己注意よりも大幅に少ないメモリと計算量で済みます。特に、私たちのモデルは3万トークンの幾何学的コンテキストを同変トランスフォーマーよりも20倍高速に処理し、同じ予算内で72倍長いコンテキストを扱うことができます。

&lt;img src="https://arxiv.org/html/2505.22560v1/x1.png"/&gt;&lt;p&gt;Artem Moskalev, Junjie Xu, Mangal Prakash, Rui Liao, Tianyu Cui, Tommaso Mansi&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22560v1</guid><pubDate>Wed, 28 May 2025 16:38:35 +0000</pubDate></item><item><title>LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models</title><link>http://arxiv.org/abs/2505.22208v1</link><description>ニューラルネットワークポテンシャル（NNP）は、密度汎関数理論（DFT）計算の代替として、計算材料科学を加速するために不可欠です。その精度向上は、事前学習とファインチューニングによって可能になります。ここでは、NNPモデルはまず大規模なデータセットで事前学習され、次に小規模なターゲットデータセットでファインチューニングされます。しかし、このアプローチは、主にDFTベースのデータセットラベリングのコストと、大規模な事前学習中の負荷分散の不均衡のために、計算コストが高くなります。これに対処するため、我々はLaMMを提案します。これは、改善されたノイズ除去自己教師あり学習と、効率的なマルチノードトレーニングのための負荷分散アルゴリズムを組み込んだ半教師あり事前学習法です。我々は、我々のアプローチが、約3億の半ラベル付きサンプルからなる大規模なデータセットを効果的に活用して、単一のNNPモデルをトレーニングし、速度と精度の両方の点で、ファインチューニングのパフォーマンスを向上させることを示します。

&lt;img src="https://arxiv.org/html/2505.22208v1/x2.png"/&gt;&lt;p&gt;Eiji Ohta, Japan {oyama.yosuke, Kanagawa 211–8588, Yasufumi Sakai Kawasaki, Yosuke Oyama, Yusuke Majima, eiji, majima.yusuke, sakaiyasufumi}@fujitsu.com&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22208v1</guid><pubDate>Wed, 28 May 2025 10:36:49 +0000</pubDate></item><item><title>BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</title><link>http://arxiv.org/abs/2505.21974v1</link><description>ベイズ最適化（BO）は、ガウス過程事前分布と獲得関数（AF）の助けを借りて、ブラックボックス関数を効率的に最適化するためのパイプラインを提供します。近年、単目的BOの文脈において、学習ベースのAFは、その有利な非近視眼的な性質から、有望な経験的結果を示しています。しかし、これらのアプローチを多目的ベイズ最適化（MOBO）に直接拡張すると、MOBO問題の非マルコフ性から生じる\textit{ハイパーボリューム識別可能性の問題}に悩まされます。これに対処するため、非マルコフ強化学習の文献と、言語モデリングにおけるTransformerの成功に触発され、一般化された深層Q学習フレームワークを提示し、シーケンスモデリングを通じてMOBOのためにこのフレームワークを具体化する\textit{BOFormer}を提案します。広範な評価を通じて、BOFormerが、様々な合成MOBOおよび現実世界の多目的ハイパーパラメータ最適化問題において、ベンチマークのルールベースおよび学習ベースのアルゴリズムを一貫して上回ることを示します。この方向へのさらなる研究を奨励するために、ソースコードを公開しました。

&lt;img src="https://arxiv.org/html/2505.21974v1/x1.png"/&gt;&lt;p&gt;Hsinchu, National Yang Ming Chiao Tung University, Taiwan NVIDIA Research&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21974v1</guid><pubDate>Wed, 28 May 2025 05:00:50 +0000</pubDate></item><item><title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title><link>http://arxiv.org/abs/2505.22391v1</link><description>物理システムを生成的にモデル化することには、部分的な観測を扱える、多様な解を生成できる、順問題と逆問題の両方に対処できるなど、いくつかの利点があります。近年、拡散モデルは、特に偏微分方程式（PDE）によって支配される物理システムのモデリングにおいて、ますます注目を集めています。しかし、拡散モデルは中間ステップでのノイズの多いデータ$\boldsymbol{x}_t$にしかアクセスできないため、各ノイズレベルでクリーンなサンプル$\boldsymbol{x}_0$に直接制約を課すことは実現不可能です。その代替として、制約は通常、学習されたスコアネットワークを用いて推定される、クリーンなサンプルの期待値$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$に適用されます。しかし、期待値にPDE制約を課すことは、真のクリーンなデータに対する制約を厳密に表すものではなく、これはイェンセンのギャップとして知られています。このギャップはトレードオフを生み出します。PDE制約を課すことは、生成モデリングの精度低下を招く可能性があります。これに対処するため、我々はシンプルながら効果的な事後蒸留アプローチを提案します。このアプローチでは、PDE制約を拡散プロセスに直接注入するのではなく、事後蒸留段階で適用します。我々は、この手法を拡散モデルの物理情報蒸留（Physics-Informed Distillation of Diffusion Models: PIDDM）と名付けます。この蒸留は、PDEの満足度を向上させたシングルステップ生成を容易にするだけでなく、順問題と逆問題の解決、およびランダムな部分観測からの再構成もサポートします。様々なPDEベンチマークにわたる広範な実験により、PIDDMは、PIDM、DiffusionPDE、ECI-samplingなどの最近の競合するベースラインと比較して、計算オーバーヘッドを抑えつつ、PDEの満足度を大幅に向上させることが示されています。我々のアプローチは、物理的制約を拡散モデルに組み込むための、より効率的かつ効果的な戦略に光を当てる可能性があります。

&lt;img src="https://arxiv.org/html/2505.22391v1/extracted/6489499/figures/method_new.png"/&gt;&lt;p&gt;Data Science The University of Hong Kong, Yi Zhang Institute of Data Science The University of Hong Kong &amp;Difan Zou Institute of Data Science &amp; School of Computing&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22391v1</guid><pubDate>Wed, 28 May 2025 14:17:58 +0000</pubDate></item><item><title>Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents</title><link>http://arxiv.org/abs/2505.22655v1</link><description>大規模言語モデル（LLM）とチャットボットエージェントは、時に誤った出力をすることが知られており、最近、これを完全に防ぐことは不可能であることが判明しました。したがって、不確実性定量化は、全体的な数値、または偶然的不確実性と認識論的不確実性のための2つの数値のいずれかで、曖昧さのレベルを定量化することを目的として、重要な役割を果たします。本ポジションペーパーでは、この従来の不確実性の二分法は、LLMエージェントがユーザーとコミュニケーションをとるオープンでインタラクティブな設定には限界があり、この新しいシナリオで不確実性を豊かにする道を研究する必要があると主張します。文献をレビューした結果、偶然的不確実性と認識論的不確実性の一般的な定義は互いに直接矛盾し、インタラクティブなLLMエージェントの設定では意味を失うことがわかりました。そこで、人間とコンピュータのインタラクションにおける不確実性に焦点を当てた3つの新しい研究方向を提案します。それは、ユーザーがすべての情報を提供しない場合や、最初の段階で正確なタスクを定義しない場合の未特定化の不確実性、フォローアップの質問をして現在のコンテキストに関する不確実性を減らすためのインタラクティブな学習、そして、単なる数値以上のものとして不確実性を表現するために、豊富な言語と音声空間を活用する出力の不確実性です。これらの新しい不確実性の扱い方と伝え方が、より透明性、信頼性、直感的なLLMエージェントとのインタラクションにつながると期待しています。

&lt;img src="https://arxiv.org/html/2505.22655v1/x1.png"/&gt;&lt;p&gt;Enkelejda Kasneci, Gjergji Kasneci, Michael Kirchhof&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22655v1</guid><pubDate>Wed, 28 May 2025 17:59:08 +0000</pubDate></item><item><title>The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models</title><link>http://arxiv.org/abs/2505.22617v1</link><description>本論文は、LLMを用いた推論のための強化学習（RL）のスケーリングにおける主要な障害、すなわち方策エントロピーの崩壊を克服することを目的としています。この現象は、エントロピー介入なしの膨大なRL実行において一貫して観察され、方策エントロピーは初期の訓練段階で急激に低下し、この探索能力の低下は常に方策性能の飽和を伴います。実際には、エントロピーHと下流の性能Rの間に変換方程式R=-a*e^H+bを確立します。この経験則は、方策性能が方策エントロピーと引き換えに得られ、その枯渇によってボトルネックとなり、上限は完全に予測可能（H=0、R=-a+b）であることを強く示唆しています。我々の発見は、RLのための計算量スケーリングに向けて継続的な探索のためのエントロピー管理を必要とします。この目的のために、エントロピーのダイナミクスを理論的および経験的に調査します。我々の導出は、方策エントロピーの変化が、行動確率とロジットの変化の共分散によって駆動されることを強調しており、これは方策勾配のようなアルゴリズムを使用する場合、そのアドバンテージに比例します。経験的研究は、共分散項の値とエントロピーの差が正確に一致し、理論的な結論を裏付けていることを示しています。さらに、共分散項は訓練を通してほとんどの場合正のままであり、方策エントロピーが単調に減少する理由をさらに説明しています。エントロピーダイナミクスの背後にあるメカニズムを理解することで、共分散の高いトークンの更新を制限することによってエントロピーを制御することを動機づけます。具体的には、Clip-CovとKL-Covという、共分散の高いトークンをそれぞれクリップしたり、KLペナルティを適用したりする、シンプルながら効果的な2つの手法を提案します。実験の結果、これらの手法が探索を促進し、方策がエントロピー崩壊から脱出し、より良い下流の性能を達成するのに役立つことが示されています。

&lt;img src="https://arxiv.org/html/2505.22617v1/x1.png"/&gt;&lt;p&gt;Ganqu Cui, Haozhan Li, Jiacheng Chen, Lifan Yuan, Shanghai AI Laboratory Tsinghua University UIUC Peking University Nanjing University CUHK https://github.com/PRIME-RL/Entropy-Mechanism-of-RL, Yuchen Zhang, Zhi Wang&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22617v1</guid><pubDate>Wed, 28 May 2025 17:38:45 +0000</pubDate></item><item><title>Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems</title><link>http://arxiv.org/abs/2505.22571v1</link><description>本論文では、最近登場した大規模言語モデル（LLM）エージェントの概念を用いた、統一された検索拡張生成（RAG）システムのための新しいアプローチを提案します。具体的には、LLMを基本的なコントローラーとして利用するAgent LLMは、RAGタスクの解釈可能性、特に複雑な推論質問応答システム（例：マルチホップクエリ）において有望なアプローチとなっています。しかしながら、従来の研究は、RAGシステムをシングルホップまたはマルチホップのアプローチで個別に解決することに主眼を置いており、それらのアプローチの現実世界への応用を制限しています。本研究では、RAGシステムの有効性と解釈可能性を向上させる、統一された検索拡張LLMシステムのための、Agent-UniRAGと呼ばれる学習可能なエージェントフレームワークを提案します。主なアイデアは、入力の複雑さに応じてRAGタスクを段階的に解決するために、LLMエージェントフレームワークを設計し、シングルホップとマルチホップのクエリをエンドツーエンドで同時に含めることです。さらに、提案するエージェントフレームワークを小規模なオープンソースLLM（例：Llama-3-8B）で利用できるようにするために、合成データセットSynAgent-RAGを導入します。結果は、様々なRAGベンチマークにおいて、クローズドソースおよび大規模なオープンソースLLMと同等の性能を示すことを示しています。私たちのソースコードとデータセットは、さらなる活用のために公開されています。

&lt;img src="https://arxiv.org/html/2505.22571v1/x1.png"/&gt;&lt;p&gt;Data Services Center, Hoang Pham, Khac-Hoai Nam Bui Viettel Artificial Intelligence, Vietnam {hoangpv4, Viettel Group, nambkh}@viettel.com.vn&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22571v1</guid><pubDate>Wed, 28 May 2025 16:46:31 +0000</pubDate></item><item><title>ChatPD: An LLM-driven Paper-Dataset Networking System</title><link>http://arxiv.org/abs/2505.22349v1</link><description>科学研究は、手法の検証に適したデータセットに大きく依存しているが、PapersWithCodeのようなデータセット管理機能を持つ既存の学術プラットフォームは、手作業によるワークフローの非効率性に悩まされている。このボトルネックを克服するために、我々は大規模言語モデル（LLM）を活用して学術論文からデータセット情報を自動的に抽出し、構造化された論文-データセットネットワークを構築するシステム、ChatPDを提案する。我々のシステムは、論文-データセットネットワークを構築するために、\textit{論文収集}、\textit{データセット情報抽出}、\textit{データセットエンティティ解決}という3つの主要なモジュールで構成されている。特に、データセットの説明を対応するエンティティにマッピングするために、\textit{グラフ補完と推論}戦略を提案する。広範な実験を通して、ChatPDがデータセットの使用状況抽出において既存のプラットフォームであるPapersWithCodeを上回るだけでなく、エンティティ解決タスクにおいて約90％の精度と再現率を達成することを示す。さらに、ChatPDを継続的に論文で使用されているデータセットを抽出し、タスク固有のデータセットクエリや類似データセットの推奨などのデータセット発見サービスを提供するように展開した。ChatPDと現在の論文-データセットネットワークをこの[GitHubリポジトリ]{https://github.com/ChatPD-web/ChatPD}でオープンソースとして公開する。

&lt;img src="https://arxiv.org/html/2505.22349v1/extracted/6489085/pic/ChatPD.png"/&gt;&lt;p&gt;Anjie Xu, Leye Wang, Ruiqing Ding&lt;/p&gt;&lt;p&gt;Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education Beijing China
Key Laboratory of Process Optimization and Intelligent Decision-making, Ministry of Education Anhui China
School of Computer Science, Peking University Beijing China
School of Management, Hefei University of Technology Anhui China&lt;/p&gt;</description><guid isPermaLink="false">2505.22349v1</guid><pubDate>Wed, 28 May 2025 13:31:08 +0000</pubDate></item><item><title>MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps</title><link>http://arxiv.org/abs/2505.22264v1</link><description>本稿では、\textit{SemEval 2025 Task 8: 表形式データに対する質問応答}チャレンジを解決するための我々のアプローチを紹介する。我々の戦略は、LLMを用いたPythonコード生成を活用し、テーブルと対話して質問への答えを得るものである。このプロセスは、テーブルの内容の理解、答えを得るために従うべきステップを自然言語の指示として生成、これらの指示をコードに翻訳、実行、そして潜在的なエラーや例外の処理という複数のステップで構成される。これらのステップでは、オープンソースのLLMと、各タスク（ステップ）のために最適化されたきめ細かいプロンプトを使用する。このアプローチにより、サブタスク1で$70.50\%$のスコアを達成した。

&lt;img src="https://arxiv.org/html/2505.22264v1/extracted/6485935/img/question.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;† Fundación Centro Tecnolóxico de Telecomunicacións de Galicia (GRADIANT), Vigo, Spain&lt;/p&gt;</description><guid isPermaLink="false">2505.22264v1</guid><pubDate>Wed, 28 May 2025 11:50:22 +0000</pubDate></item><item><title>Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language</title><link>http://arxiv.org/abs/2505.22146v1</link><description>柔軟な道具の選択は、人間を他の種と区別する複雑な認知能力を反映しているが、この能力を捉える計算モデルはまだ十分に開発されていない。我々は、視覚的な道具の知覚と言語的なタスク理解を結びつけるために、低次元の属性表現を用いたフレームワークを開発した。物理的、機能的、心理的な特性に及ぶ13個の慎重に設計された属性でラベル付けされた115個の一般的な道具を含む包括的なデータセット（ToolNet）を構築し、道具の使用法を説明する自然言語シナリオと組み合わせた。視覚エンコーダ（ResNetまたはViT）は道具の画像から属性を抽出し、ファインチューニングされた言語モデル（GPT-2、LLaMA、DeepSeek）はタスクの説明から必要な属性を導き出す。我々のアプローチは、道具選択タスクにおいて74%の精度を達成し、直接的な道具のマッチング（20%）や、より小規模なマルチモーダルモデル（21%-58%）を大幅に上回り、はるかに大規模なモデルであるGPT-4o（73%）の性能に、大幅に少ないパラメータで迫る。アブレーション研究により、操作に関連する属性（把持性、手との関連性、伸長性）が、一貫してすべてのモダリティにおいて最も重要であることが明らかになった。本研究は、人間のような道具認知を模倣する、パラメータ効率が高く、解釈可能なソリューションを提供し、認知科学の理解と、道具選択タスクにおける実用的な応用を促進する。

&lt;img src="https://arxiv.org/html/2505.22146v1/x1.png"/&gt;&lt;p&gt;Beijing Normal University IDG/McGovern Institute for Brain Research, Beijing Normal University School of Future Technology, Brain-inspired Intelligence, Institute of Automation Chinese Academy of Sciences (CASIA) School of Artificial Intelligence, Laboratory of Brain Atlas, Learning, University of Chinese Academy of Sciences (UCAS), University of Chinese Academy of Sciences (UCAS) State Key Laboratory of Cognitive Neuroscience&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22146v1</guid><pubDate>Wed, 28 May 2025 09:06:04 +0000</pubDate></item><item><title>Knowledge Base Construction for Knowledge-Augmented Text-to-SQL</title><link>http://arxiv.org/abs/2505.22096v1</link><description>Text-to-SQLは、自然言語のクエリをSQLステートメントに変換することを目的としており、誰でも簡単にデータベースから必要な情報を取得できるため実用的です。近年、既存のアプローチの多くは、大規模言語モデル（LLM）を活用してこの問題に取り組んでおり、LLMのユーザーのクエリを理解し、対応するSQLコードを生成する強力な能力を利用しています。しかし、LLMのパラメータ知識は、多様でドメイン固有のクエリをすべて網羅するには限界があり、さまざまなデータベーススキーマに根ざす必要があり、生成されたSQLの精度がしばしば低下します。これに対処するために、Text-to-SQLのための知識ベースを構築することを提案します。これは知識の基礎的なソースであり、そこから与えられたクエリに必要な知識を検索および生成します。特に、既存のアプローチが手動で知識をアノテーションしたり、各クエリに対して少数の知識しか生成しないのとは異なり、私たちの知識ベースは包括的であり、利用可能なすべての質問とそれに関連するデータベーススキーマ、およびそれらの関連知識の組み合わせに基づいて構築されており、異なるデータセットやドメインからの未知のデータベースに対して再利用できます。複数のText-to-SQLデータセットで私たちのアプローチを検証し、重複するデータベースと重複しないデータベースの両方のシナリオを考慮した結果、関連するベースラインを大幅に上回りました。

&lt;img src="https://arxiv.org/html/2505.22096v1/x3.png"/&gt;&lt;p&gt;KAIST IBM Research&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22096v1</guid><pubDate>Wed, 28 May 2025 08:17:58 +0000</pubDate></item><item><title>Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO</title><link>http://arxiv.org/abs/2505.22068v1</link><description>先行研究は、検証可能な報酬を用いた強化学習（RLVR）で訓練された強力な大規模言語モデル（LLM）は、数学タスクにおいて推論能力を向上させることなく、推論経路を洗練するだけであるのに対し、蒸留を用いた教師ありファインチューニング（SFT）はそれが可能であることを示唆している。我々はこれを、LLMおよび推論LLMが小規模なBertベースのモデルよりも性能が低い科学情報抽出（SciIE）の観点から研究する。SciIEは推論と記憶の両方を必要とする。我々は、SFTとRLVRの両方が、SciIEに基づいて、単純な方法で推論経路を洗練し、推論能力を向上させることができると主張する。我々は、1. MimicSFT（高品質なchain-of-thoughtデータを必要としない構造化された推論テンプレートを使用）、2. 関連性とルール誘導報酬を用いたR$^2$GRPOという2段階の訓練を提案する。科学IEベンチマークでの実験は、両方の方法が推論能力を向上させることができることを示している。mimicSFTを用いたR$^2$GRPOは、関係抽出においてベースラインのLLMおよび特化した教師ありモデルを凌駕する。我々のコードはhttps://github.com/ranlislz/R2GRPOで入手可能である。

&lt;img src="https://arxiv.org/html/2505.22068v1/x1.png"/&gt;&lt;p&gt;China, China &amp;Chen Jing Zhipu AI Beijing, China &amp;Lei Chen HKUST(GZ), China &amp;Shimin Di SEU Jiangsu, China &amp;Yu Qiu Zhipu AI Beijing, China &amp;Yuchen Liu HKUST(GZ) Guangzhou, HKUST Guangzhou, Ran Li HKUST Hong Kong SAR&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22068v1</guid><pubDate>Wed, 28 May 2025 07:47:46 +0000</pubDate></item><item><title>VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning</title><link>http://arxiv.org/abs/2505.22019v1</link><description>視覚的に豊かな情報を効果的に検索、推論、理解することは、RAG（Retrieval-Augmented Generation）手法にとって依然として課題です。従来のテキストベースの手法では、視覚関連情報を扱うことができません。一方、現在のビジョンベースのRAGアプローチは、固定されたパイプラインによって制限され、モデルの基本的な能力の十分な活性化が不足しているため、効果的な推論に苦労することがよくあります。RL（強化学習）がモデルの推論に有益であることが証明されているため、視覚的に豊かな情報にわたる複雑な推論に特化した新しいRLフレームワークであるVRAG-RLを導入します。このフレームワークにより、VLM（Vision-Language Model）は検索エンジンと対話し、視覚認識トークンの助けを借りて、単一ターンまたは複数ターンの推論軌跡を自律的にサンプリングし、これらのサンプルに基づいて継続的な最適化を行います。我々のアプローチは、RAGドメインにおけるRLの主要な制限事項を強調しています。（i）従来のマルチモーダルRAGアプローチは、画像をコンテキストに組み込む傾向があるだけで、推論トークンの割り当てが不十分であり、視覚固有の認識を無視しています。（ii）モデルが検索エンジンと対話する際、要件を明確に表現できないため、クエリが関連情報を取得できず、最適なパフォーマンスが得られないことがよくあります。これらの課題に対処するために、視覚的に豊かな入力に合わせたアクション空間を定義し、アクションにはトリミングやスケーリングを含め、モデルが粗い視点から細かい視点へと情報を収集できるようにします。さらに、ユーザーの元の問い合わせとリトリーバーの間のギャップを埋めるために、クエリの書き換えと検索パフォーマンスをモデルベースの報酬と統合した、シンプルながら効果的な報酬を採用しています。我々のVRAG-RLは、特別に設計されたRL戦略を使用してRAGタスクのためにVLMを最適化し、モデルを実際のアプリケーションに適合させます。コードは\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}で入手できます。

&lt;img src="https://arxiv.org/html/2505.22019v1/x1.png"/&gt;&lt;p&gt;Alibaba Group, Cognition, Lin Chen MoE Key Laboratory of Brain-inspired Intelligent Perception, Qiuchen Wang, Ruixue Ding, USTC Tongyi Lab, Yu Zeng, Zehui Chen&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22019v1</guid><pubDate>Wed, 28 May 2025 06:30:51 +0000</pubDate></item><item><title>Learning Compositional Behaviors from Demonstration and Language</title><link>http://arxiv.org/abs/2505.21981v1</link><description>模倣学習とモデルベースプランニングを統合することで、長期間のロボット操作を実現するフレームワーク、Behavior from Language and Demonstration (BLADE) を紹介します。BLADEは、言語アノテーション付きのデモンストレーションを活用し、大規模言語モデル（LLM）から抽象的な行動知識を抽出し、構造化された高レベル行動表現のライブラリを構築します。これらの表現には、各高レベル行動に対する視覚認識に基づいた前提条件と効果、およびニューラルネットワークベースのポリシーとして実装された対応するコントローラーが含まれます。BLADEは、手動でラベル付けされた状態や記号的な定義なしに、このような構造化された表現を自動的に復元できます。BLADEは、新しい初期状態、外部からの状態摂動、新しい目標など、新しい状況への汎化において優れた能力を示します。関節のある部品、部分的な可観測性、幾何学的制約を持つ多様なオブジェクトを用いて、シミュレーションと実際のロボットの両方で、我々のアプローチの有効性を検証します。

&lt;img src="https://arxiv.org/html/2505.21981v1/x1.png"/&gt;&lt;p&gt;Jiajun Wu, Jiayuan Mao, Neil Nie, Ruohan Zhang, Weiyu Liu&lt;/p&gt;&lt;p&gt;
Stanford University&lt;/p&gt;</description><guid isPermaLink="false">2505.21981v1</guid><pubDate>Wed, 28 May 2025 05:19:59 +0000</pubDate></item><item><title>Judging LLMs on a Simplex</title><link>http://arxiv.org/abs/2505.21972v1</link><description>大規模言語モデル（LLM）からの自由形式の出力の自動評価は、多くの異なる回答が同様に有効であるため、困難です。一般的な手法は、LLM自体を審査員として使用することですが、このアプローチの理論的特性はまだ十分に理解されていません。審査員と候補者の両方を確率シンプレックス上の点として表現する幾何学的フレームワークが、LLM審査員を使用して何が識別可能で何が識別可能でないかについて役立つ洞察を提供できることを示します。私たちの理論的分析は、ランキングの識別可能性における「相転移」を明らかにします。二値スコアリングシステムの場合、真のランキングは、穏やかな仮定の下では弱い審査員でも識別可能ですが、3つ以上のスコアリングレベルの場合、追加の事前知識がない限り、無限のデータがあってもランキングは識別不可能になります。この識別不可能性は、ランキングの不確実性が、偶然的不確実性（つまり、データに固有の確率性）だけでなく、どの仮定が成り立つかに関する認識論的不確実性にも起因することを示しており、この側面はこれまでほとんど注目されていませんでした。両方のタイプの不確実性を統合するために、ベイズ推論を使用して仮定を事前分布としてエンコードし、ランキングの推定値と信頼区間の感度分析を行います。複数のベンチマークにわたる実証的評価は、ベイズ推論がより正確なランキングをもたらし、カバレッジ率を大幅に向上させることを示しています。これらの結果は、LLMを審査員として使用する場合、不確実性の定量化に対してより全体的なアプローチを取ることの重要性を強調しています。

&lt;img src="https://arxiv.org/html/2505.21972v1/extracted/6486487/judge_task_figure_black_font.png"/&gt;&lt;p&gt;Patrick Vossler &amp;Fan Xia &amp;Yifan Mai &amp;Jean Feng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21972v1</guid><pubDate>Wed, 28 May 2025 04:50:41 +0000</pubDate></item><item><title>Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference</title><link>http://arxiv.org/abs/2505.21919v1</link><description>大規模言語モデル（LLM）のコンテキストウィンドウ拡張に伴う採用の増加により、推論性能を最適化するために効率的なKey-Value Cache（KVC）管理が不可欠になっています。Retrieval-Augmented Generation（RAG）やエージェントのような推論ワークロードは、高いキャッシュ再利用性を示し、効率的なキャッシュは冗長性を減らし、速度を向上させるために重要です。我々は、公開されているトレースを用いて実際のKVCアクセスパターンを分析し、Redisのような商用Key-Valueストアや、最先端のRDMAベースのシステム（CHIME [1]、Sherman [2]）をKVCメタデータ管理のために評価します。我々の研究は、KVCプリフィルに特化したストレージソリューションの不足を示し、LLMワークロード向けに最適化されたメタデータ管理を備えた効率的な分散キャッシュシステムの必要性を強調し、スケーラブルで低遅延な推論のための改善されたKVC管理システムの設計に関する洞察を提供します。

&lt;img src="https://arxiv.org/html/2505.21919v1/extracted/6485249/figures/motivation/bucket_hit_cdf.png"/&gt;&lt;p&gt;Chen Wang, Eun Kyung Lee, Hao Yu, Yue Zhu, Zhuoran Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21919v1</guid><pubDate>Wed, 28 May 2025 03:05:55 +0000</pubDate></item><item><title>Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy</title><link>http://arxiv.org/abs/2505.21907v1</link><description>AIコパイロットは、ソフトウェア開発やコンテンツ作成などのタスクでユーザーを支援するように設計された、コンテキスト認識型のAI搭載システムであり、現代のワークフローに不可欠なものになりつつあります。これらのシステムの能力と普及が進むにつれて、パーソナライゼーションは、ユーザビリティ、信頼性、生産性を確保するための基礎として浮上してきました。このパーソナライゼーションの中心となるのが、好み最適化です。これは、AIコパイロットが個々のユーザーの好みを検出し、解釈し、適合させる能力です。パーソナライゼーション技術は、レコメンダーシステムや対話エージェントなどの分野では確立されていますが、AIコパイロットのようなインタラクティブなリアルタイムシステムへの適応は、断片的で未開拓のままです。本調査では、AIコパイロットの設計において、ユーザーの好みがどのように捉えられ、モデル化され、洗練されているかに関する研究を統合することで、このギャップに対処します。AIコパイロットの統一的な定義を紹介し、事前インタラクション、インタラクション中、事後インタラクションの段階を中心に構成された、好み最適化戦略の段階的な分類を提案します。好みのシグナルを取得し、ユーザーの意図をモデル化し、フィードバックループを統合するための技術を分析し、確立されたアプローチと最近のイノベーションの両方を強調します。AIパーソナライゼーション、人間とAIのコラボレーション、大規模言語モデルの適応からの洞察を結びつけることで、本調査は、適応性のある、好みを認識したAIコパイロットを設計するための構造化された基盤を提供します。利用可能な好みのリソース、それらをどのように活用できるか、そしてシステムの各設計段階に最適な技術的アプローチの全体像を提供します。

&lt;img src="https://arxiv.org/html/2505.21907v1/extracted/6475662/images/copilot_arch.jpg"/&gt;&lt;p&gt;Amin Beheshti, Australia Department of Computer Engineering, Information Technology, Iran, Macquarie University, Phuong Thao Huynh, Saleh Afzoon, Shiraz, Shiraz University of Technology, Sydney, Usman Naseem School of Computing, Zahra Jahanandish&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21907v1</guid><pubDate>Wed, 28 May 2025 02:52:39 +0000</pubDate></item><item><title>Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development</title><link>http://arxiv.org/abs/2505.21898v1</link><description>大規模言語モデル（LLM）と自律エージェントの最近の進歩は、様々な分野で目覚ましい能力を示しています。しかし、スタンドアロンのエージェントは、広範なインタラクションと多大な計算資源を必要とする複雑なタスクを処理する際に、しばしば限界に直面します。マルチエージェントシステム（MAS）は、タスク分解、反復的なコミュニケーション、役割の専門化といった協調メカニズムを通じて、これらの限界の一部を軽減しますが、通常はリソースを意識せず、高いトークン消費量と過剰な実行時間のために大きな非効率性を招きます。これらの限界に対処するため、我々はリソースを意識したマルチエージェントシステムであるCo-Saving（複数のエージェントが協力してリソース節約活動を行うことを意味する）を提案します。これは、運用効率とソリューションの質を向上させるために経験的知識を活用します。我々の主な革新は、「ショートカット」の導入です。これは、過去に成功した軌跡から学習した指示的な遷移であり、冗長な推論エージェントをバイパスし、集団的な問題解決プロセスを迅速化することを可能にします。ソフトウェア開発タスクに関する実験は、既存の方法に対する大きな利点を示しています。具体的には、最先端のMASであるChatDevと比較して、我々の方法はトークン使用量を平均50.85%削減し、全体的なコード品質を10.06%向上させます。

&lt;img src="https://arxiv.org/html/2505.21898v1/x1.png"/&gt;&lt;p&gt;Telecommunications Siemens Tencent Robotics X qrn22@mails.tsinghua.edu.cn qianc@sjtu.edu.cn liuzy@tsinghua.edu.cn sms@tsinghua.edu.cn, Tsinghua University Shanghai Jiao Tong University Beijing University of Posts&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21898v1</guid><pubDate>Wed, 28 May 2025 02:23:53 +0000</pubDate></item><item><title>GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git</title><link>http://arxiv.org/abs/2505.22583v1</link><description>ソフトウェアエンジニアリング（SE）AIエージェントのベンチマーク、特にSWE-benchは、AIエージェントのプログラミング能力の進歩を促進してきました。しかし、それらはバージョン管理システム（VCS）の操作といった重要な開発者のワークフローを見落としています。この問題に対処するため、VCSタスクにおけるAIエージェントのパフォーマンスを評価するための新しいベンチマークであるGitGoodBenchを提案します。GitGoodBenchは、許容的なオープンソースのPython、Java、Kotlinリポジトリから抽出された3つのコアなGitシナリオを網羅しています。私たちのベンチマークは、包括的な評価スイート（900サンプル）、迅速なプロトタイピングバージョン（120サンプル）、およびトレーニングコーパス（17,469サンプル）の3つのデータセットを提供します。カスタムツールを備えたGPT-4oを使用して、ベンチマークのプロトタイピングバージョンでベースラインパフォーマンスを確立し、全体で21.11％の解決率を達成しました。GitGoodBenchが、単なるプログラミングを超えた、真に包括的なSEエージェントへの重要な足がかりとなることを期待しています。

&lt;img src="https://arxiv.org/html/2505.22583v1/extracted/6489828/figures/mcr-figure.png"/&gt;&lt;p&gt;Information, JetBrains Research School of Computation, Technical University of Munich tobias.lindenbauer@jetbrains.com, Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22583v1</guid><pubDate>Wed, 28 May 2025 16:56:11 +0000</pubDate></item><item><title>SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</title><link>http://arxiv.org/abs/2505.22126v1</link><description>近年、AI駆動の画像生成は急速に進歩している。初期の拡散モデルは知覚的な品質を重視していたが、GPT-4o-imageのような新しいマルチモーダルモデルは高度な推論を統合し、意味的理解と構造的構成を向上させている。科学的なイラスト生成はこの進化を典型的に示している。一般的な画像合成とは異なり、技術的な内容の正確な解釈と、抽象的なアイデアを明確で標準化されたビジュアルに変換することが求められるからだ。このタスクは非常に知識集約的で手間がかかり、多くの場合、何時間もの手作業と特殊なツールが必要となる。これを制御可能かつインテリジェントな方法で自動化することは、実質的な実用的価値を提供するだろう。しかし、AIをこの分野で評価するためのベンチマークは現在存在しない。このギャップを埋めるために、我々は科学的な図生成のための最初のベンチマークであるSridBenchを導入する。これは、自然科学とコンピュータ科学の13分野にわたる主要な科学論文からキュレーションされた1,120のインスタンスで構成されており、人間の専門家とMLLM（大規模言語モデル）によって収集された。各サンプルは、意味的忠実度や構造的正確性を含む6つの側面で評価される。実験結果から、GPT-4o-imageのようなトップレベルのモデルでさえ、人間のパフォーマンスに遅れをとっており、テキスト/ビジュアルの明瞭さや科学的な正確性において共通の問題があることが明らかになった。これらの発見は、より高度な推論駆動型の視覚生成能力の必要性を強調している。

&lt;img src="https://arxiv.org/html/2505.22126v1/x1.png"/&gt;&lt;p&gt;Technology of China Shanghai Innovation Institute Nankai University Wuhan University Shanghai AI Laboratory, University of Science, Yifan Chang Yukang Feng Jianwen Sun Jiaxin Ai&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22126v1</guid><pubDate>Wed, 28 May 2025 08:51:01 +0000</pubDate></item><item><title>The quest for the GRAph Level autoEncoder (GRALE)</title><link>http://arxiv.org/abs/2505.22109v1</link><description>グラフベース学習は多くの注目を集めているが、グラフ表現学習は依然として困難な課題であり、その解決は化学や生物学などの重要な応用分野に影響を与える可能性がある。この目的のために、我々はGRALEという、様々なサイズのグラフを共有埋め込み空間にエンコードおよびデコードする新しいグラフオートエンコーダを導入する。GRALEは、元のグラフと再構成されたグラフを比較する最適輸送に着想を得た損失関数を用いて訓練され、エンコーダおよびデコーダと共同で訓練される微分可能なノードマッチングモジュールを活用する。提案する注意機構ベースのアーキテクチャは、AlphaFoldの中核コンポーネントであるEvoformerに依存しており、これをグラフのエンコードとデコードの両方をサポートするように拡張する。シミュレーションデータおよび分子データに関する数値実験において、GRALEが、分類や回帰から、グラフ補間、編集、マッチング、予測などのより複雑なタスクまで、幅広いダウンストリームタスクに適用可能な、非常に一般的な形式の事前学習を可能にすることを示す。

&lt;img src="https://arxiv.org/html/2505.22109v1/x1.png"/&gt;&lt;p&gt;Ecole Polytechnique, IP Paris, IP Paris &amp;Charlotte Laclau LTCI, IP Paris &amp;Rémi Flamary CMAP, IP Paris Florence d’Alché-Buc LTCI, IP Paris Gabriel Melo LTCI, Paul Krzakala LTCI &amp; CMAP, Télécom paris&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22109v1</guid><pubDate>Wed, 28 May 2025 08:37:33 +0000</pubDate></item><item><title>LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents</title><link>http://arxiv.org/abs/2505.21963v1</link><description>大規模言語モデル（LLM）は、幅広いタスクにおいて卓越した性能を示してきました。LLMを特定のドメインやアプリケーションにさらに適合させるために、教師ありファインチューニング（SFT）、Preference Learning、モデルマージなどのポストトレーニング技術が一般的に用いられます。これらの各手法は個別に広く研究されてきましたが、完全なポストトレーニングパイプラインの自動構築は、未開拓の領域のままです。既存のアプローチは通常、手動設計に依存するか、データ順序付けやマージ戦略など、個々のコンポーネントの最適化に狭く焦点を当てています。本研究では、LLMベースのエージェントを用いて、完全なポストトレーニングパイプラインを自律的に構築および最適化する新しいフレームワークであるLaMDAgent（Language Model Developing Agentの略）を紹介します。LaMDAgentは、多様なモデル生成技術、データセット、ハイパーパラメータ構成を体系的に探索し、タスクベースのフィードバックを活用して、人間の介入を最小限に抑えながら高性能なパイプラインを発見します。実験の結果、LaMDAgentはツール使用の精度を9.0ポイント向上させながら、指示追従能力を維持することが示されました。さらに、従来の人間主導の探索では見過ごされがちな効果的なポストトレーニング戦略を発見します。探索における計算コストを削減するために、データとモデルのサイズのスケーリングの影響をさらに分析し、モデルサイズのスケールアップは新たな課題をもたらす一方、データサイズのスケールアップは費用対効果の高いパイプラインの発見を可能にすることを見出しました。

&lt;img src="https://arxiv.org/html/2505.21963v1/extracted/6487383/fig/ours.png"/&gt;&lt;p&gt;Taro Yano NEC Corporation &amp;Yoichi Ishibashi NEC Corporation &amp;Masafumi Oyamada NEC Corporation&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21963v1</guid><pubDate>Wed, 28 May 2025 04:30:51 +0000</pubDate></item><item><title>AI Mathematician: Towards Fully Automated Frontier Mathematical Research</title><link>http://arxiv.org/abs/2505.22451v1</link><description>大規模推論モデル（LRM）は、近年、数学的能力において著しい進歩を遂げています。しかし、これらの成功は主に競技レベルの問題に限定されていました。本研究では、LRMの推論能力を活用して最先端の数学研究を支援するAI Mathematician（AIM）フレームワークを提案します。競技と比較して、数学研究における2つの重要な課題、すなわち{\it 研究問題の固有の複雑さ}と{\it 手続き的厳密性の要求}を特定しました。これらの課題に対処するため、AIMは、より長い解決策の道筋を促進する探索メカニズムと、信頼性を確保するための悲観的な合理的検証方法という2つの主要な戦略を組み込んでいます。
  この初期バージョンのAIMは、すでに研究レベルのタスクに取り組む強力な能力を示しています。いくつかの現実世界の数学トピックにわたって広範な実験を行い、有望な結果を得ました。AIMは、証明の大部分を自律的に構築し、各研究分野内で非自明な洞察を発見することができます。これらの発見は、数学的発見におけるLRMの可能性を強調し、LRMベースのエージェントシステムが将来的に数学研究を大幅に加速させる可能性があることを示唆しています。

&lt;img src="https://arxiv.org/html/2505.22451v1/x1.png"/&gt;&lt;p&gt;Beijing, China Department of Mathematical Sciences, China Institute for AI Industry Research (AIR), Institute for AI, Tsinghua University, Tsinghua University Qiuzhen College, Yang Liu Dept. of Comp. Sci. &amp; Tech., Yuanhang Liu absent {}^{\phantom{*}} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT Yanxing Huang Yanqiao Wang Peng Li&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22451v1</guid><pubDate>Wed, 28 May 2025 15:10:37 +0000</pubDate></item><item><title>AgentDNS: A Root Domain Naming System for LLM Agents</title><link>http://arxiv.org/abs/2505.22368v1</link><description>大規模言語モデル（LLM）エージェントの急速な進化は、ベンダー間のサービス発見、相互運用性、および通信における重要な課題を浮き彫りにしました。モデルコンテキストプロトコルやエージェント間プロトコルなどの既存のプロトコルは、エージェントとツール間の相互運用性、およびマルチエージェント間の通信の標準化において大きな進歩を遂げています。しかし、異なるエージェントおよびツールベンダー間でのサービス発見のための標準化されたプロトコルとソリューションは依然として不足しています。本稿では、LLMエージェントが組織的および技術的な境界を越えて、サードパーティのエージェントおよびツールサービスを自律的に発見、解決、安全に呼び出すことを可能にするように設計された、ルートドメイン命名およびサービス発見システムであるAgentDNSを提案します。従来のDNSの原則に触発されたAgentDNSは、サービス登録、セマンティックサービス発見、安全な呼び出し、および統一された請求のための構造化されたメカニズムを導入します。AgentDNSのアーキテクチャ、コア機能、およびユースケースを詳細に説明し、現実世界のシナリオにおけるマルチエージェントコラボレーションを効率化する可能性を示します。ソースコードはhttps://github.com/agentdnsで公開されます。

&lt;img src="https://arxiv.org/html/2505.22368v1/extracted/6466035/fig/arch.png"/&gt;&lt;p&gt;Dan Liu, Enfang Cui, Minxin Guo, Qian Wei, Rui She, Tianzheng Li, Wenjuan Xing, Yujun Cheng, Zhijie Zhong, Zhiyuan Liang&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22368v1</guid><pubDate>Wed, 28 May 2025 13:56:22 +0000</pubDate></item><item><title>VIRAL: Vision-grounded Integration for Reward design And Learning</title><link>http://arxiv.org/abs/2505.22092v1</link><description>人間と機械のアライメント（整合性）は、今日の人工知能における重要な課題です。報酬関数を最大化することを目的とする強化学習は、特に、不適切に設計された報酬関数に関連するリスクに対して脆弱です。最近の進歩により、報酬生成のための大規模言語モデル（LLM）が、この文脈において人間のパフォーマンスを上回る可能性があることが示されています。我々は、マルチモーダルLLMの使用を通じて報酬関数を生成および改良するためのパイプラインであるVIRALを紹介します。VIRALは、与えられた環境と目標プロンプトまたはアノテーション付き画像に基づいて、自律的に報酬関数を作成し、インタラクティブに改善します。改良プロセスには、人間のフィードバックを組み込んだり、エージェントのポリシーをビデオ形式で説明するビデオLLMによって生成された記述によって誘導されたりすることができます。我々は、5つのGymnasium環境でVIRALを評価し、ユーザーの意図との整合性を向上させながら、新しい行動の学習を加速することを示しました。ソースコードとデモビデオは、https://github.com/VIRAL-UCBL1/VIRAL および https://youtu.be/t4_BXugBm9Q で入手できます。

&lt;img src="https://arxiv.org/html/2505.22092v1/extracted/6488136/img/viral_3.png"/&gt;&lt;p&gt;Alexandre Faure, Bruno Yun Université Claude Bernard Lyon 1, Ecole Centrale de Lyon, Emilien Komlenovic, France, France CNRS, INSA Lyon, LIRIS, UMR5205, Université Lumière Lyon 2, Valentin Cuzin-Rambaud&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22092v1</guid><pubDate>Wed, 28 May 2025 08:16:09 +0000</pubDate></item><item><title>From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models</title><link>http://arxiv.org/abs/2505.21935v1</link><description>大規模言語モデル（LLM）の登場以来、その取り組みは主に指示遂行能力と演繹的推論能力の向上に集中しており、これらのモデルが真に新しい知識を発見できるのかという疑問は未解決のままです。汎用人工知能（AGI）の追求において、命令を実行したり情報を検索したりするだけでなく、新しい仮説や理論を構築して学習、推論、そして世界の理解を深める新しい知識を生成するモデルの必要性が高まっています。パースの演繹、帰納、そしてアブダクションのフレームワークに導かれ、本調査はLLMベースの仮説発見を検討するための構造化されたレンズを提供します。仮説の生成、応用、そして検証に関する既存の研究を統合し、主要な成果と重要なギャップの両方を特定します。これらの糸を統合することで、LLMが単なる「情報実行者」から、真のイノベーションのエンジンへと進化し、研究、科学、そして現実世界の問題解決を潜在的に変革する可能性を明らかにします。

&lt;img src="https://arxiv.org/html/2505.21935v1/x1.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21935v1</guid><pubDate>Wed, 28 May 2025 03:40:02 +0000</pubDate></item><item><title>On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models</title><link>http://arxiv.org/abs/2505.22598v1</link><description>近年、従来の手段では研究できない、サンプリングが困難な系のシミュレーションを支援するために、機械学習技術の応用が増加している。多くの異なるアーキテクチャや手順が導入されているにもかかわらず、広範な理論的理解は依然として不足しており、最適でない実装のリスクがある。このギャップに対処するための第一歩として、ここでは、キュリー・ワイスモデルに適用される浅いMADEアーキテクチャに対する、広く使用されているSequential Tempering手順の完全な解析的研究を提供する。本研究の貢献は二つある。第一に、最適な重みと勾配降下法による最適化下での学習について記述する。第二に、Sequential Temperingにおいて、局所的なメトロポリス・モンテカルロステップを追加した場合とそうでない場合で何が起こるかを比較する。これにより、この場合に適用する最適な手順に関する理論的な予測を提供することができる。本研究は、機械学習技術をモンテカルロサンプリングおよび最適化に統合するための明確な理論的基盤を確立する。

&lt;img src="https://arxiv.org/html/2505.22598v1/x1.png"/&gt;&lt;p&gt;Federico Ricci-Tersenghi, Francesco Zamponi, Luca Maria Del Bono&lt;/p&gt;&lt;p&gt;CNR-Nanotec, Rome unit, Piazzale Aldo Moro 5, Rome 00185, Italy
Dipartimento di Fisica, Sapienza Università di Roma, Piazzale Aldo Moro 5, Rome 00185, Italy
INFN, sezione di Roma1, Piazzale Aldo Moro 5, Rome 00185, Italy&lt;/p&gt;</description><guid isPermaLink="false">2505.22598v1</guid><pubDate>Wed, 28 May 2025 17:13:11 +0000</pubDate></item><item><title>Reduced order modelling of air puff test for corneal material characterisation</title><link>http://arxiv.org/abs/2505.22495v1</link><description>エアパフ検査における流体構造連成（FSI）モデルの解析が行われた。Abaqusを用いて、材料特性、角膜厚、半径などの様々な生体力学的パラメータを持つ眼に対してエアパフ検査が適用される。エアパフの縮約モデル（乱流衝突噴流）を取得することで、FSIモデルのシミュレーション時間を48時間から有限要素解析（FEA）モデル単独で約12分に短縮した。シミュレーションをさらに高速化し、モデルの精度を向上させるために、物理情報ニューラルネットワーク（PINN）が縮約モデルと統合される。このハイブリッドアプローチは、モデルをより大きなデータセットに拡張し、逆FEAによる眼圧（IOP）推定精度と角膜材料特性アルゴリズムを向上させるのに役立つ。さらに、空間的および時間的パラメータの関数として角膜表面上の圧力および変形分布をモデル化するために、埋め込みガウス変調波形を備えたニューラルネットワーク（NN）フレームワークが提案されている。角膜中心厚（CCT）、眼圧（IOP）、ベースライン特性（Mu）などの角膜生体力学的入力と、圧力および変形の支配係数との関係を学習することにより、ネットワークは高忠実度CFDデータとよく一致する結果を正確に再構築する。このアプローチは、圧力と変形の分布を迅速に捉えることができる。また、圧力と変形の明確な空間的および時間的ダイナミクスに関する洞察を提供し、エアパフ検査における流体構造連成現象のより包括的な理解を可能にする。

&lt;img src="https://arxiv.org/html/2505.22495v1/extracted/6488994/Figures/Fig_1.png"/&gt;&lt;p&gt;Muting Hao, Osama M. Maklad&lt;/p&gt;&lt;p&gt;Centre for Advanced Manufacturing and Materials, University of Greenwich, London, UK
Oxford Thermofluid Institute, University of Oxford, Oxford, UK&lt;/p&gt;</description><guid isPermaLink="false">2505.22495v1</guid><pubDate>Wed, 28 May 2025 15:46:21 +0000</pubDate></item><item><title>Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz</title><link>http://arxiv.org/abs/2505.22083v1</link><description>本研究では、量子多体系の基底状態波動関数を近似する変分モンテカルロ法で使用するための、双曲GRU（リカレントニューラルネットワーク（RNN）の一種）の形式による、非ユークリッドニューラル量子状態（NQS）アンザッツの最初のタイプを紹介します。特に、最大100スピンの1次元および2次元横磁場イジングモデル（TFIM）と、最大50スピンの1次元ハイゼンベルグ$J_1J_2$および$J_1J_2J_3$系という典型的な設定において、従来のユークリッドRNN/GRUと双曲GRUの両方から構築されたNQSアンザッツの性能を検証します。本研究で行われたすべての実験において、双曲GRUは、文献で広く研究されているユークリッドRNNと同等以上の性能を発揮できるという事実により、我々の研究は、量子多体系に対する最初のタイプの非ユークリッドNQSアンザッツとしての双曲GRUの実現可能性の概念実証となります。さらに、ハミルトニアンが明確な階層的相互作用構造を示す設定、例えば、1次、2次、さらには3次近傍相互作用を持つ1Dハイゼンベルグ$J_1J_2$および$J_1J_2J_3$系では、双曲GRUがすべての場合においてユークリッド版を明確に上回ることを結果は示しています。これらの結果が、訓練データが木構造または階層構造を示す場合に双曲GRUがほぼ常にユークリッドRNNを上回るという自然言語処理からの確立された結果を彷彿とさせるという事実は、双曲GRU NQSアンザッツが、異なる次数の近傍相互作用を含む量子スピン系において、ユークリッドRNN/GRU NQSアンザッツよりも優れている可能性が高いという仮説につながります。最後に、本研究を通じて、双曲GRUを超える他のタイプの非ユークリッドNQSの将来の研究を開始したいと考えています。

&lt;img src="https://arxiv.org/html/2505.22083v1/extracted/6428914/figs/1d_tfim_comparison.png"/&gt;&lt;p&gt;H. L. Dao&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22083v1</guid><pubDate>Wed, 28 May 2025 08:06:25 +0000</pubDate></item><item><title>Machine-Learned Potentials for Solvation Modeling</title><link>http://arxiv.org/abs/2505.22402v1</link><description>溶媒環境は、分子構造、エネルギー、反応性、界面現象を決定する上で中心的な役割を果たします。しかし、第一原理からの溶媒和のモデリングは、相互作用の複雑な絡み合いと、第一原理計算のシステムサイズに対する計算コストのスケーリングの悪さから、依然として困難です。機械学習ポテンシャル（MLP）は近年、量子化学的手法の効率的な代替手段として登場し、大幅に計算コストを削減しながら第一原理の精度を提供します。MLPは、基礎となるポテンシャルエネルギー曲面を近似し、溶媒和された系におけるエネルギーと力の効率的な計算を可能にし、水素結合、長距離分極、構造変化などの効果を考慮することができます。本レビューでは、溶媒和モデリングにおけるMLPの開発と応用を概説します。MLPベースのエネルギーおよび力の予測の理論的基礎を要約し、トレーニングターゲット、モデルタイプ、アーキテクチャ、記述子、トレーニングプロトコルに関連する設計上の選択に基づいてMLPを分類します。確立された溶媒和ワークフローへの統合について、小分子、界面、反応系にわたるケーススタディを用いて議論します。最後に、溶媒和を考慮した原子モデリングのための、転送可能でロバストで物理的に根拠のあるMLPに向けた、未解決の課題と今後の方向性を概説します。

&lt;img src="https://arxiv.org/html/2505.22402v1/x1.png"/&gt;&lt;p&gt;Raghunathan Ramakrishnan, Roopshree Banchode, Shampa Raghunathan, Surajit Das&lt;/p&gt;&lt;p&gt;Tata Institute of Fundamental Research, Hyderabad 500046, India
École Centrale School of Engineering, Mahindra University, Hyderabad 500043, India&lt;/p&gt;</description><guid isPermaLink="false">2505.22402v1</guid><pubDate>Wed, 28 May 2025 14:29:09 +0000</pubDate></item><item><title>Machine Learning Interatomic Potentials: library for efficient training, model development and simulation of molecular systems</title><link>http://arxiv.org/abs/2505.22397v1</link><description>機械学習原子間ポテンシャル（MLIP）は、分子特性予測のための新しいインシリコアプローチであり、経験的な力場と密度汎関数理論（DFT）の精度/速度のトレードオフを打破する代替手段となります。本ホワイトペーパーでは、2つの主要な目的で作成された当社のMLIPライブラリを紹介します。（1）機械学習のバックグラウンドを持たない業界の専門家に対し、MLIPモデルを実験するためのユーザーフレンドリーで計算効率の高いツールセットを提供すること、（2）機械学習の開発者に対し、分子動力学ツールと完全に統合された新しいアプローチを開発するためのフレームワークを提供すること。このリリースに含まれるライブラリには、3つのモデルアーキテクチャ（MACE、NequIP、ViSNet）と、2つの分子動力学（MD）ラッパー（ASE、JAX-MD）、および一連の事前学習済み有機モデルが含まれています。特にJAX-MDとのシームレスな統合により、非常に効率的なMDシミュレーションが容易になり、MLIPモデルを産業応用へと大幅に近づけます。このライブラリは、GitHubおよびPyPIでApacheライセンス2.0の下で入手可能です。

&lt;img src="https://arxiv.org/html/2505.22397v1/x1.png"/&gt;&lt;p&gt;Christoph Brunken, Edan Toledo, Fabio Falcioni, Heloise Chomet, Jules Tilly, Lucien Walewski, Manus McAuliffe, Marie Bluntzer, Martin Maarand, Olivier Peltre, Silvia Acosta-Gutiérrez, Solal Attias, Valentin Heyraud, Yessine Khanfir&lt;/p&gt;&lt;p&gt;InstaDeep&lt;/p&gt;</description><guid isPermaLink="false">2505.22397v1</guid><pubDate>Wed, 28 May 2025 14:24:47 +0000</pubDate></item><item><title>Apax: A Flexible and Performant Framework For The Development of Machine-Learned Interatomic Potentials</title><link>http://arxiv.org/abs/2505.22168v1</link><description>JAXにおける原子論的学習ポテンシャル（apax）を紹介します。これは、機械学習された原子間ポテンシャルの学習と推論のための、柔軟で効率的なオープンソースソフトウェアパッケージです。JAXフレームワーク上に構築されたapaxは、GPUアクセラレーションをサポートし、高速な開発のための柔軟なモデル抽象化を実装しています。カーネルベースのデータ選択、適切に調整された不確実性推定、および強化されたサンプリングなどの機能を備えており、アクティブラーニングアプリケーションと使いやすさに合わせて調整されています。apaxで行われた機能と設計上の決定について議論した後、その機能のいくつかを紹介します。まず、室温イオン液体EMIM+BF4-のデータセットをアクティブラーニングを使用して作成します。反復間の継続的な学習モデルが、モデルの精度をわずかに低下させるだけで、トレーニング時間を最大85％削減できることを強調します。次に、データ並列トレーニング設定での優れたスケーラビリティを示します。apaxに実装されているガウスモーメントニューラルネットワークモデルは、パフォーマンスが最適化されたAllegroモデルよりも高い精度と最大10倍高速な推論時間を達成することを報告します。最近公開されたLi3PO4データセットを比較対象として使用し、同等の精度と推論パフォーマンスメトリクスを報告します。さらに、利用可能なシミュレーションエンジンの推論速度を比較します。最後に、apaxのモジュール性を強調するために、等変メッセージパッシングモデルを浅いアンサンブルとしてトレーニングし、不確実性主導のダイナミクスを実行するために使用します。

&lt;img src=""/&gt;&lt;p&gt;Christian Holm, Fabian Zills, Johannes Kästner, Moritz R. Schäfer, Nico Segreto&lt;/p&gt;&lt;p&gt;[&lt;/p&gt;</description><guid isPermaLink="false">2505.22168v1</guid><pubDate>Wed, 28 May 2025 09:34:03 +0000</pubDate></item><item><title>Emergence of Diverse Topological States in Ge Doped MnBi2Te4</title><link>http://arxiv.org/abs/2505.22348v1</link><description>対称性、トポロジー、磁性の相互作用を研究するための理想的なプラットフォームとして、磁性トポロジカル絶縁体（MTI）MnBi2Te4は広範な注目を集めています。しかし、その強いn型固有欠陥が、エキゾチックな現象の実現を妨げています。最近のGeドーピングがフェルミ準位の位置を効率的に調整できるという発見に刺激を受け、ここではMTI MnBi2Te4から強トポロジカル絶縁体GeBi2Te4へのドーピング濃度によるバンド構造の進化とトポロジカル相図を系統的に調査します。磁性ドープされたBi2Se3とは異なり、ここのトポロジーは、反強磁性と非磁性/強磁性単位胞間の2つの時間反転不変運動量のバンド折り畳みから生じる2つのバンド反転の競合によって決定されます。バンド運動量マッピング法を用いることで、既知のMTI相に加えて、注目すべきことに、反強磁性状態において2種類の磁性ディラック半金属相、強磁性状態において2種類のワイル半金属相、そして異なるドーピング領域において中間的な自明な状態を見出しました。興味深いことに、自明な状態は、小さな歪みによって、2つの共存するバンド反転と非常に長いフェルミ弧を持つワイル相に調整できます。私たちの研究は、固有の量子現象を伴う多様なトポロジカル状態が実現可能であり、将来の電子デバイスを設計するための大きな可能性を秘めていることを明らかにしています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.22348v1</guid><pubDate>Wed, 28 May 2025 13:30:31 +0000</pubDate></item><item><title>Prediction and Synthesis of Mg$_4$Pt$_3$H$_6$: A Metallic Complex Transition Metal Hydride Stabilized at Ambient Pressure</title><link>http://arxiv.org/abs/2505.22546v1</link><description>高い臨界温度($T_c$)を持つ超伝導水素化物の低圧安定化は依然として大きな課題であり、実験的に検証された超伝導水素化物は一般的に限られた数の構造プロトタイプに制約されています。遷移金属複合水素化物（ヒドリド錯体）は、通常水素貯蔵材料と見なされますが、低圧で安定化された広範囲の化合物を示し、最近では高い$T_c$超伝導性の予測があります。この材料群に触発され、既知の三元水素化物化合物がないMg-Pt-H系における複合水素化物の形成を調査しました。第一原理構造予測に基づいて、レーザー加熱ダイヤモンドアンビルセルを用いて、新しい複合遷移金属水素化物Mg$_4$Pt$_3$H$_6$の合成に成功しました。この化合物は、8〜25 GPaの中程度の圧力で体心立方構造プロトタイプで形成されます。既知のヒドリド錯体の大部分とは異なり、Mg$_4$Pt$_3$H$_6$は金属であり、形式電荷は4[Mg]$^{2+}$.3[PtH$_2$]$^{2-}$と記述されます。減圧中に得られたX線回折（XRD）測定により、Mg$_4$Pt$_3$H$_6$は常温常圧まで冷却しても安定であることが明らかになりました。磁場および温度依存の電気輸送測定は、理論計算と妥当な一致を示す、$T_c$ (50%) = 2.9 Kの常圧超伝導性を示しています。これらの発見は、Mg-Pt-H系の相挙動を明らかにし、水素リッチ超伝導体の新しいクラスとしての遷移金属複合水素化物に関する貴重な洞察を提供します。

&lt;img src="https://arxiv.org/html/2505.22546v1/extracted/6489962/pictures/figure1.jpg"/&gt;&lt;p&gt;Anna Pakhomova, Chris J. Pickard, Christoph Heil, Dominik Daisenberger, Kapildeb Dolui, Mads F. Hansen, Matthew N. Julian, Michael J. Hutcheon, Mihir R. Sahoo, Mohamed Mezouar, Rohit P. Prasankumar, Shubham Sinha, Stella Chariton, Timothy A. Strobel, Vitali Prakapenka, Wencheng Lu&lt;/p&gt;&lt;p&gt;Advanced Institute for Materials Research, Tohoku University, Sendai, 980-8577, Japan
Cavendish Laboratory, University of Cambridge, JJ Thomson Avenue,
Cambridge, CB3 0HE, United Kingdom
Center for Advanced Radiation Sources, The University of Chicago, Lemont, Illinois 60439, USA
Department of Materials Science and Metallurgy, University of Cambridge, 27 Charles Babbage Road, Cambridge, CB3 0FS, UK
Diamond Light Source, Chilton, Didcot OX11 0DE, United Kingdom
Earth and Planets Laboratory, Carnegie Institution for Science, 5241 Broad Branch Road NW, Washington, DC 20015, USA
Enterprise Science Fund, Intellectual Ventures, 3150 139th Ave SE, Bellevue, WA, 98005, USA
European Synchrotron Radiation Facility, B.P.220, F-38043 Grenoble Cedex, France
Institute of Theoretical and Computational Physics, Graz University of Technology, NAWI Graz, 8010 Graz, Austria&lt;/p&gt;</description><guid isPermaLink="false">2505.22546v1</guid><pubDate>Wed, 28 May 2025 16:28:16 +0000</pubDate></item><item><title>Weak valley-layer coupling and valley polarization in centrosymmetric $\mathrm{FeCl_2}$ monolayer</title><link>http://arxiv.org/abs/2505.22392v1</link><description>バレー自由度を情報の記憶と処理の担い手として利用する上で、バレー偏極は重要な役割を果たします。バレー偏極の様々なメカニズムが提案されており、その中でもバレー層間結合メカニズムは、面外電場によるバレー偏極の誘導を伴います。本研究では、第一原理計算を通じて、中心対称な$\mathrm{FeCl_2}$単層において弱いバレー層間結合が存在することを発見しました。重要なのは、バレー層間結合が面外磁化でのみ発生し、面内磁化では消失することです。強いバレー層間結合を持つ単層と比較して、$\mathrm{FeCl_2}$は同じ大きさのバレースプリッティングを達成するために非常に強い電場を必要とします。バレー偏極のスイッチングは、磁化と電場の方向を操作することで実現できます。これらの方向のいずれか一方のみを反転させるとバレー偏極が切り替わり、両方を同時に反転させるとバレー偏極は変化しません。さらに、単純に積層された二層$\mathrm{FeCl_2}$は、$PT$-反強磁性体として、外部電場なしに自発的にバレー偏極を達成でき、小型化、超高密度化、超高速性能の可能性を示しています。本研究は、弱いバレー層間結合を持つ材料を特定するための指針を提供し、さらに電場と積層エンジニアリングによるバレー偏極の制御を可能にします。

&lt;img src="https://arxiv.org/html/2505.22392v1/x1.png"/&gt;&lt;p&gt;Gangqiang Zhu, Liguo Zhang, San-Dong Guo, Xiao-Shu Guo&lt;/p&gt;&lt;p&gt;School of Electronic Engineering, Xi’an University of Posts and Telecommunications, Xi’an 710121, China
School of Physics and Electronic Information, Shaanxi Normal University, Xi’an 716000, Shaanxi, China&lt;/p&gt;</description><guid isPermaLink="false">2505.22392v1</guid><pubDate>Wed, 28 May 2025 14:18:31 +0000</pubDate></item><item><title>Systematic generation of electron models for Second-Principles Density Functional Theory Methods</title><link>http://arxiv.org/abs/2505.22056v1</link><description>本稿では、第二原理密度汎関数理論（SPDFT）の枠組みにおいて、電子モデルを生成するための系統的かつ準自動化された手法を提示します。このアプローチにより、注意深く設計されたトレーニングセットに対する第一原理計算から必要なすべてのパラメータを導出することで、正確かつ計算効率の高いモデルの構築が可能になります。本手法の重要な特徴は、空間群対称性を強制的に適用することであり、これにより独立パラメータの数と必要な計算量を削減します。この形式には、一次および二次項の両方を通じた電子-格子結合、および電子-電子相互作用を含む、1電子ハミルトニアンの改良された扱いが含まれており、構造的および電子的応答の正確なモデリングを可能にします。この手法を、それぞれ遷移金属ペロブスカイトおよびワイドバンドギャップ絶縁体の代表的な材料であるSrTiO$_{3}$とLiFに適用します。どちらの場合も、得られたモデルは、さまざまな原子配置および電荷状態において、DFT参照データを高い忠実度で再現します。我々の結果は、このアプローチの堅牢性を検証し、ポラロンや励起子などの複雑な現象をシミュレートする可能性を強調しています。本研究は、SPDFTを光電子特性のリアルタイムシミュレーションに拡張し、機械学習手法とのさらなる統合のための基礎を築きます。

&lt;img src="https://arxiv.org/html/2505.22056v1/x1.png"/&gt;&lt;p&gt;Javier Junquera, Jorge Íñiguez-González, Nayara Carral-Sainz, Pablo García-Fernández, Toraya Fernández-Ruiz&lt;/p&gt;&lt;p&gt;Departamento de Ciencias de la Tierra y
Física de la Materia Condensada, Universidad de Cantabria,
Avenida de los Castros s/n, 39005 Santander, Spain.
Department of Physics and Materials Science, University of Luxembourg, 41 Rue du Brill, Belvaux L-4422, Luxembourg
Materials Research and Technology Department, Luxembourg Institute of Science and Technology, 5 avenue des Hauts-Fourneaux, L-4362 Esch/Alzette, Luxembourg&lt;/p&gt;</description><guid isPermaLink="false">2505.22056v1</guid><pubDate>Wed, 28 May 2025 07:27:04 +0000</pubDate></item><item><title>Enhanced thermopower in two-dimensional ruthenium dichalcogenides $RuX_2$ (X = S, Se): a first-principles study</title><link>http://arxiv.org/abs/2505.22510v1</link><description>遷移金属ダイカルコゲナイド（TMD）は、その独特な電子特性と調整可能なバンドギャップにより、熱電応用における可能性で注目を集めています。本研究では、第一原理計算と半古典ボルツマン輸送方程式を用いて、$T^{\prime}-RuX_2$（X = S, Se）の電子特性と熱電特性を系統的に調査します。我々の発見は、$T^{\prime}-RuX_2$がエネルギー的および機械的に安定であり、高い熱電能値を持つことを確認しており、$T^{\prime}-RuS_2$は正孔ドーピングで$2685~\mu V/K$、電子ドーピングで$2585~\mu V/K$のゼーベック係数を示し、一方、$T^{\prime}-RuSe_2$は正孔ドーピングと電子ドーピングでそれぞれ$1515~\mu V/K$と$1533~\mu V/K$の値を示します。両方の材料は妥当なパワーファクターとZT値を示し、p型$T^{\prime}-RuS_2$と$T^{\prime}-RuSe_2$は、y方向に沿って1200 Kでそれぞれ最大ZT値0.85と0.87を達成します。これらの結果は、$T^{\prime}$-$RuS_2$と$T^{\prime}$-$RuSe_2$が高温TMDベースの熱電デバイスの有望な候補であることを強調しています。

&lt;img src="https://arxiv.org/html/2505.22510v1/extracted/6489905/fig-1.png"/&gt;&lt;p&gt;Ajay Kumar, Parbati Senapati, Prakash Parida&lt;/p&gt;&lt;p&gt;Department of Physics,
Indian Institute of Technology Patna, Bihta, Bihar, 801106, India&lt;/p&gt;</description><guid isPermaLink="false">2505.22510v1</guid><pubDate>Wed, 28 May 2025 16:00:12 +0000</pubDate></item><item><title>Structural Hole Traps in III-V Quantum Dots</title><link>http://arxiv.org/abs/2505.22419v1</link><description>無毒なIII-V族量子ドット（QD）は、II-VI族やIV-VI族QDよりも性能を制限するトラップ状態の密度が高いという問題があります。このようなトラップ状態は一般に、QD表面の配位不足の原子から生じると考えられています。本稿では、InPおよびGaP QDにおいて、構造的な歪みを持つ完全配位原子から生じるトラップ状態（ここでは構造トラップと呼ぶ）に関する計算上の証拠と、その探求について提示します。特に、アニオンを中心とした正孔トラップの特性に焦点を当て、これらが（通常カチオンを配位する）配位子の選択に比較的影響を受けないことを示します。トラップ中心の切り出しを補間することにより、構造トラップの存在に関する単純な分子軌道（MO）の議論に到達し、2つの主要な様式、すなわち結合の伸長とシーソーのような形状への角度歪みを見出します。これらの構造トラップ状態は、III-V族QDの低い性能を理解する上で重要であり、コアシェルパッシベーションでさえ、構造を硬化させることができない限り、これらの欠陥を除去できない可能性があります。さらに、歪んだ構造が一時的に形成される可能性があるため、興味深い動的特性につながる可能性があります。

&lt;img src="https://arxiv.org/html/2505.22419v1/x2.png"/&gt;&lt;p&gt;Alexandra Alexiu, Ezra Alexander, Matthias Kick, Troy Van Voorhis&lt;/p&gt;&lt;p&gt;Department of Chemistry, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA&lt;/p&gt;</description><guid isPermaLink="false">2505.22419v1</guid><pubDate>Wed, 28 May 2025 14:44:42 +0000</pubDate></item></channel></rss>