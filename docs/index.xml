<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Tue, 27 May 2025 03:18:31 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-27</title><link>https://arxiv.org/2025-05-27</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19627v1"&gt;In-depth Investigation of Conduction Mechanism on Defect-induced Proton-conducting Electrolytes BaHfO$_3$&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19686v1"&gt;Symmetry-broken charge-ordered ground state in CsV$_3$Sb$_5$ Kagome metal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19778v1"&gt;Giant spontaneous polarization in zincblende III-V semiconductors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19968v1"&gt;Investigation of local surrounding of Mn atoms in Ni-Mn-Ga Heusler alloy using nuclear magnetic resonance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19999v1"&gt;Analysis of real-space transport channels for electrons and holes in halide perovskites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20092v1"&gt;Impact of anharmonicity on the carrier mobility of the Pb-free CsSnBr$_3$ perovskite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20145v1"&gt;Auger parameter analysis for TiN and AlN thin films via combined in-situ XPS and HAXPES&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19485v1"&gt;Universal Symmetries in Twisted Moiré Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19881v1"&gt;Effective potential of scalar Lee-Wick pseudo-electrodynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19894v1"&gt;Laser-dressed partial density of states&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19935v1"&gt;Nonlinear Transport in Carbon Quantum Dot Electronic Devices: Experiment and Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20039v1"&gt;Exploring Aperiodic Order in Photonic Time Crystals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20071v1"&gt;Crystallographic control of hydrogen ingress in bcc-Iron: Insights from ab initio simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20141v1"&gt;Sublimation of orientated amino acid films for reliable, amplified piezoelectric performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19900v1"&gt;Yielding and memory in a driven mean-field model of glasses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20055v1"&gt;Triplets in the cradle: ultrafast dynamics in a cyclic disulfide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19856v1"&gt;Beyond the Electric Dipole Approximation: Electric and Magnetic Multipole Contributions Reveal Biaxial Water Structure from SFG Spectra at the Air-Water Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19748v1"&gt;Boundary local time on wedges and prefractal curves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19841v1"&gt;Efficient Deconvolution in Populational Inverse Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20030v1"&gt;Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19474v1"&gt;Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19477v1"&gt;Judging with Many Minds: Do More Perspectives Mean Less Prejudice?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19550v1"&gt;Turing Test 2.0: The General Intelligence Threshold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19562v1"&gt;AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19653v1"&gt;Token-Importance Guided Direct Preference Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19690v1"&gt;Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19792v1"&gt;Types of Relations: Defining Analogies with Category Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19892v1"&gt;Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19933v1"&gt;Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19965v1"&gt;Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20075v1"&gt;Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20119v1"&gt;Spatiotemporal Causal Decoupling Model for Air Quality Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20120v1"&gt;Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20127v1"&gt;Agentic AI Process Observability: Discovering Behavioral Variability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20148v1"&gt;MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19414v1"&gt;Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19419v1"&gt;It's Not Just Labeling" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19423v1"&gt;Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19427v1"&gt;WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19430v1"&gt;Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19434v1"&gt;CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19436v1"&gt;Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19459v1"&gt;Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19465v1"&gt;Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19466v1"&gt;Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19488v1"&gt;Understanding Transformer from the Perspective of Associative Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19498v1"&gt;Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19505v1"&gt;Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19509v1"&gt;Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19525v1"&gt;Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19547v1"&gt;STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19548v1"&gt;How Syntax Specialization Emerges in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19563v1"&gt;Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19567v1"&gt;LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19568v1"&gt;MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19572v1"&gt;DocMEdit: Towards Document-Level Model Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19588v1"&gt;LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19607v1"&gt;Energy-based Preference Optimization for Test-time Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19609v1"&gt;Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19611v1"&gt;Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19620v1"&gt;Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19621v1"&gt;Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19623v1"&gt;AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19624v1"&gt;Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19625v1"&gt;Search-Based Software Engineering in the Landscape of AI Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19631v1"&gt;Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19641v1"&gt;SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19645v1"&gt;MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19648v1"&gt;Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19658v1"&gt;Large Language Models in Code Co-generation for Safe Autonomous Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19660v1"&gt;GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19662v1"&gt;FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19667v1"&gt;LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19675v1"&gt;Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19679v1"&gt;KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19683v1"&gt;Large Language Models for Planning: A Comprehensive and Systematic Survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19700v1"&gt;Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19706v1"&gt;Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19719v1"&gt;OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19722v1"&gt;Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19734v1"&gt;ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19754v1"&gt;NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19764v1"&gt;Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19769v1"&gt;TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19776v1"&gt;Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19785v1"&gt;MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19815v1"&gt;Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19819v1"&gt;FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19823v1"&gt;LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19827v1"&gt;Revisiting Glorot Initialization for Long-Range Linear Recurrences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19838v1"&gt;FoodTaxo: Generating Food Taxonomies with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19847v1"&gt;DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19851v1"&gt;Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19853v1"&gt;Two Causally Related Needles in a Video Haystack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19867v1"&gt;Deep Active Inference Agents for Delayed and Long-Horizon Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19914v1"&gt;Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19915v1"&gt;Evaluating AI cyber capabilities with crowdsourced elicitation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19920v1"&gt;A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19956v1"&gt;DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19966v1"&gt;Learning to Select In-Context Demonstration Preferred by Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19973v1"&gt;DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20021v1"&gt;Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20026v1"&gt;Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20033v1"&gt;EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20065v1"&gt;SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20068v1"&gt;On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20072v1"&gt;Incentivizing Reasoning from Weak Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20081v1"&gt;Inference-time Alignment in Continuous Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20085v1"&gt;Explanation User Interfaces: A Systematic Literature Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20087v1"&gt;Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20089v1"&gt;Homophily Enhanced Graph Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20100v1"&gt;AdaTP: Attention-Debiased Token Pruning for Video Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20109v1"&gt;Language-Agnostic Suicidal Risk Detection Using Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20110v1"&gt;Proxy-Free GFlowNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20112v1"&gt;ResSVD: Residual Compensated SVD for Large Language Model Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20113v1"&gt;Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20137v1"&gt;Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20150v1"&gt;On the (Non) Injectivity of Piecewise Linear Janossy Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19426v1"&gt;The Role of Diversity in In-Context Learning for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19441v1"&gt;Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19455v1"&gt;MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19457v1"&gt;BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19469v1"&gt;Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19481v1"&gt;Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19504v1"&gt;DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19514v1"&gt;SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19527v1"&gt;Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19528v1"&gt;AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19531v1"&gt;Minimalist Softmax Attention Provably Learns Constrained Boolean Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19534v1"&gt;Training-Free Multi-Step Audio Source Separation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19536v1"&gt;FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19574v1"&gt;Situationally-Aware Dynamics Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19578v1"&gt;Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19591v1"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19599v1"&gt;Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19601v1"&gt;Preference Optimization by Estimating the Ratio of the Data Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19616v1"&gt;Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19644v1"&gt;STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19671v1"&gt;Automated evaluation of children's speech fluency for low-resource languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19687v1"&gt;DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19693v1"&gt;EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19698v1"&gt;JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19699v1"&gt;Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19714v1"&gt;MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19715v1"&gt;Graceful Forgetting in Generative Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19752v1"&gt;Discrete Markov Bridge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19757v1"&gt;CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19790v1"&gt;Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19795v1"&gt;The Missing Point in Vision Transformers for Universal Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19809v1"&gt;Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19825v1"&gt;Foundation Models for Tabular Data within Systemic Contexts Need Grounding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19850v1"&gt;DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19866v1"&gt;HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19874v1"&gt;StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19887v1"&gt;Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19896v1"&gt;Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19912v1"&gt;APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19944v1"&gt;Can Visual Encoder Learn to See Arrows?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19947v1"&gt;Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19948v1"&gt;SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19951v1"&gt;Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19964v1"&gt;The Limits of Preference Data for Post-Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19983v1"&gt;ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20011v1"&gt;The Many Challenges of Human-Like Agents in Virtual Game Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20024v1"&gt;ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20029v1"&gt;Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20047v1"&gt;Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20053v1"&gt;Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20063v1"&gt;SAEs Are Good for Steering -- If You Select the Right Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20066v1"&gt;Automated data curation for self-supervised learning in underwater acoustic analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20067v1"&gt;Community Moderation and the New Epistemology of Fact Checking on Social Media&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20099v1"&gt;Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20132v1"&gt;Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20139v1"&gt;StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20149v1"&gt;Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20152v1"&gt;Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19538v1"&gt;DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19663v1"&gt;A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20027v1"&gt;Multi-modal brain encoding models for multi-modal stimuli&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19431v1"&gt;Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19432v1"&gt;Advanced long-term earth system forecasting by learning the small-scale nature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19445v1"&gt;MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19497v1"&gt;Learning for Dynamic Combinatorial Optimization without Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19532v1"&gt;Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19533v1"&gt;ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19561v1"&gt;Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19602v1"&gt;Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19605v1"&gt;Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19669v1"&gt;Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19680v1"&gt;Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19682v1"&gt;Deep Actor-Critics with Tight Risk Certificates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19740v1"&gt;Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19789v1"&gt;What Can RL Bring to VLA Generalization? An Empirical Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19820v1"&gt;InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19855v1"&gt;Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19888v1"&gt;Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19943v1"&gt;Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19946v1"&gt;Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19949v1"&gt;Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19982v1"&gt;Rethinking Probabilistic Circuit Parameter Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19996v1"&gt;Learning Optimal Multimodal Information Bottleneck Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20010v1"&gt;Data-Dependent Regret Bounds for Constrained MABs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20034v1"&gt;Graph Wave Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20051v1"&gt;Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20074v1"&gt;An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20076v1"&gt;Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20095v1"&gt;Spurious Privacy Leakage in Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20135v1"&gt;Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20142v1"&gt;Model Stitching by Functional Latent Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19440v1"&gt;The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19470v1"&gt;Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19479v1"&gt;Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19491v1"&gt;Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19507v1"&gt;Multimodal Machine Translation with Visual Scene Graph Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19522v1"&gt;Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19537v1"&gt;Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19543v1"&gt;Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19544v1"&gt;Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19558v1"&gt;EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19587v1"&gt;WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19589v1"&gt;Model Agnostic Differentially Private Causal Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19590v1"&gt;Learning to Reason without External Rewards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19603v1"&gt;Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19604v1"&gt;Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19614v1"&gt;Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19720v1"&gt;A Structured Tour of Optimization with Finite Differences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19731v1"&gt;Accelerating Nash Learning from Human Feedback via Mirror Prox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19743v1"&gt;Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19770v1"&gt;Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19802v1"&gt;GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19807v1"&gt;Density Ratio-Free Doubly Robust Proxy Causal Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19821v1"&gt;Poison in the Well: Feature Embedding Disruption in Backdoor Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19840v1"&gt;One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19862v1"&gt;REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19863v1"&gt;FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19893v1"&gt;ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19923v1"&gt;Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19925v1"&gt;Cellwise and Casewise Robust Covariance in High Dimensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19932v1"&gt;Logic Gate Neural Networks are Good for Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19940v1"&gt;Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19954v1"&gt;An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19986v1"&gt;Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20003v1"&gt;TabPFN: One Model to Rule Them All?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20017v1"&gt;Linear Bandits with Non-i.i.d. Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20020v1"&gt;Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20036v1"&gt;Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20052v1"&gt;Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20079v1"&gt;A fast sound power prediction tool for genset noise using machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20107v1"&gt;Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20123v1"&gt;Understanding Generalization in Diffusion Models via Probability Flow Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20133v1"&gt;AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20144v1"&gt;SeMe: Training-Free Language Model Merging via Semantic Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19425v1"&gt;Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19458v1"&gt;Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19486v1"&gt;VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19521v1"&gt;Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19635v1"&gt;When fractional quasi p-norms concentrate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19712v1"&gt;On the Relation between Rectified Flows and Optimal Transport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19737v1"&gt;Weighted Leave-One-Out Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19779v1"&gt;Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19806v1"&gt;Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19837v1"&gt;Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19969v1"&gt;Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19997v1"&gt;Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20032v1"&gt;ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20048v1"&gt;Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20098v1"&gt;Transformer in Protein: A Survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20130v1"&gt;Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19428v1"&gt;Frictional Agent Alignment Framework: Slow Down and Don't Break Things&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19429v1"&gt;Rhapsody: A Dataset for Highlight Detection in Podcasts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19435v1"&gt;Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19439v1"&gt;Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19472v1"&gt;Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19475v1"&gt;Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19484v1"&gt;CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19510v1"&gt;LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19511v1"&gt;Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19515v1"&gt;Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19529v1"&gt;Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19549v1"&gt;Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19586v1"&gt;TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19598v1"&gt;Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19606v1"&gt;Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19628v1"&gt;HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19630v1"&gt;DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19634v1"&gt;Faster and Better LLMs via Latency-Aware Test-Time Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19640v1"&gt;Interleaved Reasoning for Large Language Models via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19674v1"&gt;Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19756v1"&gt;Efficient Reasoning via Chain of Unconscious Thought&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19766v1"&gt;SGM: A Framework for Building Specification-Guided Moderation Filters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19768v1"&gt;T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19797v1"&gt;The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19804v1"&gt;Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19848v1"&gt;Improving Multilingual Math Reasoning for African Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19959v1"&gt;MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19970v1"&gt;CP-Router: An Uncertainty-Aware Router Between LLM and LRM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19971v1"&gt;Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19987v1"&gt;How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20006v1"&gt;Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20013v1"&gt;WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20014v1"&gt;Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20016v1"&gt;TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20023v1"&gt;Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20045v1"&gt;Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20088v1"&gt;Multi-Domain Explainability of Preferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20097v1"&gt;S2LPP: Small-to-Large Prompt Prediction across LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20101v1"&gt;Adaptive Deep Reasoning: Triggering Deep Thinking When Needed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20154v1"&gt;UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20155v1"&gt;Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19494v1"&gt;Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19678v1"&gt;Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19773v1"&gt;What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20015v1"&gt;On the class of coding optimality of human languages and the origins of Zipf's law&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20050v1"&gt;MVP: Multi-source Voice Pathology detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20118v1"&gt;TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19670v1"&gt;Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19937v1"&gt;ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.19978v1"&gt;DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-27</guid><pubDate>Tue, 27 May 2025 12:14:22 +0900</pubDate></item><item><title>SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment</title><link>http://arxiv.org/abs/2505.20103v1</link><description>引用は、現在の研究と先行研究との関連性を強調するため、科学研究論文において非常に重要です。しかし、このプロセスは研究者にとって時間のかかる作業であることが多いです。本研究では、論文中の引用箇所に対して、引用論文を自動的に推薦し、引用文を生成することを目的としたSciRGCフレームワークを提案します。このフレームワークは、学術的な引用生成における2つの主要な課題、すなわち、1) 著者の引用意図を正確に特定し、関連する引用論文を見つける方法、2) 人間の好みに沿った高品質な引用文を生成する方法、に取り組んでいます。引用論文推薦モジュールでは、引用ネットワークと感情的な意図を組み込むことで、引用推薦の精度を向上させ、引用文生成モジュールでは、元の論文の要約、局所的な文脈、引用意図、および推薦された論文を入力として使用することで、推論に基づいた引用文を生成します。さらに、生成された引用文の質を公平に評価するための新しい評価指標を提案します。ベースラインモデルとの比較やアブレーション実験を通じて、SciRGCフレームワークは、引用推薦の精度と関連性を向上させるだけでなく、生成された引用文が文脈において適切であることを保証し、学際的な研究者にとって貴重なツールとなります。

&lt;img src="https://arxiv.org/html/2505.20103v1/extracted/6477567/introduction.png"/&gt;&lt;p&gt;China, China &amp;Jingqiang Chen Nanjing University of Posts, Telecommunications Nanjing, Xiangyu Li Nanjing University of Posts&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20103v1</guid><pubDate>Mon, 26 May 2025 15:09:10 +0000</pubDate></item><item><title>REARANK: Reasoning Re-ranking Agent via Reinforcement Learning</title><link>http://arxiv.org/abs/2505.20046v1</link><description>我々は、大規模言語モデル（LLM）に基づいたリストワイズ推論リランキングエージェントであるREARANKを提案します。REARANKは、リランキングの前に明示的に推論を行うことで、性能と解釈可能性を大幅に向上させます。強化学習とデータ拡張を活用することで、REARANKは、人気のある情報検索ベンチマークにおいて、ベースラインモデルを大幅に上回る性能を達成しており、特にわずか179個のアノテーション付きサンプルしか必要としません。Qwen2.5-7Bをベースに構築されたREARANK-7Bは、ドメイン内およびドメイン外のベンチマークの両方でGPT-4に匹敵する性能を示し、推論集約的なBRIGHTベンチマークではGPT-4を凌駕することさえあります。これらの結果は、我々のアプローチの有効性を強調し、強化学習がリランキングにおけるLLMの推論能力をどのように強化できるかを示しています。

&lt;img src="https://arxiv.org/html/2505.20046v1/x1.png"/&gt;&lt;p&gt;Le Zhang Bo Wang Mila - Quebec AI Institute Université de Montréal Fudan University McGill University Canada CIFAR AI Chair&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20046v1</guid><pubDate>Mon, 26 May 2025 14:31:48 +0000</pubDate></item><item><title>Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers</title><link>http://arxiv.org/abs/2505.20128v1</link><description>大規模言語モデル（LLM）は、従来の手法を高度化するために情報検索に広く統合されてきました。しかし、複雑なタスクにおいてLLMが正確な知識を求めることを効果的に実現することは、マルチホップクエリの複雑さや、検索された無関係なコンテンツのために、依然として課題となっています。これらの制限に対処するために、我々はEXSEARCHというエージェント型検索フレームワークを提案します。ここでは、LLMは自己インセンティブ化されたプロセスを通じて、推論が展開するにつれて有用な情報を検索することを学習します。各ステップで、LLMは何を検索するかを決定し（思考）、外部リトリーバーをトリガーし（検索）、次のステップの推論をサポートするために、きめ細かい証拠を抽出します（記録）。LLMにこの能力を持たせるために、EXSEARCHは一般化された期待値最大化アルゴリズムを採用しています。Eステップでは、LLMは複数の検索軌跡を生成し、それぞれに重要度を割り当てます。Mステップでは、再重み付けされた損失関数を用いてそれらに対してLLMを訓練します。これにより、自己インセンティブ化されたループが作成され、LLMは自身が生成したデータから反復的に学習し、検索のために徐々に自己改善していきます。さらに、このトレーニングプロセスを理論的に分析し、収束保証を確立します。4つの知識集約型ベンチマークに関する広範な実験により、EXSEARCHがベースラインを大幅に上回ることが示されています。例えば、完全一致スコアで+7.8%の改善が見られます。これらの有望な結果に動機付けられ、我々はEXSEARCH-Zooを導入します。これは、将来の研究を促進するために、我々の手法をより広範なシナリオに拡張する拡張機能です。

&lt;img src="https://arxiv.org/html/2505.20128v1/x1.png"/&gt;&lt;p&gt;Amsterdam, Beijing, China Baidu. Inc, China Leiden University, Leiden, Qingdao, The Netherland, The Netherland University of Amsterdam, Zhengliang Shi Lingyong Yan Dawei Yin Shandong University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20128v1</guid><pubDate>Mon, 26 May 2025 15:27:55 +0000</pubDate></item><item><title>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</title><link>http://arxiv.org/abs/2505.19800v1</link><description>メタデータ抽出は、データセットのカタログ化と保存に不可欠であり、特に科学研究が指数関数的に増加している現状において、効果的な研究の発見と再現性を可能にする。Masader (Alyafeai et al., 2021) は、アラビア語NLPデータセットの学術論文から広範囲のメタデータ属性を抽出するための基礎を築いたが、手動アノテーションに大きく依存している。本稿では、大規模言語モデル（LLM）を活用して、アラビア語以外の言語のデータセットを扱う科学論文からメタデータ属性を自動的に抽出するフレームワークであるMOLEを紹介する。我々のスキーマ駆動型手法は、複数の入力形式にわたってドキュメント全体を処理し、一貫した出力を実現するために堅牢な検証メカニズムを組み込んでいる。さらに、このタスクにおける研究の進捗を評価するための新しいベンチマークを導入する。コンテキスト長、少数ショット学習、およびウェブブラウジング統合の体系的な分析を通じて、最新のLLMがこのタスクの自動化において有望な結果を示していることを実証し、一貫性と信頼性のあるパフォーマンスを確保するための今後のさらなる改善の必要性を強調する。研究コミュニティのために、コード（https://github.com/IVUL-KAUST/MOLE）とデータセット（https://huggingface.co/datasets/IVUL-KAUST/MOLE）を公開する。

&lt;img src="https://arxiv.org/html/2505.19800v1/x1.png"/&gt;&lt;p&gt;KFUPM, Zaid Alyafeai Maged S. Al-Shaibani Bernard Ghanem KAUST SDAIA-KFUPM Joint Research Center for AI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19800v1</guid><pubDate>Mon, 26 May 2025 10:31:26 +0000</pubDate></item><item><title>Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation</title><link>http://arxiv.org/abs/2505.19647v1</link><description>自動関連研究生成 (RWG) は、関連研究セクション (RWS) の草稿を作成する際に、その後の修正作業における時間と労力を節約できます。しかし、既存の RWG 手法は、参考文献論文の限られた部分を入力として使用することによる浅い理解と、参考文献間の関係性を効果的に捉えられないことによる各参考文献に対する孤立した説明という問題に常に悩まされています。これらの問題に対処するために、我々はフルテキストベースの RWG タスクに焦点を当て、新しいマルチエージェントフレームワークを提案します。我々のフレームワークは、論文のどのセクションを次に読むかを決定するセレクター、選択されたセクションを理解し、共有ワーキングメモリを更新するリーダー、そして最終的にキュレーションされたメモリに基づいて RWS を生成するライターという3つのエージェントで構成されています。参考文献間の関係性をより良く捉えるために、セレクターに対して2つのグラフ認識戦略も提案し、グラフ構造の制約下で読み込み順序を最適化できるようにします。広範な実験により、我々のフレームワークが一貫して3つのベースモデルと様々な入力構成において性能を向上させることが示されています。グラフ認識セレクターは、代替セレクターよりも優れた性能を発揮し、最先端の結果を達成しています。コードとデータは https://github.com/1190200817/Full_Text_RWG で入手できます。

&lt;img src="https://arxiv.org/html/2505.19647v1/x1.png"/&gt;&lt;p&gt;Beijing, China Engineering Research Center of Next-Generation Intelligent Search, Gaoling School of Artificial Intelligence, Intelligent Governance, Ministry of Education Beijing Key Laboratory of Research on Large Models, Recommendation, Renmin University of China&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19647v1</guid><pubDate>Mon, 26 May 2025 08:02:34 +0000</pubDate></item><item><title>MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning</title><link>http://arxiv.org/abs/2505.20131v1</link><description>分子編集は、構造的類似性を維持しながら、所望の化学的特性を最適化するために、与えられた分子を修正することを目的としています。しかし、現在のアプローチは通常、文字列ベースまたは連続的な表現に依存しており、分子の離散的でグラフ構造的な性質を適切に捉えることができず、構造的忠実度の制限や制御性の低下につながっています。本論文では、構造的制約と正確な特性最適化を明示的に統合した分子編集フレームワークであるMolEditRLを提案します。具体的には、MolEditRLは2つの段階で構成されています。（1）ソース構造と自然言語の指示に基づいてターゲット分子を再構築するように事前学習された離散グラフ拡散モデル、（2）グラフ制約下で編集の決定を明示的に最適化することにより、特性の整合性と構造の維持をさらに強化する編集認識型強化学習のファインチューニング段階。包括的な評価のために、単一および複数の特性タスクにまたがる10の化学的属性にわたる300万の多様な例を含む、最大かつ最も特性が豊富な分子編集データセットであるMolEdit-Instructを構築します。実験結果は、MolEditRLが特性最適化の精度と構造的忠実度の両方において最先端の手法を大幅に上回り、パラメータ数を98％削減しながら、編集成功率を74％向上させることを示しています。

&lt;img src="https://arxiv.org/html/2505.20131v1/extracted/6480378/figure/parameter.png"/&gt;&lt;p&gt;Astronautics, Dazhong Shen, Hong Kong University of Science, Nanjing University of Aeronautics, Technology, Technology (Guangzhou) College of Computer Science, Ying Sun Artificial Intelligence Thrust, Yuanxin Zhuang&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20131v1</guid><pubDate>Mon, 26 May 2025 15:29:08 +0000</pubDate></item><item><title>Unfolding AlphaFold's Bayesian Roots in Probability Kinematics</title><link>http://arxiv.org/abs/2505.19763v1</link><description>我々は、AlphaFold1の新しい理論的解釈を提示する。深層学習によるタンパク質構造予測におけるAlphaFold1の画期的なブレークスルーは、後のAlphaFold2やAlphaFold3のエンドツーエンドアーキテクチャとは対照的に、学習されたポテンシャルエネルギー関数に依存していた。このポテンシャルは当初、平均力ポテンシャル（PMF）を参照することで正当化されていたが、我々はAlphaFold1のポテンシャルを、確率運動学（ジェフリー条件付けとしても知られる）の一例として再解釈する。確率運動学は、従来のベイズ更新の原則的でありながら過小評価されている一般化である。確率運動学は、分割された確率の更新という形で、不確実またはソフトなエビデンスに対応する。この視点から、AlphaFold1のポテンシャルは、熱力学的ポテンシャルというよりも、一般化されたベイズ更新の一形態であることが明らかになる。我々の確率的フレームワークの範囲と精度を確認するために、AlphaFold1のアプローチを反映して、角度ランダムウォーク事前分布が確率運動学を介して距離に関するエビデンスで更新される合成2Dモデルを分析する。この理論的貢献は、AlphaFold1を、正当性の高いベイズ法のより広範なクラスに結び付け、単なるPMFに基づく定性的なヒューリスティクスを超える、正確な定量化を可能にする。より広く言えば、AlphaFold1の成果を考えると、確率運動学は確率的深層学習にとって大きな可能性を秘めている。なぜなら、少数のより単純なコンポーネントから複雑なモデルを構築できるからである。

&lt;img src="https://arxiv.org/html/2505.19763v1/extracted/6466724/AF1_3D_samples.png"/&gt;&lt;p&gt;Denmark, Kanti V. Mardia University of Leeds, Thomas Hamelryck University of Copenhagen, UK&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19763v1</guid><pubDate>Mon, 26 May 2025 09:46:07 +0000</pubDate></item><item><title>Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation</title><link>http://arxiv.org/abs/2505.19685v1</link><description>拡散モデルはグラフ生成のための強力な生成モデルとして登場しましたが、条件付きグラフ生成への利用は依然として根本的な課題です。特に、任意の報酬信号下でグラフ上の拡散モデルを誘導することは困難です。勾配ベースの手法は強力ですが、グラフの離散的かつ組み合わせ的な性質のため、しばしば不適切であり、微分不可能な報酬は勾配ベースの誘導をさらに複雑にします。我々は、この課題に対処するために、グラフ上の条件付き拡散を確率制御問題として解釈する新しい誘導フレームワークであるGraph Guided Diffusion (GGDiff)を提案します。GGDiffは、勾配ベースの誘導（微分可能な報酬の場合）、制御ベースの誘導（フォワード報酬評価からの制御信号を使用）、およびゼロ次近似（勾配ベースと勾配フリーの最適化を橋渡しする）を含む、複数の誘導戦略を統合します。この包括的なプラグアンドプレイフレームワークにより、微分可能および微分不可能な報酬関数の両方で、事前学習済みの拡散モデルのゼロショット誘導が可能になり、確立された誘導技術をグラフ生成に適応させます。これは、ほとんど探求されていない方向性です。我々の定式化は、計算効率、報酬アラインメント、およびサンプル品質のバランスを取り、多様な報酬タイプにわたる実用的な条件付き生成を可能にします。グラフモチーフ、公平性、およびリンク予測に関する制約など、さまざまなタスクでGGDiffの有効性を示し、多様性と忠実度を維持しながら、ターゲット報酬との優れたアラインメントを実現します。

&lt;img src="https://arxiv.org/html/2505.19685v1/extracted/6478939/images/teaser.png"/&gt;&lt;p&gt;Victor M. Tenorio King Juan Carlos University &amp;Nicolas Zilberstein Rice University Santiago Segarra Rice University &amp;Antonio G. Marques King Juan Carlos University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19685v1</guid><pubDate>Mon, 26 May 2025 08:45:22 +0000</pubDate></item><item><title>Energy-based generator matching: A neural sampler for general state space</title><link>http://arxiv.org/abs/2505.19646v1</link><description>エネルギー関数に基づく生成モデルを、データなしで学習するための、モダリティに依存しないアプローチであるEnergy-based generator matching (EGM)を提案します。最近提案されたgenerator matchingを拡張したEGMは、拡散、フロー、ジャンプなど、任意の連続時間マルコフ過程の学習を可能にし、連続、離散、および2つのモダリティの混合からデータを生成できます。この目的のために、自己正規化重要度サンプリングと、重要度の重みの分散を減らすための追加のブートストラップトリックを使用して、generator matching損失を推定することを提案します。EGMを、それぞれ最大100次元と20次元の離散タスクとマルチモーダルタスクの両方で検証します。

&lt;img src="https://arxiv.org/html/2505.19646v1/x1.png"/&gt;&lt;p&gt;Dongyeop Woo Minsu Kim Minkyu Kim Kiyoung Seong Sungsoo Ahn Korea Advanced Institute of Science, Technology (KAIST) Mila - Quebec AI Institute&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19646v1</guid><pubDate>Mon, 26 May 2025 08:02:29 +0000</pubDate></item><item><title>On scalable and efficient training of diffusion samplers</title><link>http://arxiv.org/abs/2505.19552v1</link><description>データが存在しない状況で、非正規化エネルギー分布からサンプリングするように拡散モデルを訓練するという課題、いわゆる拡散サンプラーに取り組む。これらのアプローチは有望ではあるものの、エネルギー評価が高価でサンプリング空間が高次元である、より要求の厳しいシナリオではスケールするのが難しい。この制限に対処するため、強力な古典的サンプリング手法と拡散サンプラーを適切に調和させる、スケーラブルでサンプル効率の高いフレームワークを提案する。具体的には、モンテカルロマルコフ連鎖（MCMC）サンプラーを、拡散サンプラーがほとんど訪れないモードの探索を補償するために補助エネルギー関数を用いた、新規性に基づく補助エネルギーを持つ探索器として利用し、オフポリシーサンプルを収集する。これらのオフポリシーサンプルをオンポリシーデータと組み合わせて拡散サンプラーを訓練し、エネルギーランドスケープのカバー範囲を拡大する。さらに、訓練中の初期の経験に対するサンプラーの優先度であるプライマシーバイアスを、訓練中のモード崩壊の主な原因として特定し、この問題を解決するために周期的な再初期化トリックを導入する。我々の手法は、拡散サンプラーの標準的なベンチマークにおけるサンプル効率を大幅に向上させ、より高次元の問題や現実世界の分子コンフォーマー生成においても優れている。

&lt;img src="https://arxiv.org/html/2505.19552v1/x1.png"/&gt;&lt;p&gt;Minkyu Kim Kiyoung Seong Dongyeop Woo Sungsoo Ahn Minsu Kim Korea Advanced Institute of Science, Technology (KAIST) Mila - Quebec AI Institute&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19552v1</guid><pubDate>Mon, 26 May 2025 06:16:34 +0000</pubDate></item><item><title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title><link>http://arxiv.org/abs/2505.19433v1</link><description>訓練後の圧縮は、大規模言語モデル（LLM）の計算コストとメモリコストを削減し、リソース効率の高いデプロイを可能にします。しかし、既存の圧縮ベンチマークは、言語モデリング（例えば、perplexity）や自然言語理解タスク（例えば、GLUEの精度）にのみ焦点を当てており、エージェントとしての能力、つまりワークフロー、ツール使用/関数呼び出し、長文脈理解、および現実世界への応用を無視しています。我々は、LLMのエージェント能力に対する圧縮の影響を評価するための最初の包括的なベンチマークであるAgent Compression Benchmark（ACBench）を紹介します。ACBenchは、（1）4つの能力にわたる12のタスク（例えば、ワークフロー生成のためのWorfBench、長文脈検索のためのNeedle-in-Haystack）、（2）量子化（GPTQ、AWQ）と枝刈り（Wanda、SparseGPT）、そして（3）小型（Gemma-2B）、標準（Qwen2.5 7B-32B）、蒸留された推論LLM（DeepSeek-R1-Distill）を含む15のモデルを網羅しています。我々の実験は、圧縮のトレードオフを明らかにしています。4ビット量子化は、ワークフロー生成とツール使用を維持しますが（1%-3%の低下）、現実世界への応用精度を10%-15%低下させます。我々は、分析を体系化するために、ERank、Top-kランキング相関、およびエネルギーを導入します。ACBenchは、エージェントシナリオにおけるLLM圧縮を最適化するための実用的な洞察を提供します。コードはhttps://github.com/pprp/ACBenchにあります。

&lt;img src="https://arxiv.org/html/2505.19433v1/extracted/6477732/figs/main_figure_v2.png"/&gt;&lt;p&gt;Bo Li, Lujun Li, Peijie Dong, Xiang Liu, Xiaowen Chu, Zhenheng Tang&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19433v1</guid><pubDate>Mon, 26 May 2025 02:49:07 +0000</pubDate></item><item><title>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</title><link>http://arxiv.org/abs/2505.19955v1</link><description>AIエージェントの最近の進歩は、科学的発見を推進し支援する可能性が高まっていることを示しています。本研究では、オープンエンドな機械学習研究におけるAIエージェントを評価するための包括的なベンチマークであるMLR-Benchを紹介します。MLR-Benchは、(1) NeurIPS、ICLR、ICMLのワークショップから収集された、多様なMLトピックを網羅する201の研究タスク、(2) 研究の質を評価するために、LLMベースのレビューアと慎重に設計された評価基準を組み合わせた自動評価フレームワークであるMLR-Judge、(3) アイデア生成、提案策定、実験、論文執筆の4つの段階を経て研究タスクを完了できるモジュール型エージェントの足場であるMLR-Agentの3つの主要なコンポーネントで構成されています。私たちのフレームワークは、これらの異なる研究段階にわたる段階的な評価と、最終的な研究論文のエンドツーエンド評価の両方をサポートしています。次に、MLR-Benchを使用して、6つの最先端のLLMと高度なコーディングエージェントを評価した結果、LLMは一貫性のあるアイデアと構造化された論文を生成するのに効果的ですが、現在のコーディングエージェントは頻繁に（例えば、80％のケースで）捏造または無効化された実験結果を生成することがわかりました。これは、科学的信頼性に対する大きな障壁となっています。MLR-Judgeを人間の評価を通じて検証し、専門家のレビューアとの高い一致を示し、研究評価のためのスケーラブルなツールとしての可能性を裏付けています。信頼できる透明性の高い科学的発見に向けて、AI研究エージェントのベンチマーク、診断、改善を支援するために、MLR-Benchをオープンソース化します。

&lt;img src="https://arxiv.org/html/2505.19955v1/x1.png"/&gt;&lt;p&gt;Design, Hui Chen Miao Xiong National University of Singapore University of California, Santa Barbara Singapore University of Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19955v1</guid><pubDate>Mon, 26 May 2025 13:18:37 +0000</pubDate></item><item><title>ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows</title><link>http://arxiv.org/abs/2505.19897v1</link><description>大規模言語モデル（LLM）は、自然言語処理の領域を超えて影響力を拡大し、学際的な研究の発展を大きく促進しています。近年、様々なLLMベースのエージェントが開発され、複数の側面と領域にわたる科学的発見の進展を支援しています。中でも、人間のようにオペレーティングシステムと対話できるコンピュータ利用エージェントは、科学的な問題解決の自動化や、研究者のワークフローにおけるルーチン作業への対応を可能にする道を切り開いています。これらのエージェントの変革的な可能性を認識し、私たちはScienceBoardを導入します。これは、（i）専門的なソフトウェアが統合された、動的で視覚的に豊かな科学的ワークフローを特徴とする、現実的なマルチドメイン環境です。ここでは、エージェントが異なるインターフェースを介して自律的に対話することで、複雑な研究タスクや実験を加速できます。そして、（ii）生化学、天文学、地球情報学などの分野における科学的発見ワークフローにまたがる、人間によって厳選された169の高品質で厳密に検証された現実世界のタスクからなる、挑戦的なベンチマークです。最先端のバックボーン（例：GPT-4o、Claude 3.7、UI-TARS）を備えたエージェントの広範な評価では、有望な結果が見られるものの、複雑なワークフローにおいて科学者を確実に支援するにはまだ不十分であり、全体的な成功率はわずか15％であることが示されています。詳細な分析により、現在のエージェントの限界に対処し、より効果的な設計原則を策定するための貴重な洞察が得られ、科学的発見のためのより有能なエージェントの構築への道が開かれます。私たちのコード、環境、およびベンチマークは、https://qiushisun.github.io/ScienceBoard-Home/ にあります。

&lt;img src="https://arxiv.org/html/2505.19897v1/x1.png"/&gt;&lt;p&gt;Chang Ma et. al Department of Computer Science Cranberry-Lemon University Pittsburgh, PA 15213, Qiushi Sun, The University of Hong Kong Shanghai AI Laboratory Fudan University Peking University Nanjing University East China Normal University Yale University, Zhoumianze Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19897v1</guid><pubDate>Mon, 26 May 2025 12:27:27 +0000</pubDate></item><item><title>Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI</title><link>http://arxiv.org/abs/2505.19443v1</link><description>本レビューでは、AI支援ソフトウェア開発における2つの新たなパラダイム、すなわち「バイブコーディング」と「エージェントコーディング」について包括的な分析を行います。どちらも大規模言語モデル（LLM）を活用していますが、自律性、アーキテクチャ設計、開発者の役割において根本的に異なります。バイブコーディングは、アイデア出し、実験、創造的な探求をサポートするプロンプトベースの会話型ワークフローを通じて、直感的で人間中心のインタラクションを重視します。対照的に、エージェントコーディングは、最小限の人的介入でタスクを計画、実行、テスト、反復できる目標駆動型エージェントを通じて、自律的なソフトウェア開発を可能にします。概念的基礎、実行モデル、フィードバックループ、安全メカニズム、デバッグ戦略、および現実世界のツールエコシステムに及ぶ詳細な分類法を提案します。比較ワークフロー分析と20の詳細なユースケースを通じて、バイブシステムが初期段階のプロトタイピングと教育でどのように成功し、エージェントシステムがエンタープライズグレードの自動化、コードベースのリファクタリング、およびCI/CD統合でどのように優れているかを説明します。さらに、自然言語インターフェースが自律的な実行パイプラインと結合されたハイブリッドアーキテクチャにおける新たなトレンドを検証します。最後に、信頼性、説明可能性、および協調的なシステムに必要なインフラストラクチャを概説し、エージェントAIの将来のロードマップを明確にします。私たちの調査結果は、AIソフトウェアエンジニアリングの成功は、1つのパラダイムを選択するのではなく、統一された人間中心の開発ライフサイクル内でそれらの強みを調和させることに依存することを示唆しています。

&lt;img src="https://arxiv.org/html/2505.19443v1/x1.png"/&gt;&lt;p&gt;Konstantinos I. Roumeliotis2, Manoj Karkee13, Ranjan Sapkota13&lt;/p&gt;&lt;p&gt;1Cornell University, Department of Biological and Environmental Engineering, USA
2University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece&lt;/p&gt;</description><guid isPermaLink="false">2505.19443v1</guid><pubDate>Mon, 26 May 2025 03:00:21 +0000</pubDate></item><item><title>MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2505.20096v1</link><description>我々は、複雑な情報探索タスクに内在する曖昧さと推論の課題に対処する、Retrieval-Augmented Generation (RAG)のためのマルチエージェントフレームワークであるMA-RAGを提案します。エンドツーエンドのファインチューニングや、個別のコンポーネントの強化に依存する従来のRAG手法とは異なり、MA-RAGは、プランナー、ステップ定義エージェント、抽出エージェント、QAエージェントといった、専門化されたAIエージェントの協調的なセットを編成し、タスク認識型の推論を用いてRAGパイプラインの各段階に取り組みます。曖昧さは、不明確なクエリ、検索されたドキュメント内の乏しいまたは間接的な証拠、あるいは複数のソースに散在する情報を統合する必要性から生じる可能性があります。MA-RAGは、クエリの曖昧さ解消、証拠抽出、回答合成などのサブタスクに問題を分解し、思考連鎖プロンプトを備えた専用のエージェントにそれらを割り当てることで、これらの課題を軽減します。これらのエージェントは、中間的な推論を伝達し、検索と合成のプロセスを段階的に洗練します。我々の設計により、モデルのファインチューニングなしに、情報フローをきめ細かく制御できます。重要なことに、エージェントはオンデマンドで呼び出され、不要な計算を回避する動的で効率的なワークフローを可能にします。このモジュール式で推論主導のアーキテクチャにより、MA-RAGは堅牢で解釈可能な結果を提供できます。マルチホップおよび曖昧なQAベンチマークでの実験は、MA-RAGが最先端のトレーニングフリーのベースラインを上回り、ファインチューニングされたシステムに匹敵することを示しており、RAGにおける協調的なエージェントベースの推論の有効性を検証しています。

&lt;img src="https://arxiv.org/html/2505.20096v1/x1.png"/&gt;&lt;p&gt;Thang Nguyen Dartmouth College &amp;Peter Chin Dartmouth College &amp;Yu-Wing Tai Dartmouth College&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20096v1</guid><pubDate>Mon, 26 May 2025 15:05:18 +0000</pubDate></item><item><title>PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints</title><link>http://arxiv.org/abs/2505.19842v1</link><description>大気質予測（AQF）は、公衆衛生と環境管理にとって非常に重要ですが、排出、気象、化学変化の複雑な相互作用により、依然として困難な課題です。CMAQやWRF-Chemなどの従来の数値モデルは、物理的に根拠のあるシミュレーションを提供しますが、計算コストが高く、不確実な排出インベントリに依存しています。深層学習モデルは、計算効率が良い一方で、物理的な制約がないため、汎化に苦労することがよくあります。このギャップを埋めるために、数値モデリングの原則と深層学習を統合した代替モデルであるPCDCNetを提案します。PCDCNetは、汚染物質の生成、輸送、拡散をモデル化するために、排出、気象の影響、およびドメインに基づいた制約を明示的に組み込んでいます。グラフベースの空間輸送モデリング、時間的蓄積のための再帰的構造、および局所的な相互作用のための表現の強化を組み合わせることで、PCDCNetは、72時間のステーションレベルのPM2.5およびO3予測において、最先端（SOTA）のパフォーマンスを達成し、計算コストを大幅に削減します。さらに、当社のモデルはオンラインプラットフォームに展開されており、無料のリアルタイム大気質予測を提供し、そのスケーラビリティと社会的影響を示しています。深層学習を物理的な整合性と整合させることで、PCDCNetはAQFのための実用的で解釈可能なソリューションを提供し、個人および規制の両方のアプリケーションのための情報に基づいた意思決定を可能にします。

&lt;img src="https://arxiv.org/html/2505.19842v1/x3.png"/&gt;&lt;p&gt;Jiang Zhang, Jingfang Fan, Lothar Thiele, Olga Saukh, Qingye Meng, Shuo Wang, Xingyuan Yuan, Yuanting Zhang, Yun Cheng&lt;/p&gt;&lt;p&gt;ColorfulClouds Technology Co.,Ltd. Beijing China
Complexity Science Hub Vienna Austria
D-ITET, ETH Zurich Zurich Switzerland
Graz University of Technology Graz Austria
Potsdam Institute for Climate Impact Research Potsdam Germany
School of Systems Science / Institute of Nonequilibrium Systems Beijing Normal University Beijing China
School of Systems Science Beijing Normal University Beijing China
Swiss Data Science Center ETH Zurich Zurich Switzerland&lt;/p&gt;</description><guid isPermaLink="false">2505.19842v1</guid><pubDate>Mon, 26 May 2025 11:27:07 +0000</pubDate></item><item><title>CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation</title><link>http://arxiv.org/abs/2505.19502v1</link><description>コードスニペットの信頼できる評価方法は、ニューラルコード生成において重要な役割を果たします。従来の方法は、参照解に依存するか、実行可能なテストケースを必要とするかのいずれかであり、柔軟性と拡張性に本質的な限界があります。最近のLLM-as-Judge（LLMを審査員とする）手法は、問題記述と生成されたコード間の機能的な一貫性を直接評価することで、有望な代替手段を提供します。これらのLLM-as-Judge手法の全体像を体系的に理解するために、3つの多様なデータセットにわたって包括的な実証研究を実施しました。私たちの調査により、LLM-as-Judge手法の2つのカテゴリの長所と短所が明らかになりました。汎用的な基盤モデルに基づく手法は、優れたパフォーマンスを達成できますが、複雑なプロンプトが必要であり、説明可能性に欠けます。一方、推論基盤モデルに基づく手法は、よりシンプルなプロンプトで優れた説明可能性を提供しますが、パラメータサイズが大きいため、かなりの計算リソースを必要とします。これらの制限に対処するために、精度、効率、説明可能性のバランスを取る新しいコード評価方法であるCODE-DITINGを提案します。DeepSeek-R1671BからCODE-DITING 1.5Bおよび7Bモデルに推論能力を効果的に転送し、評価の説明可能性を大幅に向上させ、計算コストを削減するデータ蒸留フレームワークを開発しました。推論プロセスにおける多数決戦略により、CODE-DITING 1.5Bは、同じ規模のパラメータを持つすべてのモデルを上回り、通常は5倍のパラメータスケールを持つモデルで見られるパフォーマンスを達成します。CODE-DITING 7Bは、GPT-4oおよびDeepSeek-V3 671Bを上回りますが、これらの大規模モデルのパラメータボリュームのわずか1％しか使用していません。さらに実験により、CODEDITINGは好みリークに対して堅牢であり、コード評価の有望な代替手段として機能することが示されています。

&lt;img src="https://arxiv.org/html/2505.19502v1/x1.png"/&gt;&lt;p&gt;David Lo6, Guang Yang2, Taolue Chen81, Wei Zheng4, Xiang Chen3, Xin Zhou6, Xing Hu5, Yu Zhou21&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19502v1</guid><pubDate>Mon, 26 May 2025 04:29:14 +0000</pubDate></item><item><title>Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs</title><link>http://arxiv.org/abs/2505.19489v1</link><description>Linuxカーネルは、数多くのシステムの基盤となる重要なシステムです。Linuxカーネルのバグは、数十億のユーザーに影響を与える深刻な結果を引き起こす可能性があります。ソフトウェア内のバグのあるコード要素を特定することを目的とするFault Localization（FL、故障箇所特定）は、ソフトウェア品質保証において不可欠な役割を果たします。最近のLLMエージェントは、SWE-benchのような最近のベンチマークでFLにおいて有望な精度を達成していますが、大規模なコードベース、限られた可観測性、多様な影響要因によりFLがはるかに困難なLinuxカーネルで、これらの手法がどの程度うまく機能するかは不明なままです。本論文では、実際のLinuxカーネルのバグから構築されたFLベンチマークであるLinuxFLBenchを紹介します。最先端のLLMエージェントのLinuxカーネルにおける性能を評価するための実証的研究を実施します。初期の結果から、既存のエージェントはこのタスクに苦戦しており、ファイルレベルでの最高のtop-1精度はわずか41.6%であることが明らかになりました。この課題に対処するために、LinuxカーネルにおけるLLMエージェントのFL効果を向上させるように設計された拡張フレームワークであるLinuxFL$^+$を提案します。LinuxFL$^+$は、最小限のコストで、調査対象のすべてのエージェントのFL精度を大幅に向上させます（例：7.2%〜11.2%の精度向上）。データとコードはhttps://github.com/FudanSELab/LinuxFLBenchで入手できます。

&lt;img src="https://arxiv.org/html/2505.19489v1/extracted/6477688/fig/bug_product_distribution.png"/&gt;&lt;p&gt;Zhenhao Zhou Fudan University &amp;Zhuochen Huang Fudan University Yike He Fudan University &amp;Chong Wang Nanyang Technological University &amp;Jiajun Wang Fudan University &amp;Yijian Wu Fudan University &amp;Xin Peng Fudan University &amp;Yiling Lou Fudan University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19489v1</guid><pubDate>Mon, 26 May 2025 04:15:48 +0000</pubDate></item><item><title>SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale</title><link>http://arxiv.org/abs/2505.20094v1</link><description>科学的なシミュレーションシステムは、物理的に一貫性があり、設計段階から解釈可能で、かつ様々な条件下でスケーラブルであり得るだろうか？数十年にわたる進歩にもかかわらず、この三位一体は依然として実現困難である。動的モンテカルロ法のような古典的な手法は、熱力学的な精度を保証するが、スケーラビリティに劣る。学習ベースの手法は効率性を提供するが、物理的な一貫性と解釈可能性を犠牲にすることが多い。我々は、原子スケールのシミュレーションを物理的に根拠のある群知能システムとして再構築する強化学習フレームワークであるSwarmThinkersを提案する。拡散する各粒子は、熱力学的制約下で訓練された共有ポリシーネットワークを介して遷移を選択する、ローカルな意思決定エージェントとしてモデル化される。再重み付けメカニズムは、学習された選好を遷移速度と融合させ、統計的な忠実度を維持しながら、解釈可能な段階的な意思決定を可能にする。訓練は、集中型訓練・分散型実行のパラダイムに従い、ポリシーが再訓練なしにシステムサイズ、濃度、温度にわたって一般化できるようにする。放射線誘起Fe-Cu合金析出をシミュレートするベンチマークにおいて、SwarmThinkersは、以前はスーパーコンピュータ上のOpenKMCでのみ達成可能であったフルスケールで物理的に一貫性のあるシミュレーションを、単一のA100 GPU上で初めて実現したシステムである。最大4963倍（平均3185倍）高速な計算と、485倍低いメモリ使用量を提供する。SwarmThinkersは、粒子を受動的なサンプラーではなく意思決定者として扱うことで、科学的シミュレーションにおけるパラダイムシフトを示す。それは、エージェント駆動の知能を通じて、物理的な一貫性、解釈可能性、およびスケーラビリティを統合するものである。

&lt;img src="https://arxiv.org/html/2505.20094v1/x1.png"/&gt;&lt;p&gt;Qi Li University of Science, Technology of China
&amp;Kun Li Microsoft Research
&amp;Haozhi Han Peking University
&amp;Honghui Shang University of Science, Technology of China
&amp;Ting Cao Microsoft Research
&amp;Mao Yang Microsoft Research, Technology of China
&amp;Xinfu He China Institute of Atomic Energy
&amp;Yunquan Zhang Chinese Academy of Sciences
&amp;Hong An University of Science&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20094v1</guid><pubDate>Mon, 26 May 2025 15:04:37 +0000</pubDate></item><item><title>TCP: a Benchmark for Temporal Constraint-Based Planning</title><link>http://arxiv.org/abs/2505.19927v1</link><description>大規模言語モデル(LLM)にとって、時間的推論と計画は不可欠な能力ですが、既存のベンチマークのほとんどは、それらを個別に、かつ限定的な複雑さの下で評価しています。このギャップを埋めるために、時間的制約に基づく計画(TCP)ベンチマークを導入し、両方の能力を共同で評価します。TCPの各インスタンスは、共同プロジェクトに関する自然な対話で構成され、多様で相互依存的な時間的制約が明示的または暗黙的に表現されており、モデルはすべての制約を満たす最適なスケジュールを推論する必要があります。TCPを構築するために、まず抽象的な問題のプロトタイプを生成し、それをさまざまなドメインからの現実的なシナリオと組み合わせ、LLMを使用して対話形式に豊かにします。ベンチマークの信頼性を確認するために、サンプリングされたサブセットに対して人間の品質チェックを実施します。最先端のLLMを評価した結果、最も強力なモデルでさえTCPに苦戦しており、その難しさを示すとともに、LLMの時間的制約に基づく計画能力の限界を明らかにしています。根本的な失敗事例を分析し、ベンチマークをオープンソース化し、私たちの発見が今後の研究を刺激することを願っています。

&lt;img src="https://arxiv.org/html/2505.19927v1/extracted/6473176/figure/data_examples_improved.png"/&gt;&lt;p&gt;Sikuan Yan, University of Cambridge LMU Munich Amazon University of Oxford, Zhangdie Yuan, Zifeng Ding&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19927v1</guid><pubDate>Mon, 26 May 2025 12:53:01 +0000</pubDate></item><item><title>EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM</title><link>http://arxiv.org/abs/2505.19905v1</link><description>大規模言語モデル（LLM）は、テキストベースの推論および計画タスクにおいて優れた能力を示すものの、ロボット制御への実装は、重大な欠点によって制約されています。（1）LLMエージェントは、主に視覚的な条件ではなく、テキストによる入力で動作するように設計されています。（2）現在のマルチモーダルエージェントは、LLMを静的なプランナーとして扱い、その推論を環境のダイナミクスから分離するため、ドメイン固有の知識を考慮しないアクションにつながります。（3）LLMは、視覚的なインタラクションから学習するように設計されていないため、特定のドメインに適したポリシーを作成することが困難になります。本論文では、双方向のトレーニングパラダイムを通じてLLMとVLMを協調的に統合する、具現化されたマルチモーダルエージェントであるEMAC+を紹介します。既存の手法とは異なり、EMAC+は、VLMが低レベルの視覚制御タスクを実行することによって得られるリアルタイムのフィードバックを使用して、LLMによって生成された高レベルのテキストプランを動的に洗練します。LLMが静的なシンボリックマッピングのみに依存するのではなく、インタラクティブな経験を通じて視覚的な環境のダイナミクスを直接内部化できるようにすることで、以前のモデルの重大な制限に対処します。ALFWorldおよびRT-1ベンチマークでの広範な実験的評価により、EMAC+が優れたタスクパフォーマンス、ノイズの多い観測に対するロバスト性、および効率的な学習を達成することが示されています。また、徹底的なアブレーションスタディを実施し、成功例と失敗例の詳細な分析を提供します。

&lt;img src="https://arxiv.org/html/2505.19905v1/x1.png"/&gt;&lt;p&gt;Flora D. Salim, Shuang Ao, Simon Khan University of New South Wales, Sydney ; Air Force Research Laboratory&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19905v1</guid><pubDate>Mon, 26 May 2025 12:34:16 +0000</pubDate></item><item><title>Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition</title><link>http://arxiv.org/abs/2505.19788v1</link><description>大規模推論モデル（LRM）は、最終的な答えを導き出すための過度に冗長な思考の連鎖（CoT）が、最初のトークンまでの遅延と全体的な遅延を招いていると批判されています。通常、LRMのCoTは複数の思考ユニットを混在させており、各ユニットは元のクエリに対する候補となる答えを生成しようとします。したがって、効率を改善するための自然なアイデアは、ユニット数を減らすことです。しかし、従来のCoTにおける思考ユニットは明示的に管理できないため、それを行うことは困難です。本論文では、このギャップを埋めるために、従来のCoTを明示的、構造化された、ターンごとのインタラクションのシーケンスにデコードする多段階分解（MinD）を紹介します。MinDでは、モデルはクエリに対して多段階の応答を提供し、各ターンは思考ユニットを含み、対応する答えを生成します。後続のターンでは、以前のターンの思考部分と解答部分の両方について、反省、検証、修正、または代替アプローチの探索を行うことができます。これにより、より迅速に答えを提供できるだけでなく、反復的な推論プロセスを明示的に制御できます（つまり、ユーザーは任意のターンで停止または続行できます）。MinDを実現するために、教師ありファインチューニング（SFT）と強化学習（RL）のパラダイムに従います。まず、別のLLMにプロンプトを与えることで、LRMの出力を多段階形式に言い換え、次にそのデータでLRMを調整します。調整されたモデルは、元のモデルよりもさらに多くのトークンを消費する傾向がある（おそらく、多段階形式が追加の解答トークンを導入するため）ことを観察し、GRPOのようなRLアルゴリズムを活用して、より少ないターンで正しい出力を優先することを提唱します。R1-Distillモデルを使用してMATHデータセットでトレーニングされたMinDは、出力トークンの使用量と最初のトークンまでの時間（TTFT）の両方を最大約70％削減しながら、MATH-500、AIME24、AMC23、GPQA-Diamondなどの推論ベンチマークで競争力のあるパフォーマンスを維持できます。

&lt;img src="https://arxiv.org/html/2505.19788v1/x1.png"/&gt;&lt;p&gt;Boxiu Li, Hao Zhang, San Diego, Xuyao Huang, Zihao Zeng, and Zhijie Deng Shanghai Jiao Tong University RealAI University of California&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19788v1</guid><pubDate>Mon, 26 May 2025 10:18:57 +0000</pubDate></item><item><title>Language Model-Enhanced Message Passing for Heterophilic Graph Learning</title><link>http://arxiv.org/abs/2505.19762v1</link><description>従来のグラフニューラルネットワーク（GNN）は、同質性に基づいたメッセージパッシングに依存しており、接続されたノードが類似しない特徴や異なるラベルを示す異質性グラフでは苦戦します。既存の手法は、グラフ構造の改良や近傍集約関数の適応を通じて異質性に対処していますが、ノードテキストの持つ意味的な可能性を見過ごしたり、伝播に最適でないメッセージ表現に頼ったり、同質性グラフでの性能を損なったりすることがよくあります。これらの限界に対処するため、我々は異質性グラフ学習のための新しい言語モデル（LM）強化型メッセージパッシング手法（LEMP4HG）を提案します。具体的には、テキスト属性付きグラフの文脈において、LMにノードペアのテキストを提供し、それらの接続分析を生成させ、それをエンコードし、ゲーティング機構を通じてノードペアのテキスト埋め込みと融合します。合成されたメッセージは意味的に豊かになり、両方のノードの情報と適応的にバランスが取れるため、異質性領域での近傍集約における矛盾した信号を軽減します。さらに、我々はヒューリスティックなMVRD（信頼できる距離の変調された変動）によって導かれるアクティブラーニング戦略を導入し、メッセージパッシングで最も苦しむノードペアを選択的に強化することで、分析生成のコストと、同質性領域への副作用を軽減します。広範な実験により、我々のアプローチが異質性グラフで優れており、グラフ畳み込みネットワーク（GCN）バックボーンと実用的な予算で、同質性グラフでも堅牢に機能することが検証されています。

&lt;img src="https://arxiv.org/html/2505.19762v1/x1.png"/&gt;&lt;p&gt;China, China &amp;Dawei Cheng Tongji University Shanghai, Wenjun Wang Tongji University Shanghai&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19762v1</guid><pubDate>Mon, 26 May 2025 09:45:16 +0000</pubDate></item><item><title>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning</title><link>http://arxiv.org/abs/2505.19761v1</link><description>大規模言語モデル（LLM）は高度な推論能力を示す一方で、特に疎な報酬シナリオにおいて、探索の不足と長期的な貢献度評価の困難さから、長期的な意思決定タスクには依然として苦戦しています。分割統治の原則に触発され、私たちは、LLMポリシーにパラメータ効率が良く、汎用的に適用可能な階層構造を導入する革新的なフレームワーク**GLIDER**（**G**rounding **L**anguage Models as Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical **R**einforcement Learning：オフライン階層型強化学習による効率的な意思決定エージェントとしての言語モデルの基礎付け）を提案します。このフレームワークでは、低レベルのコントローラーは、高レベルのポリシーによって学習され指示される、抽象的で段階的な計画によって監督されます。この設計により、複雑な問題は一連の首尾一貫した思考連鎖推論のサブタスクに分解され、柔軟な時間的抽象化を提供することで、長期的なタスクにおける探索と学習を大幅に強化します。さらに、GLIDERは、タスクに依存しない低レベルスキルの強力な転移性により、非定常環境への迅速なオンライン適応を促進します。ScienceWorldおよびALFWorldベンチマークでの実験では、GLIDERが一貫したパフォーマンス向上を達成し、汎化能力も向上することが示されています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19761v1</guid><pubDate>Mon, 26 May 2025 09:43:40 +0000</pubDate></item><item><title>Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting</title><link>http://arxiv.org/abs/2505.19716v1</link><description>既存のChain-of-Thought（CoT）蒸留法は、推論能力をベースモデルに効果的に伝達できますが、2つの主要な制限があります。それは、推論トレースの過剰な冗長性と、問題の難易度に対する不十分な適応性です。長い推論トレースは推論コストを大幅に増加させ、均一な長さの解法はベースモデルが適応的な推論戦略を学習することを妨げます。これらの問題に対処するために、パフォーマンスを損なうことなく動的に推論トレースを短縮する、難易度認識プロンプティング（DAP）法を提案します。我々のアプローチでは、まず大規模な教師モデルが各問題の難易度を判断し、その後、その推論トレースを適切な短い長さに書き換えて、簡潔でありながら完全な推論トレースを生成します。DAPパイプラインを活用して、平均わずか720トークン（典型的なCoTよりも桁違いに短い）の解法を含む、10万件の簡潔な推論例からなる蒸留データセットLiteCoTをキュレーションしました。LiteCoTを使用して、Qwen2.5アーキテクチャに基づいて、新しい推論モデルファミリーLiter（1.5B、7B、および32B）を蒸留しました。実験の結果、わずか10万件の難易度で剪定されたCoTサンプルでファインチューニングされた学生モデルが、80万件の元のLong CoTサンプルで蒸留されたモデルよりも優れた性能を発揮し、トレーニングと推論のコストを大幅に削減することが示されました。我々の手法は汎用性も高く、11の多様なベンチマークにおいて、短い難易度認識CoTは、はるかに少ないトークンを使用しながら、Longチェーンと同等またはそれ以上の精度を達成しています。たとえば、難易度の高いAIME24試験では、我々のアプローチはわずか約5Kの推論トークンを使用して$74.2\%$のPass@1を達成し、より多くのトークンを消費する他の手法を上回っています。我々のコードとデータはhttps://github.com/Evanwu1125/LiteCoTで入手できます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19716v1</guid><pubDate>Mon, 26 May 2025 09:04:44 +0000</pubDate></item><item><title>Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models</title><link>http://arxiv.org/abs/2505.19676v1</link><description>大規模言語モデル（LLM）が自動定理証明器（ATP）の推論戦略を利用する能力を検証するための経験的手法について研究します。2023年12月と2024年8月の最先端モデルの性能を、PRONTOQAのsteamroller推論問題で評価します。そのために、LLMの応答精度と正答相関を評価する方法を開発します。

我々の結果は、LLMの推論能力の向上は過去9ヶ月間停滞していることを示しています。完了トークンを追跡することで、GPT-4のリリース以降の推論能力の向上のほとんどが、隠されたシステムプロンプト、またはモデルが汎用的なChain of Thoughtプロンプト戦略を自動的に使用するように訓練されたことに起因すると考えられることを示します。試したATP推論戦略の中で、現在の最先端LLMはボトムアップ（フォワードチェイニングとも呼ばれる）戦略に最も従うことができることがわかりました。LLMの応答に正しい推論が含まれていることと、正しい結論に到達することの間には、低い正の相関関係が見られました。

&lt;img src="https://arxiv.org/html/2505.19676v1/extracted/6478875/MermaidDiagram.png"/&gt;&lt;p&gt;Lachlan McGinness, Peter Baumgartner&lt;/p&gt;&lt;p&gt;Data61, CSIRO
School of Computer Science, Australian National University and CSIRO&lt;/p&gt;</description><guid isPermaLink="false">2505.19676v1</guid><pubDate>Mon, 26 May 2025 08:34:07 +0000</pubDate></item><item><title>Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions</title><link>http://arxiv.org/abs/2505.19501v1</link><description>この短い報告書では、ゲノミクス分野に特化した自動化パイプラインを紹介し、ゲノム工学に関する10年以上にわたる科学フォーラムの議論から構築された新しいベンチマークである\textit{Genome-Bench}を紹介します。私たちのパイプラインは、生のインタラクションを、基礎生物学、実験のトラブルシューティング、ツールの使用法など、3000件以上の高品質な質問と回答のペアでサポートされた、強化学習に適した多肢選択式の質問形式に変換します。私たちの知る限り、これは科学的な議論からLLMに推論を教えるための最初のエンドツーエンドのパイプラインであり、生物学以外の科学分野全体への一般化に有望な可能性を秘めています。

&lt;img src="https://arxiv.org/html/2505.19501v1/x1.png"/&gt;&lt;p&gt;Ming Yin Princeton University &amp;Yuanhao Qu Stanford University &amp;Dyllan Liu UC Berkerly Ling Yang Princeton University &amp;Le Cong Stanford University &amp;Mengdi Wang Princeton University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19501v1</guid><pubDate>Mon, 26 May 2025 04:28:46 +0000</pubDate></item><item><title>Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models</title><link>http://arxiv.org/abs/2505.19490v1</link><description>複雑なコンピュータ支援設計（CAD）モデルの設計は、計算効率の悪さや正確なモデルの生成の難しさなどの課題により、しばしば時間がかかります。これらの問題に対処するため、大規模言語モデル（LLM）とコンピュータ自動設計（CAutoD）を統合した、産業設計自動化のための新しい言語誘導型フレームワークを提案します。このフレームワークを通じて、CADモデルはパラメータと外観記述から自動的に生成され、詳細なCAD設計段階における設計タスクの自動化をサポートします。我々のアプローチは、3つの主要な革新を導入します。（1）LLMと視覚言語大規模モデル（VLLM）を活用して高品質なパラメータと外観記述を生成する、半自動データアノテーションパイプライン、（2）二重チャネル特徴集約を通じてモデリングシーケンスを予測する、TransformerベースのCADジェネレーター（TCADGen）、（3）TCADGenからの信頼度スコアを組み込むことで生成されたシーケンスを洗練するように設計された、CADLLMと呼ばれる強化されたCADモデリング生成モデル。実験結果は、提案されたアプローチが精度と効率の両方で従来の方法を上回り、産業ワークフローの自動化とテキストプロンプトからの複雑なCADモデルの生成のための強力なツールを提供することを示しています。コードはhttps://jianxliao.github.io/cadllm-page/で入手できます。

&lt;img src="https://arxiv.org/html/2505.19490v1/x1.png"/&gt;&lt;p&gt;518000, China i4AI Ltd, London WCIN3AX, Shenzhen, Shenzhen Institute for Advanced Study, Technology of China, United Kingdom Shui.Yu@i4AI.org, University of Electronic Science, Yun.Li@ieee.org&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19490v1</guid><pubDate>Mon, 26 May 2025 04:17:51 +0000</pubDate></item><item><title>Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning</title><link>http://arxiv.org/abs/2505.19442v1</link><description>制御可能なコード生成、つまり指定されたスタイルに従いながら機能を維持するコードを合成する能力は、依然として困難な課題です。我々は、柔軟なスタイル制御を可能にするために、コントラスト学習と条件付きデコーディングを組み合わせた二段階の学習フレームワークを提案します。第一段階では、コードスタイルの表現を意味的および構造的な特徴と整合させます。第二段階では、学習されたスタイルベクトルを条件として、言語モデル（例：Flan-T5）をファインチューニングし、生成を誘導します。我々の手法は、軽量な混合を通じて、スタイルの補間とユーザーのパーソナライズをサポートします。従来の研究と比較して、我々の統一されたフレームワークは、コードの正確性を犠牲にすることなく、改善されたスタイルの制御を提供します。これは、スタイルガイドされたコード生成のために、コントラスト整合と条件付きデコーディングを組み合わせた最初のアプローチの一つです。

&lt;img src="https://arxiv.org/html/2505.19442v1/frog.jpg"/&gt;&lt;p&gt;You&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19442v1</guid><pubDate>Mon, 26 May 2025 03:00:20 +0000</pubDate></item><item><title>SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows</title><link>http://arxiv.org/abs/2505.19619v1</link><description>深層生成モデルは近年、物理学から化学に至るまで、様々な分野で大きな注目を集めています。これらの分野では、規格化されていないボルツマンのような分布からのサンプリングが根本的な課題となっています。特に、自己回帰モデルと正規化フローは、閉形式の確率密度を生成できる魅力的な能力から、顕著な存在となっています。さらに、対称性などの事前知識を深層ニューラルネットワークに組み込むことで、学習性能を大幅に向上させることができることはよく知られています。このような背景から、最近の進歩は、対称性共変生成モデルの開発に焦点を当てており、目覚ましい成果を上げています。これらの基盤の上に、本論文では、Symmetry-Enforcing Stochastic Modulation (SESaMo) を提案します。共変正規化フローと同様に、SESaMoは、確率的変調と呼ばれる新しい技術を通じて、帰納的バイアス（例えば、対称性）を正規化フローに組み込むことを可能にします。このアプローチは、生成モデルの柔軟性を高め、様々な厳密な対称性および破れた対称性を効果的に学習することを可能にします。我々の数値実験では、8-Gaussian混合モデルや、$\phi^4$理論やHubbardモデルなどの物理的に関連する場の理論を含む、様々なシナリオでSESaMoのベンチマークを行います。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19619v1</guid><pubDate>Mon, 26 May 2025 07:34:11 +0000</pubDate></item><item><title>Truncated Variational Hamiltonian Ansatz: efficient quantum circuit design for quantum chemistry and material science</title><link>http://arxiv.org/abs/2505.19772v1</link><description>量子コンピュータは、古典コンピュータでは達成できない複雑な問題に対する解決策を提供することで、量子化学および材料科学に革命をもたらす可能性を秘めています。しかし、ノイズの多い条件下でも効率的な量子アルゴリズムの開発は、依然として大きな課題です。本論文では、ノイズのある中間規模量子 (NISQ) デバイス上で量子計算を行うための新しい回路設計である、truncated Variational Hamiltonian Ansatz (tVHA) を紹介します。tVHAは、固体物理学における断熱定理の原理を利用することで、幅広いアプリケーションに対して有望なアプローチを提供します。提案するアンザッツは、パラメータ数を大幅に削減し、回路サイズを大幅に縮小できますが、精度とのトレードオフがあります。したがって、tVHAは、Unitary Coupled Cluster (UCC) や Hardware-Efficient Ansatz (HEA) などの最先端のアンザッツと比較して、変分量子固有値ソルバーのフレームワーク内での収束を容易にします。本論文では、量子化学におけるtVHAの実用的な応用、特に弱相関系および強相関系の両方への適合性、および活性空間計算との互換性に焦点を当てていますが、その根底にある原理は、量子コンピューティングプラットフォーム上での材料科学計算のより広い分野にまで拡張できる可能性を示唆しています。

&lt;img src="https://arxiv.org/html/2505.19772v1/x1.png"/&gt;&lt;p&gt;Clemens Possel * —, Dmitry Bagrets * —, Frank K. Wilhelm * —, Marina Walt * —, Peter Pinski * —, Reza Shirazi * —, Walter Hahn * —&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.19772v1</guid><pubDate>Mon, 26 May 2025 09:54:46 +0000</pubDate></item><item><title>Local Pseudopotential Unlocks the True Potential of Neural Network-based Quantum Monte Carlo</title><link>http://arxiv.org/abs/2505.19909v1</link><description>ニューラルネットワークに基づく量子モンテカルロ法（NNQMC）は、多体量子系を高精度に解くための新しい手法ですが、計算コストが大きいため、小規模な系への適用に限られてきました。本研究では、局所擬ポテンシャルに基づくアプローチを導入することで、この制限を打破し、NNQMCの計算効率とスケーラビリティを大幅に向上させます。局所擬ポテンシャルの導入は、ニューラルネットワークで扱う電子数を減らすだけでなく、複雑な系において全電子NNQMC計算よりも優れた精度を達成します。この直感に反する結果は、NNQMCに固有の独特な特性によって可能になります。我々のアプローチにより、総電子数が268にも及ぶ鉄硫黄クラスターのような、これまでNNQMC法では到達できなかった大規模で困難な系を確実に扱うことができます。全体として、我々の発見は、NNQMCと局所擬ポテンシャルの相乗効果が、高精度な第一原理計算の範囲を大幅に拡大し、量子化学と計算物理学のフロンティアを押し広げることを示しています。

&lt;img src="https://arxiv.org/html/2505.19909v1/x1.png"/&gt;&lt;p&gt;Kenta Hongo, Ruichen Li, Ryunosuke Fujimaru, Weizhong Fu, Xiang Li, Xuelan Wen, Yuzhi Liu&lt;/p&gt;&lt;p&gt;*&lt;/p&gt;</description><guid isPermaLink="false">2505.19909v1</guid><pubDate>Mon, 26 May 2025 12:38:30 +0000</pubDate></item><item><title>Density-Functional Green Function Theory: Dynamical exchange-correlation field in lieu of self-energy</title><link>http://arxiv.org/abs/2505.19852v1</link><description>多電子系の1粒子グリーン関数は、伝統的に自己エネルギーの枠組みで定式化されてきた。最近、自己エネルギーを、空間的にも時間的にも局所的にグリーン関数に作用する動的交換相関場で置き換える、異なる形式主義が提案された。光電子放出や逆光電子放出実験のように、系に正孔や電子が導入された際に多電子系に誘起される有効な密度ゆらぎとして解釈できる、動的交換相関ホールと呼ばれる基本的な量が存在することがわかった。この動的交換相関ポテンシャルは、この交換相関ホールのクーロンポテンシャルに他ならず、密度汎関数理論における静的交換相関ホールが満たすものと同一の総和則と厳密な制約を満たす。提案された形式主義は、半充填1次元ハバードモデル、1次元反強磁性ハイゼンベルクモデル、単一不純物アンダーソンモデルなど、いくつかのモデル系に適用されている。均一電子ガスの動的交換相関ホールと場も、局所密度近似のような密度汎関数近似を構築する目的で研究されている。交換相関ポテンシャルの単純だが正確な近似が利用可能になれば、従来の自己エネルギーの高コストな計算を回避できるだろう。この形式主義は、多体問題に対する新たな視点と洞察も提供する可能性がある。

&lt;img src="https://arxiv.org/html/2505.19852v1/x11.png"/&gt;&lt;p&gt;F. Aryasetiawan&lt;/p&gt;&lt;p&gt;Department of Physics, Division of Mathematical Physics,
Lund University, Professorsgatan 1, 223 63, Lund, Sweden
LINXS Institute of advanced Neutron and X-ray Science,
IDEON Building: Delta 5, Scheelevägen 19, 223 70 Lund, Sweden&lt;/p&gt;</description><guid isPermaLink="false">2505.19852v1</guid><pubDate>Mon, 26 May 2025 11:37:24 +0000</pubDate></item><item><title>Design Rules for Optimizing Quaternary Mixed-Metal Chalcohalides</title><link>http://arxiv.org/abs/2505.19882v1</link><description>四元系混合金属M(II)2M(III)Ch2X3カルコハロゲン化物は、鉛系ハロゲン化物ペロブスカイトの有益な光電子特性と金属カルコゲン化物の安定性を組み合わせた、光起電性吸収体として注目されている新しい材料群です。近年発見された鉛フリー混合金属カルコハロゲン化物材料に触発され、密度汎関数理論と機械学習を組み合わせることで、鉛フリーおよび鉛系材料空間における組成傾向と化学設計規則を決定しました。合計54種類のM(II)2M(III)Ch2X3材料（M(II) = Sn, Pb、M(III) = In, Sb, Bi、Ch = S, Se, Te、X = Cl, Br, I）を、各相（Cmcm、Cmc21、P21/c）について調査しました。P21/c相は低温での平衡相であり、Cmc21、Cmcmと続きます。CmcmおよびCmc21の基本バンドギャップはP21/cよりも小さいですが、直接バンドギャップはCmcmおよびCmc21でより一般的です。P21/cにおける有効電子質量はCmcmおよびCmc21と比較して著しく大きい一方、有効正孔質量は3つの相すべてでほぼ同じです。ランダムフォレスト回帰を用いて、2つの電子受容体サイト（ChおよびX）が混合金属カルコハロゲン化合物の特性を形成する上で重要であることを発見しました。さらに、電子ドナーサイト（M(II)およびM(III)）は、材料特性を目的の用途に合わせて微調整するために使用できます。これらの設計規則により、さまざまな用途に合わせて混合金属カルコハロゲン化合物を正確に調整できます。

&lt;img src="https://arxiv.org/html/2505.19882v1/extracted/6479041/abb/a2bch2x3_structure_pol_v4_arial.jpg"/&gt;&lt;p&gt;Jingrui Li, Pascal Henkel, Patrick Rinke&lt;/p&gt;&lt;p&gt;Atomistic Modelling Center, Munich Data Science Institute, Technical University of Munich, Garching, Germany
Department of Applied Physics, Aalto University, P.O.Box 11100, FI-00076 AALTO, Finland
Munich Center for Machine Learning (MCML)
Physics Department, Technical University of Munich, Garching, Germany
State Key Laboratory for Manufacturing Systems Engineering; Electronic Materials Research Laboratory, Key Laboratory of the Ministry of Education, School of Electronic Science and Engineering; International Joint Laboratory for Micro/Nano Manufacturing and Measurement Technology, Xi’an Jiaotong University, Xi’an 710049, China&lt;/p&gt;</description><guid isPermaLink="false">2505.19882v1</guid><pubDate>Mon, 26 May 2025 12:08:27 +0000</pubDate></item><item><title>Machine Learning the Energetics of Electrified Solid/Liquid Interfaces</title><link>http://arxiv.org/abs/2505.19745v1</link><description>電気を帯びた金属表面のエネルギー論に対する、応答拡張型機械学習（ML）アプローチを提示します。局所記述子を活用して、バイアス電荷の導入に対する一次のエネルギー変化として仕事関数を学習し、Born有効電荷を通じてこの学習を安定化させます。これにより、ML原子間ポテンシャルのアーキテクチャを、二次までの有限バイアス効果を含むように効率的に拡張できます。Cu(100)上のOHへの適用により、実験的に観察された好ましい吸着サイトのpH依存性を、非ネルンスト的な電荷誘起サイトスイッチングとして合理的に説明できます。

&lt;img src="https://arxiv.org/html/2505.19745v1/extracted/6478874/NN_workflow.png"/&gt;&lt;p&gt;Karsten Reuter, Nicola Marzari, Nicolas Bergmann, Nicolas G. Hörmann, Nicéphore Bonnet&lt;/p&gt;&lt;p&gt;Fritz-Haber-Institut der Max-Planck-Gesellschaft, Faradayweg 4-6, D-14195 Berlin, Germany
Laboratory for Theory and Simulation of Materials, EPFL, Lausanne&lt;/p&gt;</description><guid isPermaLink="false">2505.19745v1</guid><pubDate>Mon, 26 May 2025 09:26:28 +0000</pubDate></item><item><title>Composition dependent $\mathbf{k}\cdot\mathbf{p}$ band parameters for wurtzite (Al,Ga)N alloys from density functional theory</title><link>http://arxiv.org/abs/2505.19730v1</link><description>半導体合金である窒化アルミニウムガリウム（(Al,Ga)N）をベースとしたUVエミッターは、光電子デバイスへの応用が期待されるため、近年大きな関心を集めています。効率が向上したこのようなデバイスの設計を導くために、いわゆるk.p法に基づく理論的枠組みが文献で広く応用されています。k.pモデルは本質的に経験的であるため、(Al,Ga)N合金の有効質量や結晶場分裂エネルギーなどのパラメータは、第一原理計算または実験からの入力として提供する必要があります。これらのパラメータはGaNとAlNでは利用可能ですが、組成依存性に関する詳細な情報は乏しいのが現状です。本研究では、この問題に取り組み、広く使用されているk.pハミルトニアンに対する(Al,Ga)Nバンドパラメータを提供します。まず、密度汎関数理論（DFT）を用いて、全組成範囲にわたる(Al,Ga)N合金の電子構造をサンプリングします。k.pパラメータは、DFTデータを再現するための自由パラメータとして扱われます。GaNとAlNについては、ここで抽出されたパラメータは文献値とよく一致しています。k.pパラメータの組成依存性に目を向けると、我々の計算では、ほとんどのパラメータがGaNとAlNの値の線形補間から大きく逸脱していることが示されています。これは、文献で広く行われている近似です。さらに、Al含有量に伴うバンドパラメータの変化を記述するためには、DFTデータを正確に記述するために、組成依存性のボーイングパラメータを考慮する必要があります。最後に、我々の分析は、(Al,Ga)N合金の電子構造に対するk.pパラメータの非線形組成依存性の結果についても初期的な洞察を提供します。特に、バンドの順序は、組成に伴う結晶場分裂エネルギーの非線形な変化によって影響を受けることがわかりました。これは、Al含有量の高い(Al,Ga)N合金の発光偏光特性にとって重要な側面です。

&lt;img src="https://arxiv.org/html/2505.19730v1/x1.png"/&gt;&lt;p&gt;Alvaro Gomez-Iglesias, Amit Kumar Singh, Stefan Schulz&lt;/p&gt;&lt;p&gt;School of Physics, University College Cork, Cork T12 YN60, Ireland
Tyndall National Institute, University College Cork, Dyke Parade, Cork T12 R5CP, Ireland
ams-OSRAM International GmbH, Regensburg 93055, Germany&lt;/p&gt;</description><guid isPermaLink="false">2505.19730v1</guid><pubDate>Mon, 26 May 2025 09:17:28 +0000</pubDate></item></channel></rss>