<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Wed, 21 May 2025 03:19:09 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-21</title><link>https://arxiv.org/2025-05-21</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13956v1"&gt;Modulating Thermometric Performance via Dopant Concentration and Morphology in Luminescence Thermometer Exhibiting Dual Structural Phase Transitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14095v1"&gt;Reactive Glass Metal Interaction under Ambient Conditions Enables Surface Modification of Gold Nanoislands&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14096v1"&gt;Atomic Topology and Magnetic Microstructure of Highly Mobile Type I and Supermobile Type II Twin Boundaries in 10M Ni-Mn-Ga Single Crystal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14259v1"&gt;In-situ observation of elastic instability of stress-induced B19$^\prime$ martensite in thin NiTi wires&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14372v1"&gt;phaser: An all-in-one package for (multislice) electron ptychography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14488v1"&gt;Acidity-Mediated Metal Oxide Heterointerfaces: Roles of Substrates and Surface Modification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14567v1"&gt;Electrical manipulation of magnetic domain structure in van der Waals ferromagnetic Fe$_3$GaTe$_2$ using ferroelectric PMN-PT single crystal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13970v1"&gt;Micromagnetic Study of the Dipolar-Exchange Spin Waves in Antiferromagnetic Thin Films&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13999v1"&gt;Hybridized and Localized 4f Electronic States of Nd-based Intermetallic Compounds in Cubic Symmetry Probed by High-Energy Photoemission&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14032v1"&gt;Revealing Information from Weak Signal in Electron Energy-Loss Spectroscopy with a Deep Denoiser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14094v1"&gt;Scalable alloy-based sputtering of high-conductivity PdCoO2 for advanced interconnects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14194v1"&gt;Poleval: A Python package for HAXPES analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14434v1"&gt;Thermal conductivity of boron arsenide above 2100 watts per meter per Kelvin at room temperature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14013v1"&gt;A novel variant of rhombic Penrose tiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14334v1"&gt;Water-rich amorphous state from drying mixed-metal sulfate solutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14416v1"&gt;Reference lattice, sound, stiffness, and magnetic transitions of Ising monolayers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14571v1"&gt;Dynamic correlations of frustrated quantum spins from high-temperature expansion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14589v1"&gt;Interplay between altermagnetic order and crystal symmetry probed using magnetotransport in epitaxial altermagnet MnTe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13835v1"&gt;Ionic environment-modulated nucleation and stability of multiscale nanodomains in surfactant-free microemulsions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14108v1"&gt;A novel approach to process TRISO nuclear fuel using plasma-aided chemistry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14248v1"&gt;Enhanced 133cs Triple-Quantum Excitation in Solid-State NMR of Cs-Bearing Zeolites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14127v1"&gt;Quantum and Critical Casimir Effects: Bridging Fluctuation Physics and Nanotechnology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14210v1"&gt;A marching cubes based method for topology changes in three-dimensional two-phase flows with front tracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14373v1"&gt;Time domain analysis of microstructured materials through the reduced relaxed micromorphic model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13831v1"&gt;TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13851v1"&gt;A Challenge to Build Neuro-Symbolic Video Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13914v1"&gt;Parallel Belief Revision via Order Aggregation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13946v1"&gt;Visual Instruction Bottleneck Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14137v1"&gt;Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14140v1"&gt;RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14141v1"&gt;Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14143v1"&gt;Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14147v1"&gt;SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14193v1"&gt;Dynamic Replanning for Improved Public Transport Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14209v1"&gt;Embedded Mean Field Reinforcement Learning for Perimeter-defense Game&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14235v1"&gt;Toward Embodied AGI: A Review of Embodied AI and the Road Ahead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14289v1"&gt;EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14381v1"&gt;SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14391v1"&gt;Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14403v1"&gt;Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14419v1"&gt;SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14524v1"&gt;Guarded Query Routing for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14539v1"&gt;A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14604v1"&gt;Let LLMs Break Free from Overthinking via Self-Braking Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14656v1"&gt;Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13834v1"&gt;Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13840v1"&gt;EfficientLLM: Efficiency in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13857v1"&gt;Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13860v1"&gt;Domain Adaptation of VLM for Soccer Video Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13872v1"&gt;Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13873v1"&gt;Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13887v1"&gt;Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13921v1"&gt;APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13934v1"&gt;RLVR-World: Training World Models with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13936v1"&gt;EEG-to-Text Translation: A Model for Deciphering Human Brain Activity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13949v1"&gt;FlashThink: An Early Exit Method For Efficient Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13965v1"&gt;CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13969v1"&gt;Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13986v1"&gt;Solving Normalized Cut Problem with Constrained Action Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13989v1"&gt;When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14005v1"&gt;Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14024v1"&gt;FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14027v1"&gt;CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14029v1"&gt;AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14036v1"&gt;Adaptive Cyclic Diffusion for Inference Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14038v1"&gt;ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14045v1"&gt;From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14057v1"&gt;Field Matters: A lightweight LLM-enhanced Method for CTR Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14072v1"&gt;Personalized Student Knowledge Modeling for Future Learning Resource Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14106v1"&gt;A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14107v1"&gt;DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14117v1"&gt;Collaborative Unlabeled Data Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14128v1"&gt;A Methodological Framework for Measuring Spatial Labeling Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14136v1"&gt;Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14146v1"&gt;s3: You Don't Need That Much Data to Train a Search Agent via RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14157v1"&gt;Prior Prompt Engineering for Reinforcement Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14178v1"&gt;Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14190v1"&gt;$α$-GAN by Rényi Cross Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14206v1"&gt;Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14212v1"&gt;Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14216v1"&gt;Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14217v1"&gt;Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14226v1"&gt;"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14227v1"&gt;VoQA: Visual-only Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14234v1"&gt;Fast and close Shannon entropy approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14246v1"&gt;Visual Agentic Reinforcement Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14252v1"&gt;Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14256v1"&gt;FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14260v1"&gt;Speculative Decoding Reimagined for Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14268v1"&gt;Think-J: Learning to Think for Generative LLM-as-a-Judge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14295v1"&gt;Benchmarking data encoding methods in Quantum Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14312v1"&gt;MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14316v1"&gt;Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14341v1"&gt;Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14345v1"&gt;Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14366v1"&gt;Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14377v1"&gt;When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14395v1"&gt;MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14428v1"&gt;Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14435v1"&gt;Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14436v1"&gt;Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14451v1"&gt;RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14452v1"&gt;How Managers Perceive AI-Assisted Conversational Training for Workplace Communication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14455v1"&gt;CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14469v1"&gt;Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14489v1"&gt;Reasoning Models Better Express Their Confidence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14499v1"&gt;Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14505v1"&gt;ModRWKV: Transformer Multimodality in Linear Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14510v1"&gt;BACON: A fully explainable AI model with graded logic for decision making problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14513v1"&gt;Latent Flow Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14523v1"&gt;Exploring Graph Representations of Logical Forms for Language Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14526v1"&gt;NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14533v1"&gt;Energy-Efficient Deep Reinforcement Learning with Spiking Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14544v1"&gt;Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14549v1"&gt;Can Large Language Models Really Recognize Your Name?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14555v1"&gt;Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14564v1"&gt;Bellman operator convergence enhancements in reinforcement learning algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14566v1"&gt;KIPPO: Koopman-Inspired Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14599v1"&gt;Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14627v1"&gt;Debating for Better Reasoning: An Unsupervised Multimodal Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14646v1"&gt;CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14659v1"&gt;Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14661v1"&gt;Abacus: A Cost-Based Optimizer for Semantic Operator Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14667v1"&gt;SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14684v1"&gt;Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13820v1"&gt;Structured Agent Distillation for Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13837v1"&gt;Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13847v1"&gt;Forensic deepfake audio detection using segmental speech features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13855v1"&gt;Domain Gating Ensemble Networks for AI-Generated Text Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13898v1"&gt;Do Language Models Use Their Depth Efficiently?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13904v1"&gt;Learning to Insert for Constructive Neural Vehicle Routing Solver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13906v1"&gt;XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13909v1"&gt;Efficient Agent Training for Computer Use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13911v1"&gt;Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13948v1"&gt;Memory-Centric Embodied Question Answer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13971v1"&gt;The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13973v1"&gt;Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13995v1"&gt;Social Sycophancy: A Broader Understanding of LLM Sycophancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14001v1"&gt;VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14020v1"&gt;Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14064v1"&gt;NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14080v1"&gt;Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14125v1"&gt;Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14139v1"&gt;FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14185v1"&gt;Safety Subspaces are Not Distinct: A Fine-Tuning Case Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14201v1"&gt;FLASH-D: FlashAttention with Hidden Softmax Division&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14233v1"&gt;Mechanistic Fine-tuning for In-context Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14238v1"&gt;ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14273v1"&gt;X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14285v1"&gt;AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14300v1"&gt;SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14330v1"&gt;Handloom Design Generation Using Generative Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14351v1"&gt;FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14396v1"&gt;Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14398v1"&gt;Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14551v1"&gt;Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14552v1"&gt;KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14561v1"&gt;SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14569v1"&gt;Agent Context Protocols Enhance Collective Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14603v1"&gt;Towards a Foundation Model for Communication Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14608v1"&gt;Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14615v1"&gt;SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14625v1"&gt;TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14629v1"&gt;KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14654v1"&gt;Beyond Words: Multimodal LLM Knows When to Speak&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14660v1"&gt;EmoGist: Efficient In-Context Learning for Visual Emotion Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14664v1"&gt;AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14668v1"&gt;ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14673v1"&gt;Training-Free Watermarking for Autoregressive Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14680v1"&gt;NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14103v1"&gt;AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14349v1"&gt;Upgrading Democracies with Fairer Voting Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14633v1"&gt;Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14681v1"&gt;Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13858v1"&gt;Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13896v1"&gt;CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13899v1"&gt;Exploring Causes of Representational Similarity in Machine Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13900v1"&gt;New Evidence of the Two-Phase Learning Dynamics of Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13907v1"&gt;Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13910v1"&gt;ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13954v1"&gt;VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14011v1"&gt;Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14040v1"&gt;Unsupervised Graph Clustering with Deep Structural Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14044v1"&gt;Generalized Category Discovery via Token Manifold Capacity Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14122v1"&gt;Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14161v1"&gt;Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14202v1"&gt;MSDformer: Multi-scale Discrete Transformer For Time Series Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14211v1"&gt;A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14240v1"&gt;Learning with Local Search MCMC Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14352v1"&gt;Towards eliciting latent knowledge from LLMs with mechanistic interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14407v1"&gt;Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14411v1"&gt;Byte Pair Encoding for Efficient Time Series Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14415v1"&gt;Table Foundation Models: on knowledge pre-training for tabular learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14424v1"&gt;Explaining Neural Networks with Reasons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14463v1"&gt;Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14477v1"&gt;Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14502v1"&gt;Learning to Integrate Diffusion ODEs by Averaging the Derivatives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14522v1"&gt;Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14531v1"&gt;SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14543v1"&gt;Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14595v1"&gt;Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14610v1"&gt;MMD-Newton Method for Multi-objective Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14635v1"&gt;Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14643v1"&gt;Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14669v1"&gt;Quartet: Native FP4 Training Can Be Optimal for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13852v1"&gt;Rethink the Role of Deep Learning towards Large-scale Quantum Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13878v1"&gt;InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13881v1"&gt;TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13889v1"&gt;Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13925v1"&gt;Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13947v1"&gt;A Probabilistic Perspective on Model Collapse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13963v1"&gt;Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14083v1"&gt;Computational Efficiency under Covariate Shift in Kernel Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14113v1"&gt;CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14158v1"&gt;Temporal Alignment of Time Sensitive Facts with Activation Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14160v1"&gt;Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14165v1"&gt;PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14192v1"&gt;QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14251v1"&gt;A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14264v1"&gt;AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14302v1"&gt;Scaling Law for Quantization-Aware Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14303v1"&gt;Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14310v1"&gt;Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14314v1"&gt;Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14323v1"&gt;Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14340v1"&gt;Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14354v1"&gt;WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14357v1"&gt;Vid2World: Crafting Video Diffusion Models to Interactive World Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14371v1"&gt;Layer-wise Quantization for Quantized Optimistic Dual Averaging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14417v1"&gt;Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14420v1"&gt;SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14459v1"&gt;Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14465v1"&gt;FlowTSE: Target Speaker Extraction with Flow Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14468v1"&gt;ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14476v1"&gt;Enhancing Interpretability of Sparse Latent Representations with Class Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14507v1"&gt;Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14512v1"&gt;Just One Layer Norm Guarantees Stable Extrapolation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14529v1"&gt;A simple estimator of the correlation kernel matrix of a determinantal point process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14530v1"&gt;Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14534v1"&gt;Lessons from Defending Gemini Against Indirect Prompt Injections&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14535v1"&gt;Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14553v1"&gt;Pivot Language for Low-Resource Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14581v1"&gt;Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14583v1"&gt;Instance Segmentation for Point Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14587v1"&gt;High-Dimensional Analysis of Bootstrap Ensemble Classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14592v1"&gt;Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14596v1"&gt;CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14613v1"&gt;Virtual Cells: Predict, Explain, Discover&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14620v1"&gt;Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14621v1"&gt;3D Reconstruction from Sketches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13864v1"&gt;Graphon Mixtures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13902v1"&gt;An Asymptotic Equation Linking WAIC and WBIC in Singular Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14021v1"&gt;Adversarial Training from Mean Field Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14033v1"&gt;Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14042v1"&gt;Adversarially Pretrained Transformers may be Universally Robust In-Context Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14071v1"&gt;Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14081v1"&gt;Personalized and Resilient Distributed Learning Through Opinion Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14102v1"&gt;High-dimensional Nonparametric Contextual Bandit Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14126v1"&gt;MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14164v1"&gt;Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14214v1"&gt;Regularized least squares learning with heavy-tailed noise is minimax optimal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14242v1"&gt;Technical Report on classification of literature related to children speech disorder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14338v1"&gt;Better Neural Network Expressivity: Subdividing the Simplex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14388v1"&gt;Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14470v1"&gt;PAST: Phonetic-Acoustic Speech Tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14517v1"&gt;Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14572v1"&gt;Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14638v1"&gt;Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14647v1"&gt;Sequential QCQP for Bilevel Optimization with Line Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14421v1"&gt;A system identification approach to clustering vector autoregressive time series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13844v1"&gt;Improve Language Model and Brain Alignment via Associative Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13886v1"&gt;Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13903v1"&gt;Let's Verify Math Questions Step by Step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13908v1"&gt;Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13913v1"&gt;Word length predicts word order: "Min-max"-ing drives language evolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13944v1"&gt;Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13972v1"&gt;Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13979v1"&gt;Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14015v1"&gt;AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14104v1"&gt;Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14130v1"&gt;Probing BERT for German Compound Semantics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14131v1"&gt;Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14173v1"&gt;THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14181v1"&gt;SlangDIT: Benchmarking LLMs in Interpretative Slang Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14183v1"&gt;ThinkSwitcher: When to Think Hard, When to Think Fast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14195v1"&gt;Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14244v1"&gt;TransBench: Benchmarking Machine Translation for Industrial-Scale Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14271v1"&gt;FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14297v1"&gt;Cross-Lingual Optimization for Language Transfer in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14305v1"&gt;JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14309v1"&gt;Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14311v1"&gt;HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14313v1"&gt;A MIND for Reasoning: Meta-learning for In-context Deduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14347v1"&gt;QA-prompting: Improving Summarization with Large Language Models using Question-Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14350v1"&gt;OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14367v1"&gt;Dual Decomposition of Weights and Singular Value Low Rank Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14393v1"&gt;Editing Across Languages: A Survey of Multilingual Knowledge Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14406v1"&gt;Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14418v1"&gt;Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14423v1"&gt;Scaling Low-Resource MT via Synthetic Data Generation with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14425v1"&gt;From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14464v1"&gt;Not All Correct Answers Are Equal: Why Your Distillation Source Matters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14467v1"&gt;Void in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14471v1"&gt;Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14481v1"&gt;PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14483v1"&gt;MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14536v1"&gt;Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14577v1"&gt;TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14582v1"&gt;Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14585v1"&gt;Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14590v1"&gt;MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14597v1"&gt;Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14631v1"&gt;Think Only When You Need with Large Hybrid-Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14674v1"&gt;Reward Reasoning Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14679v1"&gt;UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14685v1"&gt;Language Models use Lookbacks to Track Beliefs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13862v1"&gt;PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13957v1"&gt;Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14052v1"&gt;Improved Methods for Model Pruning and Knowledge Distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14112v1"&gt;Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14318v1"&gt;RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14368v1"&gt;Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14402v1"&gt;OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14432v1"&gt;Rank-K: Test-Time Reasoning for Listwise Reranking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14449v1"&gt;Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14462v1"&gt;RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14607v1"&gt;sudoLLM : On Multi-role Alignment of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14617v1"&gt;Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14272v1"&gt;Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14286v1"&gt;Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14356v1"&gt;PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14410v1"&gt;Pairwise Evaluation of Accent Similarity in Speech Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14438v1"&gt;S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.14518v1"&gt;Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-21</guid><pubDate>Wed, 21 May 2025 12:14:48 +0900</pubDate></item><item><title>Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information</title><link>http://arxiv.org/abs/2505.14149v1</link><description>学術論文の指数関数的な増加により、研究者が関連文献にアクセスするのに必要な時間が大幅に増加しています。キーフレーズ抽出（KPE）は、研究者が効率的に関連文献を検索できるようにすることで、この状況に対する解決策を提供します。学術論文からのKPEに関する現在の研究は、タイトルとアブストラクトを入力コーパスとして使用する革新的なアプローチを通じて、抽出モデルの性能を向上させることを目的としています。しかし、キーワードのセマンティックな豊かさは、アブストラクトの長さによって著しく制約されます。全文ベースのKPEはこの問題に対処できますが、同時にノイズが導入され、KPEの性能が大幅に低下します。この問題に対処するために、本論文では、学術論文のセクション構造情報から得られる構造的特徴とセクションテキストを利用して、学術論文からキーフレーズを抽出します。このアプローチは、主に2つの部分で構成されています。（1）7つの構造的特徴がKPEモデルに与える影響の調査、（2）キーフレーズ統合アルゴリズムを介して、KPEモデルの入力コーパスとして使用されるすべてのセクションテキストからの抽出結果を統合し、キーフレーズ統合結果を取得します。さらに、本論文では、セクション構造の分類品質がKPEの性能に与える影響についても検討しました。結果は、構造的特徴を組み込むことでKPEの性能が向上することを示していますが、特徴によってモデルの有効性に対する影響は異なります。キーフレーズ統合アプローチは最高の性能を示し、セクション構造の分類品質はKPEの性能に影響を与える可能性があります。これらの発見は、学術論文のセクション構造情報を使用することが、学術論文からの効果的なKPEに貢献することを示唆しています。本研究をサポートするコードとデータセットは、https://github.com/yan-xinyi/SSB_KPE で入手できます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14149v1</guid><pubDate>Tue, 20 May 2025 09:57:34 +0000</pubDate></item><item><title>Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</title><link>http://arxiv.org/abs/2505.14099v1</link><description>知識ベース質問応答 (KBQA) は、知識ベース (KB) からの構造化された知識を用いて自然言語の質問に答えることを目的としています。LLM (大規模言語モデル) のみのアプローチは汎化性を提供する一方で、知識の陳腐化、ハルシネーション、透明性の欠如に悩まされます。チェーンベースの KG-RAG (知識グラフ検索拡張生成) 手法は、外部 KB を組み込むことでこれらの問題に対処しますが、計画や論理構造化がないため、単純なチェーン構造の質問に限定されます。セマンティックパージング手法に触発され、我々は PDRR (Predict, Decompose, Retrieve, and Reason: 予測、分解、検索、推論) という4段階のフレームワークを提案します。我々の手法は、まず質問の種類を予測し、質問を構造化されたトリプルに分解します。次に、KB から関連情報を検索し、LLM をエージェントとして誘導し、分解されたトリプルを推論し完成させます。実験結果は、PDRR が様々な LLM バックボーンにおいて既存の手法を一貫して上回り、チェーン構造および非チェーン構造の複雑な質問の両方で優れた性能を達成することを示しています。

&lt;img src="https://arxiv.org/html/2505.14099v1/x1.png"/&gt;&lt;p&gt;Yihua Zhu Qianying Liu Akiko Aizawa Hidetoshi Shimodaira Kyoto University University of Tokyo NII RIKEN&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14099v1</guid><pubDate>Tue, 20 May 2025 09:01:52 +0000</pubDate></item><item><title>General-Reasoner: Advancing LLM Reasoning Across All Domains</title><link>http://arxiv.org/abs/2505.14652v1</link><description>強化学習（RL）は近年、大規模言語モデル（LLM）の推論能力を向上させる上で大きな可能性を示しています。特に、Deepseek-R1-Zeroによって導入された「Zero」強化学習は、中間的な教師ありファインチューニング段階に頼ることなく、ベースLLMの直接的なRLトレーニングを可能にします。しかし、これらの進歩にもかかわらず、LLM推論に関する現在の研究は、主に数学およびコーディングの分野に焦点を当てています。これは、データの豊富さと回答の検証の容易さによるものです。このことは、質問に対する回答の表現が多様であり、データがより不足している、より広範な分野へのモデルの適用性と一般化を制限します。本論文では、多様な分野にわたってLLMの推論能力を向上させるように設計された、新しいトレーニングパラダイムであるGeneral-Reasonerを提案します。主な貢献は次のとおりです。（1）ウェブクローリングによってキュレーションされた、検証可能な回答を持つ大規模で高品質な質問のデータセットを構築し、幅広い分野をカバーすること。（2）従来のルールベースの検証を、連鎖的思考とコンテキスト認識の能力に置き換える、生成モデルベースの回答検証器を開発すること。一連のモデルをトレーニングし、物理学、化学、金融、電子工学など、幅広い分野をカバーする広範なデータセットで評価します。これらの12のベンチマーク（例：MMLU-Pro、GPQA、SuperGPQA、TheoremQA、BBEH、MATH AMC）にわたる包括的な評価により、General-Reasonerは既存のベースラインメソッドを上回り、堅牢で汎用的な推論性能を実現すると同時に、数学的推論タスクにおいて優れた有効性を維持することが示されています。

&lt;img src="https://arxiv.org/html/2505.14652v1/extracted/6459789/teaser.png"/&gt;&lt;p&gt;M-A-P x93ma@uwaterloo.ca, Singapore, Vector Institute, \vardiamondsuit \vardiamondsuit {}^{\vardiamondsuit} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT Qian Liu, \vardiamondsuit \vardiamondsuit {}^{\vardiamondsuit} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT TikTok, \vardiamondsuit \vardiamondsuit {}^{\vardiamondsuit} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT Zejun Ma, \varheartsuit \varheartsuit {}^{\varheartsuit} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT Xueguang Ma, \varheartsuit ⁢ ♠ \varheartsuit ♠ {}^{\varheartsuit\spadesuit} start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT Dongfu Jiang, \varheartsuit ⁢ ♠ \varheartsuit ♠ {}^{\varheartsuit\spadesuit} start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT Wenhu Chen \varheartsuit \varheartsuit {}^{\varheartsuit} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT University of Waterloo, \varheartsuit ⁢ ♣ \varheartsuit ♣ {}^{\varheartsuit\clubsuit} start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT Ge Zhang, wenhuchen@uwaterloo.ca&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14652v1</guid><pubDate>Tue, 20 May 2025 17:41:33 +0000</pubDate></item><item><title>AutoRev: Automatic Peer Review System for Academic Research Papers</title><link>http://arxiv.org/abs/2505.14376v1</link><description>学術研究論文のレビュー作成は複雑な作業であり、文書の内容とセクション間の相互依存関係を深く理解する必要があります。技術的な詳細への洞察だけでなく、論文全体の整合性と構造を評価することも求められます。最近の手法は、主に大規模言語モデル（LLM）をファインチューニングしてこの課題に取り組むことに焦点を当ててきました。しかし、それらはしばしば、長い入力トークン長によって課せられる計算量とパフォーマンスの制約を見過ごしています。これに対処するため、私たちは学術研究論文のための自動査読システムであるAutoRevを導入します。私たちの新しいフレームワークは、学術文書をグラフとして表現し、レビューに大きく貢献する最も重要な箇所を抽出することを可能にします。このグラフベースのアプローチは、レビュー生成に有効性を示し、質問応答、要約、文書表現など、さまざまな下流タスクにも適用できる可能性があります。レビュー生成に適用した場合、私たちの手法はすべての評価指標において、SOTA（最先端）のベースラインを平均58.72%上回ります。私たちの研究が、グラフベースの抽出技術をNLPの他の下流タスクに適用する研究をさらに促進することを願っています。採択され次第、コードを公開する予定です。

&lt;img src="https://arxiv.org/html/2505.14376v1/x1.png"/&gt;&lt;p&gt;IIIT Hyderabad&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14376v1</guid><pubDate>Tue, 20 May 2025 13:59:58 +0000</pubDate></item><item><title>The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models</title><link>http://arxiv.org/abs/2505.14172v1</link><description>大規模言語モデル（LLM）は、多様な分野で目覚ましい進歩を遂げているにもかかわらず、単語内の文字数を数えるといった単純な文字レベルのタスクで一貫して失敗します。これは、トークン化という根本的な制約によるものです。本研究では、この制約を低い相互情報量の問題として捉え、概念の創発という観点から分析します。文字レベルの推論を制御された環境で分離する19の合成タスクを用いて、そのような能力がゆっくりと、突然に、そして学習の後半になって初めて創発することを示します。さらに、概念の創発に関するパーコレーションベースのモデルがこれらのパターンを説明できることを示し、文字構成の学習は常識的な知識の学習と本質的に変わらないことを示唆します。このボトルネックに対処するため、サブワードモデルの帰納的利点を維持しながら、文字レベルの推論を大幅に改善する軽量なアーキテクチャの修正を提案します。これらの結果を総合すると、トークン化されたLMにおける低レベルの知覚的なギャップを埋め、その構造的な盲点を理解し軽減するための原則的なフレームワークを提供します。コードは公開されています。

&lt;img src=""/&gt;&lt;p&gt;Adrian Cosma, Emilian Radoi, Mihai Dascalu National University of Science, Stefan Ruseti, Technology POLITEHNICA Bucharest&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14172v1</guid><pubDate>Tue, 20 May 2025 10:25:17 +0000</pubDate></item><item><title>Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst</title><link>http://arxiv.org/abs/2505.14116v1</link><description>推論時のスケーリングは大きな注目を集めており、思考の連鎖（Chain-of-Thought）の長さを増やすことで、大規模言語モデル（LLM）の複雑な推論タスクにおける性能を大幅に向上させます。これらのより長い中間的な推論過程は、反省や分解といった人間の認知における様々なメタ推論スキルを具現化していますが、作成や習得が困難です。本研究では、モデル自体がより長いCoTデータを合成し、自己学習を通じて反復的に性能を向上させることができる\textit{Self-Reasoning Language Model}（SRLM）を提案します。既存の応答から隠れた推論連鎖を展開する方法を示す少数のデモンストレーション例（すなわち、1,000サンプル）を、推論の触媒として組み込むことで、SRLMはモデルの初期性能を向上させるだけでなく、その後の反復においてより安定した一貫性のある改善を保証することを示します。提案するSRLMは、2つのバックボーンモデルにおいて、MMLU、GSM8K、ARC-C、HellaSwag、BBHの5つの推論タスク全体で、平均して+2.5ポイント以上の絶対的な改善を達成します。さらに、推論時のサンプリング回数を増やすほど、より多くの改善をもたらします。例えば、64回のサンプリングで平均+7.89の絶対的な改善が見られ、強力なベースラインと比較して、SRLMにおける深く、多様で創造的な推論経路が明らかになります。

&lt;img src="https://arxiv.org/html/2505.14116v1/x1.png"/&gt;&lt;p&gt;Deng Cai, Hongru Wang, Shijue Huang, Wanjun Zhong&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14116v1</guid><pubDate>Tue, 20 May 2025 09:21:26 +0000</pubDate></item><item><title>MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations</title><link>http://arxiv.org/abs/2505.14101v1</link><description>大規模言語モデル（LLM）は、忠実性と事実性において本質的な限界があり、一般的にハルシネーション（幻覚）と呼ばれます。英語中心のデータセットの文脈における事実性評価のためのテストベッドを提供するベンチマークがいくつか開発されていますが、ウェブリンクやテキストパッセージのような補足的な情報コンテキストに依存しており、利用可能な構造化された事実リソースを無視しています。この点において、知識グラフ（KG）は、エンティティに関する事実とその関係を最小限の言語的オーバーヘッドで構造化された方法で表現するため、ハルシネーション軽減に役立つ手段として認識されています。既存のハルシネーション評価ベンチマークにおける事実言語モデリングのためのKGパスと多言語性の欠如を埋め合わせ、生成テキスト評価のために構成された\textbf{MultiHal}と呼ばれるKGベースの多言語、マルチホップベンチマークを提案します。データ収集パイプラインの一環として、オープンなKGから14万件のKGパスをマイニングし、そこからノイズの多いKGパスを剪定し、2万5900件の高品質なサブセットをキュレーションしました。ベースライン評価では、複数の言語と複数のモデルにわたって、バニラQAに対するKG-RAGにおける意味的類似性スコアが約0.12から0.36ポイント絶対的に増加しており、KG統合の可能性を示しています。MultiHalが、グラフベースのハルシネーション軽減および事実確認タスクに関する今後の研究を促進することを期待しています。

&lt;img src="https://arxiv.org/html/2505.14101v1/extracted/6457579/figures/examples/multihal_pipeline_kg_paths_fixed.png"/&gt;&lt;p&gt;Austria Johannes Bjerva Department of Computer Science Aalborg University Copenhagen, Computation TU Wien Vienna, Denmark, Denmark Katja Hose Institute of Logic, Denmark Russa Biswas Department of Computer Science Aalborg University Copenhagen, Ernests Lavrinovics Department of Computer Science Aalborg University Copenhagen&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14101v1</guid><pubDate>Tue, 20 May 2025 09:03:35 +0000</pubDate></item><item><title>BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks</title><link>http://arxiv.org/abs/2505.14079v1</link><description>大規模言語モデル（LLM）を基盤とするエージェントは、人間の指示に従い、様々なタスクを自動的に完了させる上で大きな可能性を示しています。タスクを完了させるためには、エージェントは計画を立てることによって、それを実行しやすいステップに分解する必要があります。既存の研究では主に、エージェントの初期状態から、次に実行すべきステップを推論することによって計画を立てています。しかし、この順方向推論のパラダイムは、複雑なタスクにはうまく機能しません。そこで、現実世界のシナリオに基づいた複雑なタスクをシミュレートする仮想環境であるMinecraftで、この問題を研究することを提案します。順方向推論の失敗は、エージェントの初期状態とタスク目標との間の大きな認識ギャップが原因であると考えています。この目的のために、逆方向推論を活用し、タスク目標を1ステップで直接達成できる最終状態から計画を立てます。具体的には、BAckward Reasoning based agent（BAR）を設計します。これは、最終状態から堅牢で一貫性があり、効率的な計画を立てるために、再帰的な目標分解モジュール、状態の一貫性を維持するモジュール、およびステージメモリモジュールを備えています。実験結果は、既存の手法に対するBARの優位性と、提案されたモジュールの有効性を示しています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14079v1</guid><pubDate>Tue, 20 May 2025 08:35:35 +0000</pubDate></item><item><title>Enhancing LLMs via High-Knowledge Data Selection</title><link>http://arxiv.org/abs/2505.14070v1</link><description>大規模言語モデル（LLM）の性能は、本質的にその訓練データの質と結びついています。いくつかの研究が高品質なデータを選択する方法を提案していますが、テキストコーパスにおける知識の豊富さの重要性を考慮していません。本論文では、事前学習済みコーパスにおける知識不足の問題を軽減するために、知識の側面から高品質なデータを選択する、新しい勾配不要のHigh-Knowledge Scorer（HKS）を提案します。包括的なマルチドメイン知識要素プールを提案し、テキストの知識内容を評価するための指標として知識密度と知識カバレッジを導入します。これに基づいて、集中的な知識を持つデータを選択するための包括的な知識スコアラーを提案します。これは、知識要素を特定のドメインに制限することで、ドメイン固有の高知識データ選択にも利用できます。高知識なバイリンガルデータセットでモデルを訓練し、実験結果は、提案するスコアラーが知識集約型タスクと一般的な理解タスクにおいてモデルの性能を向上させ、モデルの汎用的な能力とドメイン固有の能力の両方を効果的に強化することを示しています。

&lt;img src="https://arxiv.org/html/2505.14070v1/x1.png"/&gt;&lt;p&gt;Feiyu Duan, Haoran Que, Sirui Wang, Wenge Rong, Xuemiao Zhang, Xunliang Cai, Yuqi Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14070v1</guid><pubDate>Tue, 20 May 2025 08:21:37 +0000</pubDate></item><item><title>Activation-Guided Consensus Merging for Large Language Models</title><link>http://arxiv.org/abs/2505.14009v1</link><description>近年の研究では、システム2の推論能力とシステム1の効率性を両立させることにますます焦点が当てられています。既存のトレーニングベースおよびプロンプトベースのアプローチは、効率と安定性の面で大きな課題に直面していますが、モデルマージングは、異なる大規模言語モデル（LLM）の多様な能力を統合されたモデルに統合するための有望な戦略として浮上しています。しかし、従来のモデルマージング手法は、層全体で均一な重要性を仮定することが多く、ニューラルコンポーネントに固有の機能的な異質性を見落としています。この制限に対処するために、我々は\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging（\textbf{ACM}、活性化誘導型コンセンサスマージング）を提案します。これは、事前学習済みモデルとファインチューニング済みモデルの活性化間の相互情報量に基づいて、層固有のマージ係数を決定するプラグアンドプレイのマージングフレームワークです。ACMは、勾配計算や追加のトレーニングを必要とせずに、タスク固有の能力を効果的に保持します。Long-to-Short（L2S）および一般的なマージングタスクに関する広範な実験により、ACMがすべてのベースラインメソッドを一貫して上回ることが示されています。たとえば、Qwen-7Bモデルの場合、ACMを搭載したTIES-Mergingは、応答長を\textbf{55.3％}削減しながら、推論精度を\textbf{1.3}ポイント向上させます。再現性のために、コードを論文とともに提出し、公開予定です。

&lt;img src="https://arxiv.org/html/2505.14009v1/extracted/6457114/figures/acm_TA.png"/&gt;&lt;p&gt;City University of Hong Kong Huawei Noah’s Ark Lab University of Hong Kong Hong Kong University of Science, Department of Computer Science, Mingyang Liu, Qintong Li, Shuqi Liu, Technology, Technology (Guangzhou) Hong Kong University of Science, Xiongwei Han, Yuxuan Yao, Zehua Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14009v1</guid><pubDate>Tue, 20 May 2025 07:04:01 +0000</pubDate></item><item><title>DecIF: Improving Instruction-Following through Meta-Decomposition</title><link>http://arxiv.org/abs/2505.13990v1</link><description>指示追従は、大規模言語モデル（LLM）にとって重要な能力として浮上しています。しかし、既存のアプローチは、指示追従データを合成するために、既存のドキュメントや外部リソースに依存することが多く、その柔軟性と一般化可能性を制限しています。本論文では、LLMのみを使用して多様で高品質な指示追従データを生成する、完全自律型のメタ分解誘導フレームワークであるDecIFを紹介します。DecIFは、分解の原則に基づいています。指示生成においては、LLMを誘導して、さまざまな種類のメタ情報を反復的に生成させ、それらを応答制約と組み合わせて、構造化され、意味的に豊かな指示を形成します。さらに、LLMを利用して、生成された指示内の潜在的な矛盾を検出し、解決します。応答生成に関しては、各指示を原子レベルの評価基準に分解し、厳密な検証と不正確な指示-応答ペアの排除を可能にします。広範なシナリオと設定にわたる広範な実験により、指示追従タスクにおけるDecIFの優れたパフォーマンスが実証されています。さらなる分析により、高品質な指示データを自動的に合成する際の、その強力な柔軟性、スケーラビリティ、および一般化可能性が強調されています。

&lt;img src="https://arxiv.org/html/2505.13990v1/x2.png"/&gt;&lt;p&gt;Beijing, Beijing University of Posts, China, China Peking University, Telecommunications&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13990v1</guid><pubDate>Tue, 20 May 2025 06:38:28 +0000</pubDate></item><item><title>The Hallucination Tax of Reinforcement Finetuning</title><link>http://arxiv.org/abs/2505.13988v1</link><description>強化学習ファインチューニング（RFT）は、大規模言語モデル（LLM）の推論能力を高めるための標準的なアプローチとなっています。しかし、モデルの信頼性に対するその影響は、まだ十分に調査されていません。本研究では、RFTの重大な副作用を特定し、体系的に調査します。これを「ハルシネーション税」と名付け、解答不能な質問に対して、モデルが自信を持ってハルシネーション（幻覚）的な回答を生成する原因となる、拒否行動の低下を指します。これを調査するために、SUM（Synthetic Unanswerable Math）という、不十分または曖昧な情報から推論することで、モデルが解答不能な質問を認識する能力を調査するために設計された、高品質な解答不能な数学問題のデータセットを導入します。その結果、標準的なRFTトレーニングは、モデルの拒否率を80%以上低下させる可能性があり、モデルのハルシネーション傾向を大幅に高めることが示されました。さらに、RFT中にわずか10%のSUMを組み込むことで、適切な拒否行動が大幅に回復し、解けるタスクでの精度低下は最小限に抑えられることを示します。重要なことに、このアプローチにより、LLMは推論時の計算能力を活用して、自身の不確実性や知識の境界について推論できるようになり、ドメイン外の数学問題だけでなく、事実に関する質問応答タスクにも汎化能力が向上します。

&lt;img src="https://arxiv.org/html/2505.13988v1/x1.png"/&gt;&lt;p&gt;Linxin Song Taiwei Shi * * {}^{\text{*}} start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT Jieyu Zhao University of Southern California lime-nlp/Synthetic_Unanswerable_Math&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13988v1</guid><pubDate>Tue, 20 May 2025 06:36:45 +0000</pubDate></item><item><title>DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models</title><link>http://arxiv.org/abs/2505.13975v1</link><description>大規模推論モデル（LRM）は、長い思考の連鎖（CoT）推論を通じて複雑な推論タスクで成功を収めていますが、その推論は過度に冗長な推論トレースを伴うことが多く、結果として大幅な非効率性をもたらします。これに対処するため、効率的な推論のための広く使用されている2つの戦略である、推論時の枝刈りとチューニングベースの蒸留を組み合わせたハイブリッドフレームワークであるDistilled Reasoning Pruning（DRP）を提案します。DRPは、教師モデルを使用してスキルを意識したステップ分解とコンテンツの枝刈りを行い、次に枝刈りされた推論パスを生徒モデルに蒸留し、効率的かつ正確に推論できるようにします。いくつかの困難な数学的推論データセットにおいて、DRPでトレーニングされたモデルは、精度を犠牲にすることなく、トークン効率の大幅な改善を達成することがわかりました。具体的には、DRPはGSM8Kでの平均トークン使用量を917から328に削減し、精度を91.7％から94.1％に向上させ、AIMEではパフォーマンスを低下させることなく43％のトークン削減を達成します。さらなる分析により、トレーニングCoTの推論構造を生徒の推論能力に合わせることが、効果的な知識伝達とパフォーマンス向上に不可欠であることが示されています。

&lt;img src="https://arxiv.org/html/2505.13975v1/x1.png"/&gt;&lt;p&gt;Baltimore County Arizona State University, Yuxuan Jiang Dawei Li Francis Ferraro University of Maryland&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13975v1</guid><pubDate>Tue, 20 May 2025 06:15:15 +0000</pubDate></item><item><title>InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion</title><link>http://arxiv.org/abs/2505.13893v1</link><description>大規模言語モデル（LLM）における最近の進歩は、異種なオープンソースモデルを、それぞれの補完的な強みを継承する統一されたシステムに融合させる取り組みを活発化させています。既存のロジットベースの融合手法は推論効率を維持しますが、語彙次元を独立して扱い、次元間の相互作用によってエンコードされた意味的依存関係を見落としています。これらの依存関係は、トークンタイプがモデルの内部推論下でどのように相互作用するかを反映し、多様な生成行動を持つモデルを整合させるために不可欠です。これらの依存関係を明示的にモデル化するために、我々は\textbf{InfiGFusion}を提案します。これは、新しい\textit{Graph-on-Logits Distillation}（GLD）損失を用いた、初の構造を意識した融合フレームワークです。具体的には、出力ごとに上位$k$個のロジットを保持し、それらの外積をシーケンス位置全体で集約して、グローバルな共活性化グラフを形成します。ここで、ノードは語彙チャネルを表し、エッジはそれらの共同活性化を定量化します。スケーラビリティと効率を確保するために、ソートベースの閉形式近似を設計し、Gromov-Wasserstein距離の元の$O(n^4)$のコストを$O(n \log n)$に削減し、証明可能な近似保証を提供します。複数の融合設定にわたる実験により、GLDが融合の品質と安定性を一貫して向上させることが示されています。InfiGFusionは、推論、コーディング、数学にまたがる11のベンチマークにおいて、SOTAモデルおよび融合ベースラインを上回る性能を示しています。特に複雑な推論タスクにおいて強みを発揮し、Multistep Arithmeticで+35.6、Causal Judgementで+37.06のSFTに対する改善を示し、優れた多段階および関係推論能力を実証しています。

&lt;img src="https://arxiv.org/html/2505.13893v1/x1.png"/&gt;&lt;p&gt;Reallm Labs, The Hong Kong Polytechnic University, Zhejiang University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13893v1</guid><pubDate>Tue, 20 May 2025 03:55:35 +0000</pubDate></item><item><title>Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM</title><link>http://arxiv.org/abs/2505.13890v1</link><description>テスト時のスケーリングにおける最近の進歩により、大規模言語モデル（LLM）は、拡張された思考の連鎖（CoT）生成を通じて、高度な推論能力を示すことができるようになった。その潜在能力にもかかわらず、これらの推論LLM（RLM）は、少数ショットプロンプト下での性能低下など、直感的でない不安定な挙動を示すことが多く、RLMに対する現在の理解に疑問を投げかけている。本研究では、RLMの推論プロセスをより良くモデル化するために、統一されたグラフベースの分析フレームワークを導入する。我々の手法は、まず、長く冗長なCoT出力を意味的に一貫した推論ステップにクラスタリングし、次に、これらのステップ間の文脈的および論理的な依存関係を捉えるために、有向推論グラフを構築する。モデルとプロンプト体制にわたる包括的な分析を通じて、探索密度、分岐、収束率などの構造的特性が、推論の精度と強く相関していることを明らかにする。我々の発見は、プロンプト戦略がRLMの内部推論構造を大幅に再構築し、タスクの結果に直接影響を与えることを示している。提案されたフレームワークは、従来の指標を超えた推論の質を定量的に評価できるだけでなく、プロンプトエンジニアリングとLLMの認知分析のための実践的な洞察を提供する。コードとリソースは、この方向での今後の研究を促進するために公開される予定である。

&lt;img src="https://arxiv.org/html/2505.13890v1/x1.png"/&gt;&lt;p&gt;Merced, San Diego University of California, Zhen Xiong Yujun Cai Zhecheng Li Yiwei Wang University of Southern California The University of Queensland University of California&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13890v1</guid><pubDate>Tue, 20 May 2025 03:54:57 +0000</pubDate></item><item><title>Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning</title><link>http://arxiv.org/abs/2505.13866v1</link><description>最近の推論に特化した言語モデルは、最終的な答えを出す前に、長い中間的な推論パスを生成することで高い精度を達成しています。このアプローチは論理的思考を必要とする問題を解決するのに効果的ですが、長い推論パスはメモリ使用量とトークン生成のスループットを大幅に増加させ、そのようなモデルの実用的な展開を制限します。そこで、推論パスのセマンティックな疎性を利用して推論を加速する、トレーニング不要な手法であるReasoning Path Compression (RPC) を提案します。RPCは、最近生成されたクエリで構成されるセレクタウィンドウを用いて計算される、高い重要度スコアを受け取ったKVキャッシュを保持することで、KVキャッシュを定期的に圧縮します。実験の結果、RPCはQwQ-32Bの生成スループットを、完全なKVキャッシュを用いた推論と比較して最大1.60倍向上させ、AIME 2024ベンチマークでの精度低下は1.2%に抑えられました。我々の発見は、推論トレースにおけるセマンティックな疎性が圧縮に効果的に利用できることを示しており、推論LLMの効率的な展開に向けた実用的な道筋を提供します。我々のコードはhttps://github.com/jiwonsong-dev/ReasoningPathCompressionで公開されています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13866v1</guid><pubDate>Tue, 20 May 2025 03:21:52 +0000</pubDate></item><item><title>Quantum Optimization via Gradient-Based Hamiltonian Descent</title><link>http://arxiv.org/abs/2505.14670v1</link><description>機械学習の急速な進歩に伴い、一次アルゴリズムは、計算効率の高さと低いメモリ要件から、現代の最適化技術のバックボーンとして台頭してきました。近年、加速勾配法と減衰重球運動の関連性、特にハミルトン力学の枠組みにおける関連性が、連続最適化のための革新的な量子アルゴリズムの開発を促しています。そのようなアルゴリズムの一つである量子ハミルトンダウンヒル（QHD）は、量子トンネル効果を利用して鞍点や局所的極小値から脱出し、複雑な最適化ランドスケープにおけるグローバル解の発見を促進します。しかし、QHDは、古典的な勾配法と比較して収束速度が遅いことや、量子状態の非局所的な性質のために高度に非凸な問題におけるロバスト性が限られているなど、いくつかの課題に直面しています。さらに、元のQHDの定式化は主に関数値の情報に依存しており、その有効性を制限しています。古典的な手法における加速メカニズムを明らかにしてきた高解像度微分方程式からの洞察に触発され、勾配情報を取り入れることでQHDを強化し、勾配ベースQHDと呼ぶものを提案します。勾配ベースQHDは、より高速な収束を実現し、グローバル解を特定する可能性を大幅に高めます。困難な問題インスタンスに関する数値シミュレーションは、勾配ベースQHDが既存の量子および古典的な手法を少なくとも1桁上回る性能を示すことを実証しています。

&lt;img src="https://arxiv.org/html/2505.14670v1/extracted/6459833/figures/fig1_st_evolution.png"/&gt;&lt;p&gt;Bin Shi, Jiaqi Leng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14670v1</guid><pubDate>Tue, 20 May 2025 17:55:52 +0000</pubDate></item><item><title>From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling</title><link>http://arxiv.org/abs/2505.14177v1</link><description>非凸ポテンシャルから生じる分布を、非調整ランジュバンアルゴリズム（ULA）を用いてサンプリングする問題を考察します。ポテンシャルが無限遠で強凸であるという仮定の下で、離散時間ULAのドリフト近似に対する安定性を証明します。多くの状況、例えば画像逆問題において、ポテンシャルは非凸かつ非滑らかです。近接確率的勾配ランジュバンアルゴリズム（PSGLA）は、そのようなポテンシャルを扱うための一般的なアルゴリズムです。これは、forward-backward最適化アルゴリズムとULAステップを組み合わせたものです。我々の主要な安定性に関する結果とモロー包絡線の特性を組み合わせることで、非凸ポテンシャルに対するPSGLAの収束に関する最初の証明を導出することができます。合成データおよび画像逆問題の文脈において、我々の手法を実験的に検証します。特に、PSGLAは事後分布サンプリングにおいて、確率的勾配ランジュバンアルゴリズムよりも高速な収束率を示し、その復元特性を維持することを確認しました。

&lt;img src="https://arxiv.org/html/2505.14177v1/extracted/6457490/images/figure_paper_GMM_2D10000.png"/&gt;&lt;p&gt;75005, Bordeaux INP, CNRS, FRANCE &amp;Arthur Leclaire LTCI, France, France
&amp;Nicolas Papadakis Univ. Bordeaux, France
Valentin de Bortoli ENS, IMB, INRIA, IP Paris, Marien Renaud Univ. Bordeaux, PSL University Paris, Télécom Paris, UMR 5251 F-33400 Talence&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14177v1</guid><pubDate>Tue, 20 May 2025 10:29:57 +0000</pubDate></item><item><title>Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators</title><link>http://arxiv.org/abs/2505.14039v1</link><description>イオンモデルは、スティッフな常微分方程式系で記述され、計算論的神経科学と心臓学の両方において、興奮性細胞の複雑なダイナミクスをシミュレートするための基本的なツールです。人工ニューラルネットワークを用いてこれらのモデルを近似することは、その固有のスティッフネス、マルチスケール非線形性、および複数の平衡点、リミットサイクル、複雑な相互作用を含む、それらが示す広範囲な動的挙動のために、重大な課題を提起します。これまでの研究では、膜電位のダイナミクスが低次元設定で予測されてきましたが、本研究では、フーリエニューラルオペレータが、高次元におけるこれらの力学系内のすべての状態変数の進化を効果的に学習できるかどうかを調査することにより、これらの結果を拡張します。2変数のFitzHugh-Nagumoモデル、4変数のHodgkin-Huxleyモデル、41変数のO'Hara-Rudyモデルという、次元が増加する3つの確立されたイオンモデルのダイナミクスを正確に学習することにより、このアプローチの有効性を示します。フーリエニューラルオペレータのほぼ最適な構成を選択するために、学習可能なパラメータの数が制限されていない制約のない設定と、学習可能なパラメータの数が固定されている制約のあるケースという、2つのシナリオで自動ハイパーパラメータチューニングを実施しました。制約のあるアーキテクチャと制約のないアーキテクチャの両方が、検討したすべてのモデルで精度に関して同等の結果を達成しました。ただし、制約のないアーキテクチャは、トレーニング中に記録された損失関数の値から明らかなように、同様のエラーレベルを達成するために、トレーニングエポック数が約半分で済みました。これらの結果は、高次元力学系においても、フーリエニューラルオペレータが複雑なマルチスケールダイナミクスを正確に捉える能力を強調しています。

&lt;img src=""/&gt;&lt;p&gt;Edoardo Centofanti, Luca Franco Pavarino, Luca Pellegrini, Massimiliano Ghiotto&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14039v1</guid><pubDate>Tue, 20 May 2025 07:37:03 +0000</pubDate></item><item><title>ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets</title><link>http://arxiv.org/abs/2505.14016v1</link><description>彗星活動は魅力的な研究対象であり、その理解において熱物理モデルは極めて重要な役割を果たします。しかし、小天体の熱物理モデルに対する従来の数値解法は計算負荷が高く、高解像度や反復的なモデリングを必要とする研究には課題があります。この制限に対処するため、我々は機械学習アプローチを採用し、彗星の温度と水氷昇華フラックスを予測するように設計されたニューラルネットワークであるThermoONetを開発しました。性能評価の結果、ThermoONetは数値シミュレーションと比較して、地下温度において平均約2%という低い誤差を達成し、計算時間を約6桁短縮することが示されました。我々はThermoONetを適用して、彗星67P/チュリュモフ・ゲラシメンコと21P/ジャコビニ・ツィナーの水分活動をモデル化しました。それぞれロゼッタミッションとSOHO望遠鏡によって得られたこれらの彗星の水分生成率曲線をうまく適合させることで、ネットワークの有効性と効率を実証しました。さらに、ThermoONetはグローバル最適化アルゴリズムと組み合わせることで、対象天体の物理的特性を推定できることが証明されました。

&lt;img src="https://arxiv.org/html/2505.14016v1/x1.png"/&gt;&lt;p&gt;Hanlun Lei, Shunjing Zhao, Xian Shi&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14016v1</guid><pubDate>Tue, 20 May 2025 07:09:20 +0000</pubDate></item><item><title>Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning</title><link>http://arxiv.org/abs/2505.14174v1</link><description>LLMはテキストからSQLへの変換のようなコード生成タスクに効果的だが、そのコストに見合う価値はあるのだろうか？多くの最先端のアプローチは、Chain-of-Thought (CoT)、自己整合性、ファインチューニングなど、タスク固有ではないLLM技術を使用している。これらの方法は推論時にコストがかかる可能性があり、推論を伴うLLMの呼び出しが100回を超えることもあり、1クエリあたり平均0.46ドルのコストがかかる。また、モデルのファインチューニングには数千ドルかかることもある。我々は、よりコスト効率の高いテキストからSQLへの変換アプローチである「N-rep」整合性を導入する。これは、他の高価な方法と同程度のBIRDベンチマークスコアを、1クエリあたりわずか0.039ドルで達成する。N-repは、単一の表現の弱点を軽減するために、同じスキーマ入力の複数の表現を活用し、ソリューションをより堅牢にし、推論やファインチューニングなしで、より小型で安価なモデルの使用を可能にする。我々の知る限り、N-repはそのコスト範囲において最高の性能を発揮するテキストからSQLへの変換アプローチである。

&lt;img src="https://arxiv.org/html/2505.14174v1/extracted/6457829/scatter_plot_v6.png"/&gt;&lt;p&gt;Gena Co.; Cornell University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14174v1</guid><pubDate>Tue, 20 May 2025 10:28:46 +0000</pubDate></item><item><title>Nonparametric Teaching for Graph Property Learners</title><link>http://arxiv.org/abs/2505.14170v1</link><description>グラフ構造化データの特性（例えば、分子の溶解度）を推論することは、本質的にグラフからその特性への暗黙的なマッピングを学習することを含みます。この学習プロセスは、グラフ畳み込みネットワーク（GCN）のようなグラフ特性学習器にとってしばしばコストがかかります。これに対処するため、我々はグラフニューラルティーチング（GraNT）と呼ばれるパラダイムを提案し、新しいノンパラメトリックなティーチングの視点から学習プロセスを再解釈します。具体的には、後者は例の選択を通じて暗黙的に定義された（すなわち、ノンパラメトリックな）マッピングを教えるための理論的枠組みを提供します。このような暗黙的なマッピングは、グラフと特性のペアの密な集合によって実現され、GraNT教師はGCNトレーニングにおけるより速い収束を促進するために、それらのサブセットを選択します。トレーニング中のパラメータベースの勾配降下に対するグラフ構造の影響を分析的に調べ、パラメータの更新によって形作られたGCNの進化を、ノンパラメトリックティーチングにおける関数勾配降下を通じて再構成することにより、グラフ特性学習器（すなわち、GCN）を教えることが、構造を意識したノンパラメトリック学習器を教えることと整合性があることを初めて示します。これらの新しい発見は、GraNTがグラフ特性学習器の学習効率を高めることに貢献し、グラフレベル回帰（-36.62%）、グラフレベル分類（-38.19%）、ノードレベル回帰（-30.97%）、およびノードレベル分類（-47.30%）において、トレーニング時間を大幅に削減しながら、汎化性能を維持することを示しています。

&lt;img src="https://arxiv.org/html/2505.14170v1/x3.png"/&gt;&lt;p&gt;Chen Zhang, Ngai Wong, Weixin Bu, Yik-Chung Wu, Zeyi Ren, Zhengwu Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14170v1</guid><pubDate>Tue, 20 May 2025 10:23:30 +0000</pubDate></item><item><title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title><link>http://arxiv.org/abs/2505.13938v1</link><description>我々は、Leanにおけるエンドツーエンドで検証されたコード生成のための、高品質で厳選された161個の問題からなるベンチマーク${\rm C{\small LEVER}}$を紹介する。各問題は、(1) 隠された正解の仕様に一致する仕様を生成するタスク、および (2) この仕様を証明可能に満たすLeanの実装を生成するタスクから構成される。従来のベンチマークとは異なり、${\rm C{\small LEVER}}$はテストケースによる教師あり学習、LLMによって生成されたアノテーション、および実装ロジックを漏洩させたり、自明な解を許容したりする仕様を回避する。すべての出力は、機械的に検証可能な正しさを保証するために、Leanの型チェッカーを使用して事後的に検証される。我々は、最先端の言語モデルに基づくいくつかのfew-shotおよびエージェント型アプローチを評価するために${\rm C{\small LEVER}}$を使用する。これらの手法はすべて完全な検証を達成するのに苦労しており、プログラム合成と形式的推論のための挑戦的なフロンティアベンチマークとして確立されている。我々のベンチマークは、GitHub(https://github.com/trishullab/clever)およびHuggingFace(https://huggingface.co/datasets/amitayusht/clever)で見つけることができる。すべての評価コードもオンライン(https://github.com/trishullab/clever-prover)で入手可能である。

&lt;img src="https://arxiv.org/html/2505.13938v1/extracted/6456689/images/img-clever-overview.png"/&gt;&lt;p&gt;Amitayush Thakur &amp;Jasper Lee &amp;George Tsoukalas &amp;Meghana Sistla &amp;Matthew Zhao &amp;Stefan Zetzsche &amp;Greg Durrett &amp;Yisong Yue &amp;Swarat Chaudhuri&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13938v1</guid><pubDate>Tue, 20 May 2025 05:15:47 +0000</pubDate></item><item><title>Enhancing Abstractive Summarization of Scientific Papers Using Structure Information</title><link>http://arxiv.org/abs/2505.14179v1</link><description>科学論文の抽象的要約は常に研究の焦点であったが、既存の手法は主に2つの課題に直面している。第一に、ほとんどの要約モデルは、論文を単語のシーケンスとして扱うEncoder-Decoderアーキテクチャに依存しており、科学論文に内在する構造化された情報を十分に捉えられていない。第二に、既存の研究では、キーワードマッピングや特徴エンジニアリングを用いて構造情報を特定することが多いが、これらの手法は科学論文の構造的な柔軟性に対応できず、異なる分野にわたるロバスト性に欠ける。これらの課題に対処するため、我々は科学論文内の構造的機能の自動認識を活用した、2段階の抽象的要約フレームワークを提案する。第一段階では、多数の科学論文から章タイトルを標準化し、構造的機能認識のための大規模なデータセットを構築する。次に、分類器を訓練して、主要な構造的要素（例：背景、方法、結果、考察）を自動的に識別し、よりバランスの取れた要約を生成するための基盤を提供する。第二段階では、Longformerを用いてセクション間の豊富な文脈関係を捉え、文脈を考慮した要約を生成する。2つのドメイン固有の科学論文要約データセットで実施した実験により、我々の手法が高度なベースラインを上回り、より包括的な要約を生成することが示された。コードとデータセットは、https://github.com/tongbao96/code-for-SFR-AS でアクセスできる。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14179v1</guid><pubDate>Tue, 20 May 2025 10:34:45 +0000</pubDate></item><item><title>Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search</title><link>http://arxiv.org/abs/2505.14156v1</link><description>セッション検索は、ユーザーの複雑な情報ニーズを満たすための一連のインタラクティブなクエリとアクションを伴います。現在の戦略は通常、深い意味理解のために逐次的なモデリングを優先し、インタラクションにおけるグラフ構造を見落としています。構造情報の捕捉に焦点を当てたアプローチもありますが、ドキュメントに対して一般化された表現を使用しており、単語レベルの意味モデリングを無視しています。本稿では、最近の大規模言語モデル（LLM）の力を活用することで、テキストベースとグラフベースの両方のアプローチの利点を活かすことを目的としたSymbolic Graph Ranker（SGR）を提案します。具体的には、まず、セッショングラフをテキストに変換するための一連の記号文法規則を導入します。これにより、セッション履歴、インタラクションプロセス、およびタスク指示をLLMへの入力としてシームレスに統合できます。さらに、テキストコーパスで事前学習されたLLMと、グラフからテキストへの文法を使用して生成する記号言語との間の自然な乖離を考慮し、テキスト形式でグラフ構造を捉えるLLMの能力を向上させることを目的としています。これを実現するために、リンク予測、ノードコンテンツ生成、生成的な対照学習など、一連の自己教師あり記号学習タスクを導入し、LLMが粗粒度から細粒度までトポロジー情報を捉えることができるようにします。2つのベンチマークデータセット、AOLとTiangong-STでの実験結果と包括的な分析により、提案アプローチの優位性が確認されました。私たちのパラダイムはまた、従来の検索戦略と最新のLLMの間のギャップを埋める、斬新で効果的な方法論を提供します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14156v1</guid><pubDate>Tue, 20 May 2025 10:05:06 +0000</pubDate></item><item><title>Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning</title><link>http://arxiv.org/abs/2505.13994v1</link><description>検索拡張生成 (RAG) システムは、大規模言語モデル (LLM) に外部知識を提供しますが、大規模な知識グラフにスケールする際に効率と精度のトレードオフに苦慮します。既存のアプローチは、多くの場合、モノリシックなグラフ検索に依存しており、単純なクエリに対して不必要な遅延が発生し、複雑なマルチホップの質問に対して断片的な推論が生じます。これらの課題に対処するため、本論文では、質問駆動型のセマンティックグラフ分割と協調的なサブグラフ検索によってこれらの制限に対処する、マルチエージェント RAG フレームワークである SPLIT-RAG を提案します。この革新的なフレームワークは、まずリンクされた情報のセマンティック分割を作成し、次にタイプ特化型知識ベースを使用してマルチエージェント RAG を実現します。属性を考慮したグラフ分割は、知識グラフをセマンティックに一貫性のあるサブグラフに分割し、サブグラフが異なるクエリタイプと一致するようにします。一方、軽量な LLM エージェントが分割されたサブグラフに割り当てられ、検索中に該当するパーティションのみがアクティブ化されるため、検索スペースが削減され、効率が向上します。最後に、階層的なマージモジュールは、論理的な検証を通じてサブグラフから得られた回答間の矛盾を解消します。広範な実験的検証により、既存のアプローチと比較して大幅な改善が示されています。

&lt;img src="https://arxiv.org/html/2505.13994v1/x1.png"/&gt;&lt;p&gt;Abu Dhabi, Austalia Mohamed Bin Zayed University of Artificial Intelligence, NSW, Ruiyi Yang Hao Xue Imran Razzak Hakim Hacid Flora D. Salim University of New South Wales, Sydney, UAE, UAE Technology Innovation Institute&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13994v1</guid><pubDate>Tue, 20 May 2025 06:44:34 +0000</pubDate></item><item><title>MLZero: A Multi-Agent System for End-to-end Machine Learning Automation</title><link>http://arxiv.org/abs/2505.13941v1</link><description>既存のAutoMLシステムは機械学習（ML）の自動化を進歩させてきましたが、特にマルチモーダルデータを扱う場合、依然としてかなりの手動設定と専門家の入力が必要です。そこで、大規模言語モデル（LLM）を搭載した新しいマルチエージェントフレームワークであるMLZeroを導入します。これにより、最小限の人的介入で、多様なデータモダリティにわたるエンドツーエンドのML自動化が可能になります。まず、認知知覚モジュールが採用され、生のマルチモーダル入力を、後続のワークフローを効果的に導く知覚的コンテキストに変換します。LLMの主要な制限事項（幻覚コードの生成や古いAPI知識など）に対処するため、セマンティックメモリとエピソードメモリを使用して、反復的なコード生成プロセスを強化します。MLZeroはMLE-Bench Liteで優れたパフォーマンスを発揮し、成功率とソリューション品質の両方で競合他社を上回り、6つの金メダルを獲得しました。さらに、多様なデータモダリティにわたる25のより困難なタスクを含む、独自のマルチモーダルAutoMLエージェントベンチマークで評価したところ、MLZeroは成功率0.92（+263.6％）と平均ランク2.28で、競合手法を大幅に上回りました。私たちのアプローチは、コンパクトな8B LLMでも堅牢な有効性を維持し、既存のソリューションのフルサイズシステムを凌駕します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13941v1</guid><pubDate>Tue, 20 May 2025 05:20:53 +0000</pubDate></item><item><title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title><link>http://arxiv.org/abs/2505.14479v1</link><description>大規模言語モデル（LLM）は、数学的証明生成のような、厳密な論理的推論と記号推論を必要とする形式的な領域で苦戦します。私たちは、この課題を克服するために、LLMの生成能力と構造化されたコンポーネントを組み合わせたニューロシンボリックなアプローチを提案します。概念実証として、幾何学の問題に焦点を当てます。私たちのアプローチは二段階です。（1）類似の問題を検索し、その証明をLLMの指針として利用します。（2）形式的な検証器が生成された証明を評価し、フィードバックを提供することで、モデルが誤った証明を修正するのを支援します。私たちの手法が、OpenAIのo1モデルの証明精度を大幅に向上させること（58%～70%の改善）を示します。類似の問題と検証器のフィードバックの両方が、これらの改善に貢献しています。より広く言えば、証明可能な正しい結論を生成するLLMに移行することで、その信頼性、精度、一貫性を劇的に向上させ、信頼性を必要とする複雑なタスクや重要な現実世界のアプリケーションを開拓できる可能性があります。

&lt;img src="https://arxiv.org/html/2505.14479v1/x1.png"/&gt;&lt;p&gt;The Hebrew University of Jerusalem&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14479v1</guid><pubDate>Tue, 20 May 2025 15:13:32 +0000</pubDate></item><item><title>Creative Preference Optimization</title><link>http://arxiv.org/abs/2505.14442v1</link><description>大規模言語モデル（LLM）は自然言語生成タスクにおいて目覚ましい性能を示していますが、真に創造的なコンテンツ、すなわち新規性、多様性、驚き、そして品質によって特徴づけられるコンテンツを生成する能力は依然として限定的です。LLMの創造性を高める既存の手法は、多様性や特定のタスクに狭く焦点を当てることが多く、創造性の多面的な性質を一般化可能な方法で捉えられていません。本研究では、創造的嗜好最適化（CrPO）という、複数の創造性次元からの信号をモジュール式に嗜好最適化の目的に注入する新しいアラインメント手法を提案します。CrPOとMuCE（20万件以上の人間が生成した応答と30以上の心理学的創造性評価からの評価を含む、大規模な人間嗜好データセット）を用いて、いくつかのモデルの創造性を強化したバージョンを学習および評価します。我々のモデルは、GPT-4oを含む強力なベースラインを、自動評価と人間による評価の両方で上回り、高い出力品質を維持しながら、より斬新で多様で驚くべき生成物を生成します。NoveltyBenchでの追加評価により、我々のアプローチの一般化可能性がさらに確認されます。これらの結果は、嗜好フレームワーク内で創造性を直接最適化することが、出力品質を損なうことなく、LLMの創造的な能力を向上させるための有望な方向性であることを示しています。

&lt;img src="https://arxiv.org/html/2505.14442v1/x1.png"/&gt;&lt;p&gt;EPFL, Pennsylvania State University mahammad.ismayilzada@epfl.ch, Università della Svizzera Italiana&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14442v1</guid><pubDate>Tue, 20 May 2025 14:43:41 +0000</pubDate></item><item><title>PRL: Prompts from Reinforcement Learning</title><link>http://arxiv.org/abs/2505.14412v1</link><description>効果的なプロンプトエンジニアリングは、LLMの能力を最大限に引き出すための中心的な課題であり続けています。適切に設計されたプロンプトはパフォーマンスを劇的に向上させることができますが、それらを作成するには通常、専門家の直感とタスクに対する微妙な理解が必要です。さらに、最も影響力のあるプロンプトは、人間の知覚を逃れる可能性がありながらも、LLMの動作を導く上で重要な、微妙な意味的合図に依存することがよくあります。本論文では、自動プロンプト生成のための新しいRLベースのアプローチであるPRL（強化学習からのプロンプト）を紹介します。以前の方法とは異なり、PRLはトレーニング中に見られなかった新しい少数ショットの例を生成できます。我々のアプローチは、テキスト分類、単純化、要約など、さまざまなベンチマークで最先端のパフォーマンスを達成しています。分類タスクでは、APEを2.58%、EvoPromptを1.00%上回っています。さらに、要約タスクの平均ROUGEスコアをAPEより4.32、EvoPromptより2.12向上させ、単純化のSARIスコアをAPEより6.93、EvoPromptより6.01向上させています。我々のコードはhttps://github.com/Batorskq/prl で入手できます。

&lt;img src=""/&gt;&lt;p&gt;Paweł Batorski  Adrian Kosmala Paul Swoboda Heinrich Heine Universität Düsseldorf&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14412v1</guid><pubDate>Tue, 20 May 2025 14:26:19 +0000</pubDate></item><item><title>YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering</title><link>http://arxiv.org/abs/2505.14279v1</link><description>大規模言語モデル（LLM）は、現代の検索エンジンにおける科学的な質問応答を推進していますが、その評価の堅牢性は十分に探求されていません。私たちは、LLM評価者の楽観バイアスを軽減するために、詳細なルーブリックベースの評価と強化学習を組み合わせたオープンソースフレームワークであるYESciEvalを紹介します。敵対的なバリアントを含む、学際的な科学Q&amp;Aデータセットを、複数のLLMからの評価スコアとともに公開します。プロプライエタリなモデルや人間のフィードバックに依存せず、私たちのアプローチはスケーラブルでコストのかからない評価を可能にします。信頼性の高いLLMをジャッジとして活用するモデルを進歩させることで、本研究はAIアライメントをサポートし、科学的探求と汎用人工知能に不可欠な、堅牢で透明性の高い評価を促進します。

&lt;img src="https://arxiv.org/html/2505.14279v1/x1.png"/&gt;&lt;p&gt;Germany, Germany Leibniz Universität Hannover, Hamed Babaei Giglou, Hannover, Jennifer D’Souza, Quentin Münch TIB Leibniz Information Centre for Science, Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14279v1</guid><pubDate>Tue, 20 May 2025 12:30:46 +0000</pubDate></item><item><title>DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery</title><link>http://arxiv.org/abs/2505.13940v1</link><description>AI4Scienceの分野において、大規模言語モデル（LLM）は、複雑な科学的意味論の解析、学際的な知識の統合、そして重要なタスク研究の支援において大きな可能性を示しています。しかし、創薬の分野では、専門的なデータによる事前学習、コンテキストウィンドウの拡張、インターネット検索による最適化にもかかわらず、既存のLLMは、大量のマルチモーダルかつ異種データの処理、ドメイン知識の動的な更新の遅延、複雑な計算タスクの結果予測における信頼性の不足といった課題に直面しています。これらの課題に対処するため、我々は創薬のためのパラメータ化された推論を持つLLMベースのエージェントであるDrugPilotを提案します。DrugPilotは、そのパラメータ化された推論アーキテクチャを通じて、従来のエンドツーエンドのLLM予測アプローチの主要な制限に対処します。このエージェントシステムは、創薬パイプラインの主要な段階をサポートし、多段階の研究タスクの自動計画と実行を促進します。マルチモーダルな薬剤データ分析（公開データセットとユーザーが提出したデータの両方を組み込む）という重要な課題に対処するために、インタラクティブなパラメータ化されたメモリプールを開発しました。この革新的なコンポーネントは、現実世界の薬剤データをパラメータ化された表現に標準化し、同時にマルチターンの対話における効率的な知識検索を可能にするとともに、テキストベースのデータ伝送に固有の情報損失を軽減します。さらに、モデルのファインチューニングと評価のために、8つの必須の創薬タスクにわたる薬剤インストラクションデータセットを作成しました。Berkeleyの関数呼び出し評価フレームワークに基づいて、DrugPilotは、我々の創薬ツールインストラクションデータセットにおいて、既存のエージェント（例：ReAct、LoT）を上回る、最も高度なツール呼び出し能力を示しました。具体的には、単純なタスクで98.0%、複数のタスクで93.5%、マルチターンのタスクで64.0%のタスク完了率を達成しています。

&lt;img src="https://arxiv.org/html/2505.13940v1/x1.png"/&gt;&lt;p&gt;Kun Li, Shoupeng Wang, Wenbin Hu, Zhennan Wu&lt;/p&gt;&lt;p&gt;
School of Computer Science, Wuhan University
School of Mathematics and Statistics, Wuhan University&lt;/p&gt;</description><guid isPermaLink="false">2505.13940v1</guid><pubDate>Tue, 20 May 2025 05:18:15 +0000</pubDate></item><item><title>Knowledge Graph Based Repository-Level Code Generation</title><link>http://arxiv.org/abs/2505.14394v1</link><description>大規模言語モデル（LLM）の最近の進歩は、自然言語クエリからのコード生成を大きく変えました。しかし、その広範な知識と高品質なコードを生成する能力にもかかわらず、LLMは、特に進化するコードベースにおいて、文脈の正確性に苦労することがよくあります。現在のコード検索および検索方法は、取得された結果の品質と文脈的関連性の両方において、しばしば堅牢性に欠け、最適でないコード生成につながります。本論文では、リポジトリレベルのタスクの文脈において、コード検索と検索を改善し、より高品質なコード生成につながる、新しい知識グラフベースのアプローチを紹介します。提案されたアプローチは、コードリポジトリをグラフとして表現し、構造的および関係的な情報を捉え、文脈を意識したコード生成を強化します。私たちのフレームワークは、文脈的関連性を改善し、ファイル間のモジュール依存関係を追跡し、より堅牢なコードを生成し、既存のコードベースとの一貫性を確保するために、コード検索のためのハイブリッドアプローチを採用しています。リポジトリレベルのコード生成ベンチマークであるEvolutionary Code Benchmark（EvoCodeBench）データセットで提案されたアプローチをベンチマークし、私たちの方法がベースラインアプローチを大幅に上回ることを示します。これらの発見は、知識グラフベースのコード生成が、堅牢で文脈に敏感なコーディング支援ツールを前進させる可能性があることを示唆しています。

&lt;img src="https://arxiv.org/html/2505.14394v1/x1.png"/&gt;&lt;p&gt;Mihir Athale1, Vishal Vaddina2&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14394v1</guid><pubDate>Tue, 20 May 2025 14:13:59 +0000</pubDate></item><item><title>DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation</title><link>http://arxiv.org/abs/2505.14163v1</link><description>大規模言語モデル（LLM）エージェントは、複雑なデータサイエンス問題を解決するためのコード生成において有望な性能を示しています。最近の研究は主に、検索、サンプリング、計画技術の改善を通じてインコンテキスト学習を強化することに焦点を当てており、推論中に問題に取り組む順序の重要性を見過ごしています。本研究では、カリキュラム学習を活用した新しい推論時最適化フレームワークであるDSMentorを開発しました。カリキュラム学習とは、学習者が上達するにつれて、より簡単なタスクから始めて徐々に複雑なタスクに移行する戦略であり、困難なデータサイエンスタスクにおけるLLMエージェントの性能を向上させます。私たちのメンターガイド付きフレームワークは、データサイエンスのタスクを難易度順に整理し、長期的な記憶を拡大して過去の経験を保持し、エージェントの学習の進捗を導き、蓄積された知識をより効果的に活用できるようにします。DSEvalおよびQRDataベンチマークでの広範な実験を通じてDSMentorを評価しました。実験の結果、Claude-3.5-Sonnetを使用したDSMentorは、DSEvalおよびQRDataにおいて、ベースラインエージェントと比較して合格率が最大5.2%向上することが示されました。さらに、DSMentorはより強力な因果推論能力を示し、Program-of-Thoughtsプロンプトを使用したGPT-4と比較して、因果関係の問題における合格率が8.8%向上しました。私たちの研究は、人間の学習プロセスを反映し、カリキュラムベースの推論最適化を通じてLLMの性能を向上させるための新たな道を開き、推論中に知識を蓄積し活用するための効果的な戦略を開発することの重要性を強調しています。

&lt;img src="https://arxiv.org/html/2505.14163v1/x1.png"/&gt;&lt;p&gt;AWS AI, He Wang Carnegie Mellon University Alexander Hanbo Li, Hideo Kobayashi, Sheng Zhang, Yiqun Hu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14163v1</guid><pubDate>Tue, 20 May 2025 10:16:21 +0000</pubDate></item><item><title>MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem</title><link>http://arxiv.org/abs/2505.14148v1</link><description>数理モデリングは、科学的発見と工学の実践の基礎であり、物理学、生物学、経済学などの分野にわたって、現実世界の問題を形式的なシステムに変換することを可能にします。あらかじめ定義された定式化を前提とする数学的推論とは異なり、モデリングには、オープンエンドな問題分析、抽象化、および原則に基づいた形式化が必要です。大規模言語モデル（LLM）は強力な推論能力を示していますが、厳密なモデル構築においては不十分であり、現実世界の問題解決における有用性を制限しています。この目的のために、LLMを活用した現実世界の数理モデリングのタスクを形式化します。ここでは、エージェントが問題を分析し、ドメインに適した定式化を構築し、完全なエンドツーエンドのソリューションを生成する必要があります。2000年から2025年までの数理モデリングコンテスト（MCM/ICM）から厳選された111の問題からなるMM-Benchを導入します。これらの問題は、物理学、生物学、経済学などの10の多様なドメインに及びます。このタスクに取り組むために、数理モデリングを、オープンエンドな問題分析、構造化されたモデル定式化、計算問題解決、レポート作成の4つの段階に分解する、専門家から着想を得たフレームワークであるMM-Agentを提案します。MM-Benchでの実験では、MM-Agentがベースラインエージェントを大幅に上回り、GPT-4oを使用してタスクあたりわずか15分と0.88ドルで、人間の専門家のソリューションよりも11.88％の改善を達成しています。さらに、公式のMCM/ICMプロトコルの下で、MM-Agentは2つの学部生チームがMCM/ICM 2025でファイナリスト賞（\textbf{27,456チーム中上位2.0％}）を受賞するのを支援し、モデリングコパイロットとしての実用的な有効性を示しました。私たちのコードはhttps://github.com/usail-hkust/LLM-MM-Agentで入手できます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14148v1</guid><pubDate>Tue, 20 May 2025 09:55:31 +0000</pubDate></item><item><title>Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models</title><link>http://arxiv.org/abs/2505.13828v1</link><description>積層造形は、複雑な設計の製造を可能にし、廃棄物を最小限に抑えますが、欠陥やプロセス異常に関連する課題に直面しています。本研究では、トレーニングデータセットではなく、文献から取得した画像や説明文などの情報を活用し、さまざまな積層造形プロセスにおける異常検出を自動化する、新しいマルチモーダルRetrieval-Augmented Generation（RAG）ベースのフレームワークを提案します。このフレームワークは、科学文献からのテキストおよび画像検索と、マルチモーダル生成モデルを統合し、レーザー粉末床溶融結合（L-PBF）環境におけるゼロショット異常識別、分類、および説明生成を実行します。提案されたフレームワークは、オークリッジ国立研究所からの4つのL-PBF製造データセットで評価され、さまざまなプリンターメーカー、モデル、および材料を特徴としています。この評価は、追加のトレーニングを必要とせずに、多様な画像に対するフレームワークの適応性と一般化可能性を示しています。提案されたフレームワーク内でMLLMとしてQwen2-VL-2BとGPT-4o-miniを使用した比較分析では、GPT-4o-miniが製造異常の分類においてQwen2-VL-2Bおよび比例ランダムベースラインよりも優れていることが強調されています。さらに、RAGシステムの評価により、検索メカニズムを組み込むことで、ハルシネーションのリスクを軽減し、追加情報を提供することにより、平均精度が12％向上することが確認されています。提案されたフレームワークは、新たな研究を統合することで継続的に更新でき、進化するAM技術の状況にシームレスに適応できます。このスケーラブルで自動化された、ゼロショット対応のフレームワークは、AM異常分析を効率化し、効率と精度を向上させます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13828v1</guid><pubDate>Tue, 20 May 2025 02:18:22 +0000</pubDate></item><item><title>Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials</title><link>http://arxiv.org/abs/2505.14606v1</link><description>ニューラルネットワーク原子間ポテンシャルの最近の進歩は、有望な研究方向として登場しています。しかし、一般的な深層学習モデルは、物理法則に基づいた補助的な制約を欠いていることが多く、物理ベースの正則化を通じて学習を加速し、忠実度を向上させる可能性があります。本研究では、自己教師あり学習で静電相互作用を学習するために、メッセージパッシングフレームワーク内でポアソン方程式を強制する汎用的なプラグインモジュールである$\Phi$-Moduleを導入します。具体的には、各原子ごとの表現が離散化されたポアソン方程式を満たすように促し、与えられた分子グラフの学習可能なラプラシアン固有基底係数にリンクされたポテンシャル$\boldsymbol{\phi}$と対応する電荷密度$\boldsymbol{\rho}$を取得できるようにします。次に、総エネルギー予測の改善に不可欠な静電エネルギー項を導出します。このアプローチは、わずかな計算オーバーヘッドで、既存のニューラルポテンシャルにシームレスに統合できます。OE62およびMD22ベンチマークでの実験により、$\Phi$-Moduleと組み合わせたモデルが、ベースラインモデルよりも堅牢な改善を達成することが確認されています。OE62のエラー削減範囲は4.5％から17.8％であり、MD22では、$\Phi$-Moduleを搭載したベースラインが14ケース中5ケースで最高の結果を達成しています。私たちの結果は、ニューラル原子間ポテンシャルに第一原理制約を組み込むことで、ハイパーパラメータに優しく、メモリ効率が良く、トレーニングが軽量でありながら、パフォーマンスを大幅に向上させることができることを強調しています。コードは\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}で入手できます。

&lt;img src="https://arxiv.org/html/2505.14606v1/x1.png"/&gt;&lt;p&gt;Innopolis University, Maksim Zhdanov AIRI, NUST MISIS &amp;Vladislav Kurenkov AIRI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14606v1</guid><pubDate>Tue, 20 May 2025 16:54:25 +0000</pubDate></item><item><title>RT-APNN for Solving Gray Radiative Transfer Equations</title><link>http://arxiv.org/abs/2505.14144v1</link><description>グレイ放射伝達方程式（GRTE）は、高次元かつマルチスケールな問題であり、従来の数値解法にとって大きな計算上の課題を提起します。物理情報ニューラルネットワーク（PINN）や漸近保存ニューラルネットワーク（APNN）を含む現在の深層学習アプローチは、主に低次元または線形GRTEに限定されています。これらの課題に対処するため、我々はAPNNを拡張した革新的なフレームワークである放射伝達漸近保存ニューラルネットワーク（RT-APNN）を提案します。RT-APNNは、複数のニューラルネットワークをまとまりのあるアーキテクチャに統合し、トレーニング時間を短縮しながら、高い解の精度を保証します。長期シミュレーションや複雑な境界条件の複雑さに取り組むために、事前学習やマルコフ連鎖モンテカルロ（MCMC）適応サンプリングなどの高度な技術が採用されています。RT-APNNは、マーシャック波問題をシミュレートすることに成功した最初の深層学習手法です。数値実験により、精度と計算効率の両方において、APNNやMD-APNNを含む既存の手法よりも優れていることが示されています。さらに、RT-APNNは高次元の非線形問題を解くことに優れており、科学および工学における多様なアプリケーションの可能性を強調しています。

&lt;img src="https://arxiv.org/html/2505.14144v1/x1.png"/&gt;&lt;p&gt;Han Wang, Wengu Chen, Xizhe Xie, Zheng Ma&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14144v1</guid><pubDate>Tue, 20 May 2025 09:48:50 +0000</pubDate></item><item><title>Extending orbital-optimized density functional theory to L-edge XPS and beyond: Spin-orbit coupling via non-orthogonal quasi-degenerate perturbation theory</title><link>http://arxiv.org/abs/2505.14570v1</link><description>2pホール状態につながる内殻電子結合エネルギー（CEBE）の量子力学的計算は、L端X線光電子分光法（XPS）およびより高エネルギー側の端の解釈に関連する。軌道最適化密度汎関数理論（OO-DFT）は、K端CEBEを正確に予測するが、L端およびより高エネルギー側の端における顕著なスピン軌道結合（SOC）の存在によって課題が生じる。OO-DFTをL端およびより高エネルギー側の端に拡張するために、我々の手法は、スカラー相対論的、スピン制限OO-DFTを利用して、選択された内殻（例えば、可能な6つの2pスピン軌道をすべてイオン化する）に対応する内殻ホール状態の最小限の準縮退基底を構築する。次に、非直交配置間相互作用（NOCI）を使用して、この準縮退モデル空間の行列式におけるSOCを含む完全なハミルトニアンの行列要素を作成する。Dirac-Coulomb-Breit（DCB）ハミルトニアンでパラメータ化された遮蔽された1電子SOC演算子を使用すると、第3周期原子の二重項分裂（DS）値は、実験とほぼ定量的に一致する。結果として得られるNOCI固有値は、（スカラー）OO-DFT CEBEの平均によってシフトされ、動的相関について補正されたCEBE（SOCによって分裂）が得られる。気相分子の計算結果と実験結果を比較すると、DCB遮蔽1電子SOC演算子を使用したSCAN汎関数（NO-QDPT/SCAN）によるNO-QDPTは、第3周期原子を含む分子のL端CEBEに対して約0.2 eVの精度であることが確立される。ただし、このNO-QDPTアプローチは、特に原子番号が増加するにつれて、3d遷移金属系列の中間から始まる第4周期元素では精度が低下する。

&lt;img src="https://arxiv.org/html/2505.14570v1/x2.png"/&gt;&lt;p&gt;Diptarka Hait, Leonardo A. Cunha, Martin Head-Gordon, Richard Kang&lt;/p&gt;&lt;p&gt;Department of Chemistry and The PULSE Institute, Stanford University, Stanford, California 94305, United States
Kenneth S. Pitzer Center for Theoretical Chemistry, Department of Chemistry, University of California, Berkeley, California 94720, USA&lt;/p&gt;</description><guid isPermaLink="false">2505.14570v1</guid><pubDate>Tue, 20 May 2025 16:28:52 +0000</pubDate></item><item><title>Path-integral molecular dynamics with actively-trained and universal machine learning force fields</title><link>http://arxiv.org/abs/2505.14245v1</link><description>核量子効果（NQEs）を考慮に入れると、有限温度における材料特性が大きく変化する可能性がある。経路積分分子動力学（PIMD）法を用いた原子モデリングは、このような効果を完全に考慮できるが、計算効率が高く正確な原子間相互作用モデルが必要となる。経験的ポテンシャルは高速だが、十分な精度がない可能性があり、量子力学的計算は非常に正確だが、計算コストが高い。機械学習された原子間ポテンシャルは、この課題に対する解決策を提供し、密度汎関数理論（DFT）計算と比較して高い計算効率を維持しながら、ほぼ量子力学的な精度を実現する。この文脈において、MLIP-2ソフトウェアパッケージのモーメントテンソルポテンシャル（MTPs）を、i-PIソフトウェアパッケージを用いたPIMD計算に統合するためのインターフェースが開発された。このインターフェースは、ポテンシャルの能動学習と、リチウムハイドライド（LiH）およびシリコン（Si）システムにおける格子定数と熱膨張係数の温度依存性、および動径分布関数といった材料特性に対するNQEsの影響を調査するために適用された。結果は、実験データ、準調和近似計算、およびユニバーサル機械学習力場MatterSimからの予測と比較された。これらの比較により、MTP-PIMDアプローチの高い精度と有効性が実証された。

&lt;img src="https://arxiv.org/html/2505.14245v1/x1.png"/&gt;&lt;p&gt;A.A. Solovykh, A.V. Shapeev, I.S. Novikov, N.E. Rybin&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.14245v1</guid><pubDate>Tue, 20 May 2025 11:55:22 +0000</pubDate></item><item><title>Physics-Guided Sequence Modeling for Fast Simulation and Design Exploration of 2D Memristive Devices</title><link>http://arxiv.org/abs/2505.13882v1</link><description>メムリスティブデバイスにおけるヒステリシススイッチングダイナミクスのモデリングは、イオンおよび電子輸送プロセスの結合により、計算負荷が高い。この課題は、未開拓な高次元設計空間を持つ新興の二次元（2D）デバイスにおいて特に重要である。本研究では、高精度な有限体積法（FV）電荷輸送シミュレーションと、動的な電流-電圧特性を予測するための長短期記憶（LSTM）人工ニューラルネットワーク（ANN）を統合した、物理に基づいたモデリングフレームワークを導入する。物理的に根拠のあるシミュレーションデータで学習されたANNサロゲートは、FVモデルと比較して4桁以上の高速化を達成し、物理的に意味のある入力パラメータへの直接アクセスと、代表的な正規化誤差&lt;1%の高い精度を維持する。これにより、実験データからの逆モデリング、メトリックマッピングと感度分析による設計空間探索、制約付き多目的設計最適化など、これまで計算上不可能だった反復的なタスクが可能になる。重要なことに、このフレームワークは、基礎となるFVモデルへの直接的なリンクを通じて、キャリア密度、空孔分布、静電ポテンシャルなどの詳細な空間ダイナミクスへのアクセスを提供し、物理的な解釈可能性を維持する。我々のアプローチは、新興の2Dメムリスティブおよびニューロモルフィックデバイスの効率的な探索、解釈、モデル駆動設計のためのスケーラブルなフレームワークを確立する。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13882v1</guid><pubDate>Tue, 20 May 2025 03:43:20 +0000</pubDate></item><item><title>Phonon Mean Free Path Spectroscopy By Raman Thermometry</title><link>http://arxiv.org/abs/2505.14506v1</link><description>本研究では、ラマン温度測定法がフォノン平均自由行程（PMFP）分光法として利用可能であることを、バルクシリコン試料を用いて実証します。我々の実験手法は、時間領域または周波数領域での熱反射率測定や過渡熱格子分光法における、異なる特性長スケール$l_{c}$の変化と類似しています。$l_{c}$の代わりに、単一レーザーラマン温度測定（1LRT）測定中に、レーザー焦点スポットサイズ（$w_{e}$）と光の侵入深さ（$h_{\alpha}$）を変化させます。最大の$w_{e}$値では、導出された有効熱伝導率$\kappa_{eff}$は、シリコンのバルク熱伝導率$\kappa_{bulk}$に収束します。しかし、$w_{e}$値が小さくなるにつれて、$\kappa_{eff}$値の顕著な増加が観察され、293Kで最大5.3倍、200Kで最大8.3倍に達します。この現象は主に準弾道的フォノン輸送に起因すると考えています。その結果、測定された$\kappa_{eff}(w_{e})$の傾向を、線形化されたフォノンボルツマン輸送方程式（BTE）の第一原理計算から導出される、熱蓄積関数$\kappa_{cum}$とそのフォノン平均自由行程$l_{ph}$への依存性と比較することができます。$w_{e}$の変化は実験的に煩雑になる可能性があるため、1LRT中に適用するラマンレーザー波長$\lambda$を介して、$h_{\alpha}(\lambda)$を変化させることも提案します。この点に関して、4つの異なる$\lambda$値に対してステップ状の$\kappa_{eff}(\lambda)$の傾向を示す原理実証1LRT測定を提示し、これも準弾道的フォノン輸送の観点から解釈します。我々の結果は、1LRTに基づく将来のPMFP分光法の基礎となり、複雑なフォノン輸送物理の理解を深めるために、$\kappa$値だけでなく$\kappa_{\text{cum}}$の傾向を比較することで、最先端の理論と直接比較することができます。

&lt;img src="https://arxiv.org/html/2505.14506v1/extracted/6453493/images/Bild1_V4.png"/&gt;&lt;p&gt;Dwaipayan Paul, Giuseppe Romano, Gordon Callsen, Guillaume Würsch, Jana Lierath, Julian Themann, Katharina Dudde, Mahmoud Elhajhasan, Nakib H. Protik&lt;/p&gt;&lt;p&gt;Institut für Festkörperphysik, Universität Bremen, Otto-Hahn-Allee 1, 28359 Bremen, Germany
Institut für Physik and Center for the Science of Materials Berlin (CSMB), Humboldt-Universität zu Berlin, 12489 Berlin, Germany
MIT-IBM Watson AI Lab, 214 Main St., Cambridge, Massachusetts 02141, USA&lt;/p&gt;</description><guid isPermaLink="false">2505.14506v1</guid><pubDate>Tue, 20 May 2025 15:35:40 +0000</pubDate></item><item><title>Infrared markers of topological phase transitions in quantum spin Hall insulators</title><link>http://arxiv.org/abs/2505.14277v1</link><description>第一原理計算を用いて、赤外光応答が二次元量子スピンホール絶縁体（QSHI）のトポロジカル相と自明な相を区別できることを示します。最近実験的に実現されたゲルマネンとジャクチンガイトを、赤外スペクトルが遷移を跨いで不連続になる典型的な系として紹介します。これは、ボルン有効電荷の値が突然かつ大きく離散的にジャンプする（最大2）ためです。これらの材料では、電界効果セットアップで外部静電ポテンシャルを印加することで、トポロジカル相転移を誘起できます。我々の結果は、低エネルギーのケーン・メレモデルの枠組みで合理化され、材料のエネルギーギャップが赤外活性フォノン周波数と同程度の場合に影響を与える動的効果に対してロバストです。ギャップの小さいQSHIであるゲルマネンでは、動的効果により、光伝導率における面内フォノン共鳴がファノプロファイルを示し、2つの相間で強度と形状に顕著な違いが見られます。一方、ギャップの大きいQSHIであるジャクチンガイトは、いくつかのIR活性フォノンモードを示し、そのスペクトル強度は2つの相間で劇的に変化します。

&lt;img src="https://arxiv.org/html/2505.14277v1/extracted/6458266/grafici/FIGURA1.png"/&gt;&lt;p&gt;Francesco Macheda, Francesco Mauri, Paolo Barone, Paolo Fachin&lt;/p&gt;&lt;p&gt;[&lt;/p&gt;</description><guid isPermaLink="false">2505.14277v1</guid><pubDate>Tue, 20 May 2025 12:29:08 +0000</pubDate></item><item><title>Topological electron and phonon flat bands in novel kagome superconductor XPd5 (X=Ca, Sr, Ba)</title><link>http://arxiv.org/abs/2505.14223v1</link><description>カゴメ格子における幾何学的フラストレーションによって誘起されるフェルミオンおよびボソン局在状態は、強相関系における創発的なエキゾチック量子現象を研究するための独特な研究基盤を提供する。本研究では、新規カゴメ超伝導体XPd5 (X=Ca, Sr, Ba) において、幾何学的フラストレーションによって誘起される電子およびフォノンフラットバンドの共存を発見したことを報告する。電子フラットバンドはフェルミ準位付近に位置し、Z2=1の非自明なトポロジカル不変量を持つ。さらに、従来のvan Hove特異点 (vHS) 、高次vHS、p型、m型vHSを含む、異なる分散と副格子特性を持つカゴメPd d軌道から生じる複数のvan Hove特異点 (vHS) を特定した。特に、フォノンフラットバンドの振動モードの調査により、その形成が逆位相振動モードを持つ隣接するカゴメ格子サイト間の破壊的干渉に起因することが明らかになった。フォノンフラットバンドの物理的メカニズムを調べるために、フォノンのばね-質量モデルを確立した。さらに、XPd5における電子-フォノン結合の計算により、CaPd5、SrPd5、BaPd5に対してそれぞれ4.25 K、2.75 K、3.35 Kの臨界温度 (Tc) を持つ超伝導基底状態が明らかになった。本研究は、フェルミオン-ボソン多体相互作用と超伝導状態を探求するための有望な基盤を提供すると同時に、量子材料におけるフォノンフラットバンドの起源を解明するための新しい分析的枠組みを確立する。

&lt;img src="https://arxiv.org/html/2505.14223v1/extracted/6457488/fig1.png"/&gt;&lt;p&gt;Jian-Min Zhang, Jiefeng Ye, Xianxin Wu, Zhigao Huang&lt;/p&gt;&lt;p&gt;1 Fujian Provincial Key Laboratory of Quantum Manipulation and New Energy Materials, College of Physics and Energy, Fujian Normal University, Fuzhou 350117, China
2 Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing 100190, China&lt;/p&gt;</description><guid isPermaLink="false">2505.14223v1</guid><pubDate>Tue, 20 May 2025 11:32:03 +0000</pubDate></item><item><title>Nanoconfined superionic water is a molecular superionic</title><link>http://arxiv.org/abs/2505.14171v1</link><description>超イオン氷は、水分子が酸素イオンの格子と急速に拡散するプロトンの「ガス」に解離したもので、惑星内部やエネルギー応用において広範な意味を持つエキゾチックな物質状態です。近年、ナノ閉じ込められた水の超イオン状態が予測されており、これはバルク氷とは対照的に、無傷の水分子で構成されています。本研究では、機械学習と電子構造シミュレーションを適用して、ナノ閉じ込められた水がどのように分子性と超イオン性の両方になり得るかを明らかにします。また、この物質が超イオン物質とその挙動全般にどのような洞察を与えるかを探求します。バルク氷や他の超イオン物質と同様に、ナノ閉じ込められた水は、協調的な鎖状のプロトン移動によって伝導し、欠陥の急速な伝播を引き起こします。しかし、他の分子相の水とは異なり、その卓越した伝導性は、（i）プロトン移動への低い障壁と（ii）柔軟な水素結合ネットワークによってグロータス機構が活性化されることに起因します。これらは、分子性超イオンにおける高速イオン伝導の2つの重要な特性であると提案します。ここで得られた洞察は、エネルギー貯蔵などへの応用が期待される、他の分子性超イオン物質の発見のための設計原理を確立します。

&lt;img src="https://arxiv.org/html/2505.14171v1/x1.png"/&gt;&lt;p&gt;Amir Hajibabaei, Angelos Michaelides, Christoph Schran, Samuel W. Coles, Stephen J. Cox, Venkat Kapil, Xavier R. Advincula&lt;/p&gt;&lt;p&gt;Cavendish Laboratory, Department of Physics, University of Cambridge, Cambridge, CB3 0HE, UK
Department of Chemistry, Durham University, South Road, Durham, DH1 3LE, UK
Department of Physics and Astronomy, University College London, 7-19 Gordon St, London WC1H 0AH, UK
Lennard-Jones Centre, University of Cambridge, Trinity Ln, Cambridge, CB2 1TN, UK
Thomas Young Centre and London Centre for Nanotechnology, 9 Gordon St, London WC1H 0AH, UK
Yusuf Hamied Department of Chemistry, University of Cambridge, Lensfield Road, Cambridge, CB2 1EW, UK&lt;/p&gt;</description><guid isPermaLink="false">2505.14171v1</guid><pubDate>Tue, 20 May 2025 10:24:48 +0000</pubDate></item><item><title>Theoretical investigation of interface atomic structure of graphene on NiFe alloy substrate</title><link>http://arxiv.org/abs/2505.14026v1</link><description>トンネル磁気抵抗デバイス用のグラフェン/NiFe合金界面を製造するために、2つのプロセスが提案されている。1つはグラフェンの転写、もう1つはグラフェンへの合金蒸着である。密度汎関数理論計算により、NiFe合金基板の形成エネルギーとNiFe合金基板上へのグラフェンの吸着エネルギーを調査し、2つのプロセス間の界面の原子構造の違いを明らかにする。その結果、Niリッチな表面は裸の基板に適しているが、Fe表面は基板上に吸着したグラフェンに対して安定であることがわかった。この結果は、表面層の組成比が界面の製造プロセスに依存することを示している。

&lt;img src="https://arxiv.org/html/2505.14026v1/x1.png"/&gt;&lt;p&gt;Mitsuharu Uemoto, Naohiro Matsumoto, Ryusuke Endo, Tomoya Ono&lt;/p&gt;&lt;p&gt;Department of Electrical and Electronic Engineering, Graduate School of Engineering, Kobe University, Nada, Kobe 657-8501, Japan&lt;/p&gt;</description><guid isPermaLink="false">2505.14026v1</guid><pubDate>Tue, 20 May 2025 07:27:51 +0000</pubDate></item><item><title>Parallel Exploration of the Optoelectronic Properties of (Sb,Bi)(S,Se)(Br,I) Chalcohalides</title><link>http://arxiv.org/abs/2505.14208v1</link><description>カルコハライドは、その独特な構造化学と振動ダイナミクスの複雑な相互作用によって形作られた、魅力的な材料特性を持つ新興の半導体ファミリーです。次世代の太陽エネルギー変換デバイスへの応用が期待されるにもかかわらず、その固有の光電子特性はほとんど解明されていません。本研究では、同じ準1次元結晶構造を共有する化合物群である(Sb,Bi)(S,Se)(Br,I)系に焦点を当てます。二段階の物理蒸着法（PVD）を用いて、8つの三元カルコハライド化合物を合成し、1.38～2.08 eVのバンドギャップと、シャープな単一成分のフォトルミネッセンス（PL）ピークを示すことを実証しました。キャリアダイナミクスと固有の電子-フォノン相互作用の並行的な研究（パワー依存性、温度依存性、時間分解PL測定を用いて包括的に研究）において、光電子性能への直接的な影響を明らかにします。第一原理密度汎関数理論（DFT）による欠陥計算に裏打ちされ、明確な構造-物性相関を確立し、固溶体エンジニアリングが固有のフォノン構造を微調整し、非放射再結合をさらに抑制するための効果的な手段であることを明らかにします。本研究は、広範な光電子応用において、カルコハライドを高効率材料として最適化するための青写真を提供します。

&lt;img src="https://arxiv.org/html/2505.14208v1/x1.png"/&gt;&lt;p&gt;Alejandro Navarro-Güell, Axel G. Medaille, Cibrán L. Álvarez, Claudio Cazorla, David R. Ferrer, Edgardo Saucedo, Ivan Caño, Mirjana Dimitrievska, Rasmus S. Nielsen, Zacharie J. Li-Kao, Ángel Labordet Álvarez&lt;/p&gt;&lt;p&gt;Department of Physics, University of Basel, 4056 Basel, Switzerland
Group of Characterization of Materials, Departament de Física, Universitat Politècnica de Catalunya (UPC), Campus Diagonal-Besòs, Av Eduard Maristany 10–14, Barcelona 08019, Spain
Nanomaterials Spectroscopy and Imaging, Transport at Nanoscale Interfaces Laboratory, Swiss Federal Laboratories for Material Science and Technology (EMPA), Ueberlandstrasse 129, 8600 Duebendorf, Switzerland
Swiss Nanoscience Institute, University of Basel, 4056 Basel, Switzerland
Universitat Politècnica de Catalunya (UPC), Barcelona Centre for Multiscale Science &amp; Engineering, Av Eduard Maristany 10-14, Barcelona 08019, Spain
Universitat Politècnica de Catalunya (UPC), Photovoltaic Lab - Micro and Nano Technologies Group (MNT), Electronic Engineering Department, EEBE, Av Eduard Maristany 10-14, Barcelona 08019, Spain&lt;/p&gt;</description><guid isPermaLink="false">2505.14208v1</guid><pubDate>Tue, 20 May 2025 11:08:49 +0000</pubDate></item></channel></rss>