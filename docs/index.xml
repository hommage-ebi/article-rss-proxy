<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Fri, 30 May 2025 03:17:47 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-30</title><link>https://arxiv.org/2025-05-30</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23064v1"&gt;Machine Learning Framework for Characterizing Processing-Structure Relationship in Block Copolymer Thin Films&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23533v1"&gt;Sequential tilting 4D-STEM for improved momentum-resolved STEM field mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23574v1"&gt;Si(100)-SiO$_2$ Trap Density Dependence on Sample Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23468v1"&gt;Spin and Charge Control of Topological End States in Chiral Graphene Nanoribbons on a 2D Ferromagnet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23502v1"&gt;Dominant Kitaev interaction and field-induced quantum phase transitions in triangular-lattice KCeSe2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23571v1"&gt;Unconventional Temperature Dependence of Exciton Diamagnetism in 2D Ruddlesden-Popper Lead Halide Perovskites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23069v1"&gt;Burgers rings as topological signatures of Eshelby-like plastic events in glasses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23699v1"&gt;Dyn-HTE: High-temperature expansion of the dynamic Matsubara spin correlator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23371v1"&gt;Dynamic signature of the thermodynamic transition in a novel mean field system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23213v1"&gt;Transparent and heat-insulation bionic hydrogel-based smart window system for long-term cooling and waste heat collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23446v1"&gt;Comparison of water models for structure prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23356v1"&gt;Revolutionising Antibacterial Warfare: Machine Learning and Molecular Dynamics Unveiling Potential Gram-Negative Bacteria Inhibitors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23460v1"&gt;Spin polarized enantio-sensitive multipolar photoelectron currents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23717v1"&gt;Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23153v1"&gt;Conceptual Framework Toward Embodied Collective Adaptive Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23281v1"&gt;MathArena: Evaluating LLMs on Uncontaminated Math Competitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23381v1"&gt;AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23399v1"&gt;GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23473v1"&gt;EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23486v1"&gt;Autoformalization in the Era of Large Language Models: A Survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23518v1"&gt;TRAP: Targeted Redirecting of Agentic Preferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23519v1"&gt;Individual differences in the cognitive mechanisms of planning strategy discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23536v1"&gt;Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23596v1"&gt;MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23667v1"&gt;Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23695v1"&gt;Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23703v1"&gt;Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23003v1"&gt;Hybrid Cross-domain Robust Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23006v1"&gt;A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23017v1"&gt;$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23026v1"&gt;Context Robust Knowledge Editing for Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23032v1"&gt;Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23034v1"&gt;Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23042v1"&gt;From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23043v1"&gt;Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23045v1"&gt;Multi-Sourced Compositional Generalization in Visual Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23053v1"&gt;Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23059v1"&gt;From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23062v1"&gt;Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23075v1"&gt;Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23078v1"&gt;Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23091v1"&gt;Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23107v1"&gt;EAD: An EEG Adapter for Automated Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23116v1"&gt;CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23117v1"&gt;Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23118v1"&gt;Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23121v1"&gt;ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23132v1"&gt;Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23134v1"&gt;Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23181v1"&gt;FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23191v1"&gt;ExpeTrans: LLMs Are Experiential Transfer Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23195v1"&gt;Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23214v1"&gt;SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23267v1"&gt;VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23276v1"&gt;The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23277v1"&gt;Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23292v1"&gt;Federated Unsupervised Semantic Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23309v1"&gt;Score-based Generative Modeling for Conditional Independence Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23337v1"&gt;Matryoshka Model Learning for Improved Elastic Student Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23349v1"&gt;Towards Reward Fairness in RLHF: From a Resource Allocation Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23352v1"&gt;Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23367v1"&gt;PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23369v1"&gt;Dynamic Spectral Backpropagation for Efficient Neural Network Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23386v1"&gt;VModA: An Effective Framework for Adaptive NSFW Image Moderation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23397v1"&gt;A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23415v1"&gt;Bidirectional predictive coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23417v1"&gt;Toward Effective AI Governance: A Review of Principles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23422v1"&gt;From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23426v1"&gt;Enhanced DACER Algorithm with High Diffusion Efficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23436v1"&gt;Emergent Risk Awareness in Rational Agents under Resource Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23444v1"&gt;CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23454v1"&gt;LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23474v1"&gt;Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23508v1"&gt;A Robot-Assisted Approach to Small Talk Training for Adults with ASD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23529v1"&gt;Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23538v1"&gt;CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23575v1"&gt;CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23580v1"&gt;Engineering Serendipity through Recommendations of Items with Atypical Aspects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23595v1"&gt;DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23624v1"&gt;Towards Explainable Sequential Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23628v1"&gt;AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23637v1"&gt;Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23643v1"&gt;Securing AI Agents with Information-Flow Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23655v1"&gt;Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23686v1"&gt;ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23704v1"&gt;CLDTracker: A Comprehensive Language Description for Visual Tracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23714v1"&gt;SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23724v1"&gt;SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23729v1"&gt;Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23735v1"&gt;ATLAS: Learning to Optimally Memorize the Context at Test Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23742v1"&gt;MAGREF: Masked Guidance for Any-Reference Video Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23744v1"&gt;Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23746v1"&gt;Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23754v1"&gt;DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22993v1"&gt;Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23008v1"&gt;Synthetic Document Question Answering in Hungarian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23020v1"&gt;AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23027v1"&gt;Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23058v1"&gt;Be.FM: Open Foundation Models for Human Behavior&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23066v1"&gt;Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23085v1"&gt;GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23145v1"&gt;FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23161v1"&gt;Implicit Inversion turns CLIP into a Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23165v1"&gt;Best Arm Identification with Possibly Biased Offline Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23183v1"&gt;Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23192v1"&gt;Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23229v1"&gt;MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23247v1"&gt;Accelerating RLHF Training with Reward Variance Increase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23266v1"&gt;Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23268v1"&gt;Unsupervised Transcript-assisted Video Summarization and Highlight Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23270v1"&gt;Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23295v1"&gt;How Does Response Length Affect Long-Form Factuality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23308v1"&gt;Spoken question answering for visual queries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23313v1"&gt;Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23315v1"&gt;Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23331v1"&gt;Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23353v1"&gt;Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23406v1"&gt;Video Editing for Audio-Visual Dubbing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23412v1"&gt;Buffer-free Class-Incremental Learning with Out-of-Distribution Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23432v1"&gt;A Mathematical Framework for AI-Human Integration in Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23437v1"&gt;Bounded-Abstention Pairwise Learning to Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23503v1"&gt;Can Large Language Models Challenge CNNS in Medical Image Analysis?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23554v1"&gt;Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23564v1"&gt;Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23576v1"&gt;Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23584v1"&gt;Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23590v1"&gt;Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23617v1"&gt;One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23631v1"&gt;Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23657v1"&gt;Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23693v1"&gt;VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23709v1"&gt;Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23720v1"&gt;COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23733v1"&gt;Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23745v1"&gt;To Trust Or Not To Trust Your Vision-Language Model's Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23747v1"&gt;Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23751v1"&gt;REOrdering Patches Improves Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23759v1"&gt;Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23761v1"&gt;Differential Information: An Information-Theoretic Perspective on Preference Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23762v1"&gt;ZeroGUI: Automating Online GUI Learning at Zero Human Cost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23765v1"&gt;From Chat Logs to Collective Insights: Aggregative Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23710v1"&gt;From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23706v1"&gt;Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22998v1"&gt;LLM Agents for Bargaining with Utility-based Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23004v1"&gt;QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23013v1"&gt;Scalable Complexity Control Facilitates Reasoning Ability of LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23014v1"&gt;Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23022v1"&gt;SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23024v1"&gt;An Empirical Study of Federated Prompt Learning for Vision Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23048v1"&gt;ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23055v1"&gt;CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23063v1"&gt;Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23071v1"&gt;Multi-Modal Learning with Bayesian-Oriented Gradient Calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23084v1"&gt;Gradient Boosting Decision Tree with LSTM for Investment Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23098v1"&gt;Learning to Search for Vehicle Routing with Multiple Time Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23099v1"&gt;Weight Spectra Induced Efficient Model Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23150v1"&gt;Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23182v1"&gt;FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23185v1"&gt;Improving the Effective Receptive Field of Message-Passing Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23190v1"&gt;DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23194v1"&gt;Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23223v1"&gt;Daunce: Data Attribution through Uncertainty Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23225v1"&gt;Generalizability vs. Counterfactual Explainability Trade-Off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23228v1"&gt;Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23244v1"&gt;Equivalence of stochastic and deterministic policy gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23246v1"&gt;Measuring Participant Contributions in Decentralized Federated Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23264v1"&gt;Efficiently Access Diffusion Fisher: Within the Outer Product Span Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23285v1"&gt;Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23334v1"&gt;X2Graph for Cancer Subtyping Prediction on Biological Tabular Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23345v1"&gt;Graph Positional Autoencoders as Self-supervised Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23347v1"&gt;Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23378v1"&gt;Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23383v1"&gt;Automated Modeling Method for Pathloss Model Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23421v1"&gt;OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23427v1"&gt;On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23433v1"&gt;Diversity-Aware Policy Optimization for Large Language Model Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23443v1"&gt;Strategic Classification with Non-Linear Classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23458v1"&gt;Diffusion Guidance Is a Controllable Policy Improvement Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23459v1"&gt;On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23489v1"&gt;SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23506v1"&gt;Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23520v1"&gt;AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23527v1"&gt;Normalizing Flows are Capable Models for RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23528v1"&gt;Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23552v1"&gt;Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23555v1"&gt;Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23569v1"&gt;Maximum Likelihood Learning of Latent Dynamics Without Reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23579v1"&gt;BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23583v1"&gt;Improving Time Series Forecasting via Instance-aware Post-hoc Revision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23593v1"&gt;Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23607v1"&gt;Data Model Design for Explainable Machine Learning-based Electricity Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23615v1"&gt;Learning Interpretable Differentiable Logic Networks for Tabular Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23627v1"&gt;Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23648v1"&gt;Continuous Chain of Thought Enables Parallel Exploration and Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23653v1"&gt;How does Transformer Learn Implicit Reasoning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23673v1"&gt;Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23681v1"&gt;Understanding Mode Connectivity via Parameter Space Symmetry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23683v1"&gt;Learning Compositional Functions with Transformers from Easy-to-Hard Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23700v1"&gt;DiCoFlex: Model-agnostic diverse counterfactuals with flexible control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23719v1"&gt;TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23725v1"&gt;MuLoCo: Muon is a practical inner optimizer for DiLoCo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23732v1"&gt;EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23760v1"&gt;Model Immunization from a Condition Number Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22994v1"&gt;Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.22997v1"&gt;Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23009v1"&gt;EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23049v1"&gt;DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23094v1"&gt;MAP: Revisiting Weight Decomposition for Low-Rank Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23105v1"&gt;LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23124v1"&gt;Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23131v1"&gt;DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23160v1"&gt;Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23173v1"&gt;Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23176v1"&gt;The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23184v1"&gt;Two Is Better Than One: Rotations Scale LoRAs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23196v1"&gt;JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23207v1"&gt;Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23260v1"&gt;Stable Thompson Sampling: Valid Inference via Variance Inflation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23271v1"&gt;LADA: Scalable Label-Specific CLIP Adapter for Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23305v1"&gt;MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23320v1"&gt;Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23355v1"&gt;Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23416v1"&gt;KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23442v1"&gt;Rethinking Regularization Methods for Knowledge Graph Completion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23448v1"&gt;Network Inversion for Uncertainty-Aware Out-of-Distribution Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23475v1"&gt;TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23495v1"&gt;Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23496v1"&gt;Epistemic Errors of Imperfect Multitask Learners When Distributions Shift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23522v1"&gt;OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23523v1"&gt;Accelerating AllReduce with a Persistent Straggler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23537v1"&gt;Domain-Aware Tensor Network Structure Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23557v1"&gt;Learning Parametric Distributions from Samples and Preferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23558v1"&gt;Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23585v1"&gt;On-Policy RL with Optimal Reward Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23587v1"&gt;PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23588v1"&gt;Accelerated Training of Federated Learning via Second-Order Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23606v1"&gt;Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23614v1"&gt;Inference-time Scaling of Diffusion Models through Classical Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23619v1"&gt;Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23620v1"&gt;Instance-Optimality for Private KL Distribution Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23640v1"&gt;Global optimization of graph acquisition functions for neural architecture search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23651v1"&gt;Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23658v1"&gt;Bayesian Perspective on Memorization and Reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23663v1"&gt;AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23666v1"&gt;LoLA: Low-Rank Linear Attention With Sparse Caching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23696v1"&gt;Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23705v1"&gt;Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23056v1"&gt;Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23081v1"&gt;Gradient Methods with Online Scaling Part I. Theoretical Foundations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23215v1"&gt;Trajectory Generator Matching for Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23240v1"&gt;Joint estimation of smooth graph signals from partial linear measurements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23431v1"&gt;Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23445v1"&gt;The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23470v1"&gt;Refining Labeling Functions with Limited Labeled Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23509v1"&gt;Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23515v1"&gt;DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23565v1"&gt;DRO: A Python Library for Distributionally Robust Optimization in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23594v1"&gt;Multilook Coherent Imaging: Theoretical Guarantees and Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23609v1"&gt;The Generalized Skew Spectrum of Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23652v1"&gt;Optimization-Free Diffusion Model -- A Perturbation Theory Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23692v1"&gt;Mobi-$π$: Mobilizing Your Robot Learning Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23702v1"&gt;(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23389v1"&gt;Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23599v1"&gt;On Transferring Transferability: Towards a Theory for Size Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23737v1"&gt;On the Convergence Analysis of Muon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23001v1"&gt;DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23015v1"&gt;Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23029v1"&gt;Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23030v1"&gt;Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23035v1"&gt;Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23037v1"&gt;Improving Multilingual Social Media Insights: Aspect-based Comment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23038v1"&gt;EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23052v1"&gt;Query Routing for Retrieval-Augmented Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23065v1"&gt;SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23108v1"&gt;Generating Diverse Training Samples for Relation Extraction with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23114v1"&gt;Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23126v1"&gt;PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23140v1"&gt;Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23146v1"&gt;Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23166v1"&gt;Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23174v1"&gt;Map&amp;Make: Schema Guided Text to Table Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23224v1"&gt;MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23242v1"&gt;ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23291v1"&gt;ScEdit: Script-based Assessment of Knowledge Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23297v1"&gt;EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23299v1"&gt;Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23304v1"&gt;Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23316v1"&gt;Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23323v1"&gt;Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23363v1"&gt;Discriminative Policy Optimization for Token-Level Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23368v1"&gt;Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23404v1"&gt;Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23410v1"&gt;From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23420v1"&gt;The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23461v1"&gt;UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23477v1"&gt;Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23480v1"&gt;Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23540v1"&gt;Probability-Consistent Preference Optimization for Enhanced LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23548v1"&gt;Translation in the Wild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23556v1"&gt;Understanding Refusal in Language Models with Sparse Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23621v1"&gt;Table-R1: Inference-Time Scaling for Table Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23623v1"&gt;Characterizing the Expressivity of Transformer Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23630v1"&gt;GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23654v1"&gt;ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23688v1"&gt;Automatic classification of stop realisation with wav2vec2.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23689v1"&gt;Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23701v1"&gt;Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23713v1"&gt;SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23715v1"&gt;Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23722v1"&gt;Label-Guided In-Context Learning for Named Entity Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23039v1"&gt;TailorSQL: An NL2SQL System Tailored to Your Query Workload&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23170v1"&gt;ZIPA: A family of efficient models for multilingual phone recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23339v1"&gt;Nosey: Open-source hardware for acoustic nasalance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23493v1"&gt;R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23494v1"&gt;Spoken Language Modeling with Duration-Penalized Self-Supervised Units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23764v1"&gt;MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.23570v1"&gt;Evaluating AI capabilities in detecting conspiracy theories on YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-30</guid><pubDate>Fri, 30 May 2025 12:14:49 +0900</pubDate></item><item><title>Identity resolution of software metadata using Large Language Models</title><link>http://arxiv.org/abs/2505.23500v1</link><description>ソフトウェアは研究に不可欠な要素です。しかし、研究データと比較して、ソフトウェアにはほとんど注意が払われていませんでした。近年、研究活動におけるソフトウェアの重要性を認識し、強調する取り組みが増加しています。

bio.tools、Bioconductor、Galaxy ToolShedなどのプラットフォームからの構造化されたメタデータは、ライフサイエンスにおける研究ソフトウェアに関する貴重な洞察を提供します。元々は発見と統合をサポートすることを目的としていましたが、このメタデータはソフトウェアの実践に関する大規模な分析に転用できます。ただし、その品質と完全性はプラットフォームによって異なり、多様なドキュメント作成慣行を反映しています。

ソフトウェアの開発と持続可能性に関する包括的な見解を得るには、このメタデータを統合する必要がありますが、その異質性と規模に対処するための堅牢なメカニズムが必要です。

この記事では、ソフトウェアメタデータの同一性解決というタスクに対する、命令調整された大規模言語モデルの評価を紹介します。これは、研究ソフトウェアの一貫性のあるコレクションを組み立てるための重要なステップです。このようなコレクションは、ライフサイエンスにおける研究ソフトウェアのFAIR性を監視するためにメタデータを集約するプラットフォームであるOpenEBenchのSoftware Observatoryのリファレンスコンポーネントとなります。

複数のモデルを人間が注釈を付けたゴールドスタンダードに対してベンチマークし、曖昧なケースでの動作を調べ、高信頼性の自動決定のための合意に基づくプロキシを導入しました。このプロキシは、高い精度と統計的ロバスト性を実現すると同時に、現在のモデルの限界と、レジストリおよびリポジトリ全体でFAIRに準拠したソフトウェアメタデータにおけるセマンティックな判断を自動化する上でのより広範な課題を浮き彫りにしました。

&lt;img src="https://arxiv.org/html/2505.23500v1/extracted/6493419/images/gold_standard_distributions.png"/&gt;&lt;p&gt;Eva Martín del Pico 0000-0001-8324-2897, Josep Lluís Gelpí 0000-0002-0566-7723, Salvador Capella-Gutiérrez 0000-0002-0309-604X&lt;/p&gt;&lt;p&gt;Barcelona Supercomputing Center, Barcelona, Spain
Department of Biochemistry and Molecular Biology, University of Barcelona, Barcelona, Spain&lt;/p&gt;</description><guid isPermaLink="false">2505.23500v1</guid><pubDate>Thu, 29 May 2025 14:47:31 +0000</pubDate></item><item><title>ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions</title><link>http://arxiv.org/abs/2505.23662v1</link><description>大規模言語モデル（LLM）は、ユーザーの問い合わせに対応するために外部ツールを使用する強力な能力を示しています。しかし、既存の評価のほとんどは、短いコンテキストでのツール使用を前提としており、現実的な長期的なインタラクションにおけるモデルの挙動に関する洞察は限られています。このギャップを埋めるために、長期的なインタラクションにおけるツール使用能力をテストするためのベンチマークであるToolHaystackを導入します。ToolHaystackの各テストインスタンスには、複数のタスク実行コンテキストと、継続的な会話内の現実的なノイズが含まれており、モデルがコンテキストを維持し、さまざまな中断に対処する能力を評価できます。このベンチマークを14の最先端LLMに適用した結果、現在のモデルは標準的な複数ターンの設定では良好なパフォーマンスを示すものの、ToolHaystackではしばしば著しく苦戦し、これまでのツールベンチマークでは明らかにならなかった長期的なロバスト性における重大なギャップが浮き彫りになりました。

&lt;img src="https://arxiv.org/html/2505.23662v1/x1.png"/&gt;&lt;p&gt;Yonsei University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23662v1</guid><pubDate>Thu, 29 May 2025 17:10:12 +0000</pubDate></item><item><title>Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers</title><link>http://arxiv.org/abs/2505.23252v1</link><description>アプローチは、科学的研究を行うための基礎を形成します。膨大な科学論文からアプローチを検索することは非常に時間がかかり、適切に組織化された管理フレームワークがなければ、研究者は関連するアプローチの検索と利用において重大な課題に直面する可能性があります。アプローチに複数の次元を構築し、これらの次元から管理することで、効率的な解決策を提供できます。まず、本論文ではトップダウン方式でアプローチパターンを特定し、意味レベル、談話レベル、構文レベル、語彙レベルの4つの異なる言語レベルを通じてパターンを洗練します。科学論文中のアプローチは、アプローチパターンに基づいて抽出されます。さらに、これらのパターンを用いて、アプローチを分類するための5つの次元を特定します。本論文では、ステップを表現するために木構造を使用し、構文レベルの類似性に焦点を当てた木構造ベースの類似性尺度を用いて、異なるステップ間の類似性を測定することを提案します。アプローチ間の類似性を計算するために、コレクション類似性尺度が提案されます。各次元内のアプローチコンポーネントのクラスタツリーを構築するために、ボトムアップクラスタリングアルゴリズムが提案されます。これは、各アプローチコンポーネントまたはクラスを、各反復で最も類似したアプローチコンポーネントまたはクラスとマージすることによって行われます。クラスタリングプロセス中に生成されるクラスラベルは、各クラスのアプローチコンポーネント内のステップコンポーネントの共通のセマンティクスを示し、クラス内のアプローチを管理するために使用されます。5つの次元のクラスタツリーは、集合的に多次元アプローチ空間を形成します。多次元アプローチ空間でのアプローチクエリの適用は、この空間内でのクエリが、ユーザーのクエリと結果の間の強い関連性を保証し、クラスベースのクエリメカニズムを通じて検索空間を迅速に削減することを示しています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23252v1</guid><pubDate>Thu, 29 May 2025 08:57:11 +0000</pubDate></item><item><title>Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification</title><link>http://arxiv.org/abs/2505.23177v1</link><description>従来のコード指示データ合成手法は、多様性の限界と貧弱なロジックに悩まされています。大規模言語モデル（LLM）のコード生成能力を向上させるために設計された、高品質な質問応答ペアを合成するための自動化フレームワークであるInfinite-Instructを紹介します。このフレームワークは、合成された問題の内部ロジックと合成されたコードの品質を向上させることに重点を置いています。まず、「逆構築」は、コードスニペットを多様なプログラミング問題に変換します。次に、「バックフィーディング構築」を通じて、プログラミング問題のキーワードを知識グラフに構造化し、より強力な内部ロジックを持つプログラミング問題に再構築します。最後に、クロスリンガル静的コード分析パイプラインは、無効なサンプルをフィルタリングして、データの品質を保証します。実験の結果、主流のコード生成ベンチマークにおいて、ファインチューニングされたモデルは、7Bパラメータモデルで平均21.70％、32Bパラメータモデルで36.95％の性能向上を達成しました。指示ファインチューニングデータの10分の1未満を使用して、Qwen-2.5-Coder-Instructに匹敵するパフォーマンスを達成しました。Infinite-Instructは、プログラミングにおけるLLMトレーニングのためのスケーラブルなソリューションを提供します。実験で使用したデータセット（静的分析によるフィルタリングされていないバージョンとフィルタリングされたバージョンの両方を含む）をオープンソースで公開します。データはhttps://github.com/xingwenjing417/Infinite-Instruct-datasetで入手できます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23177v1</guid><pubDate>Thu, 29 May 2025 07:14:43 +0000</pubDate></item><item><title>Self-Correcting Code Generation Using Small Language Models</title><link>http://arxiv.org/abs/2505.23060v1</link><description>自己修正は、言語モデルが連続的な改善を通じて出力を修正し、改善することを可能にすることで、コード生成において潜在能力を示してきました。最近の研究では、プロプライエタリモデルを使用した検証またはフィードバックループを組み込んだプロンプトベースの戦略や、その強力な推論能力を活用したトレーニングベースの手法が探求されています。しかし、より小さなモデルが自己反省を通じて効果的に出力を導く能力を持っているかどうかは、まだ探求されていません。私たちの調査結果は、より小さなモデルが両方の自己修正パラダイムにおいて、反省的な修正行動を示すのに苦労していることを明らかにしています。これに対し、私たちは、小規模言語モデルの複数ターンのコード修正能力を向上させるように設計されたアプローチであるCoCoSを導入します。具体的には、モデルが正しい出力を自信を持って維持しながら、ターンが進むにつれて誤った出力を段階的に修正するようにトレーニングする、オンライン強化学習の目的を提案します。私たちのアプローチは、軌跡全体にわたって報酬を集計する累積報酬関数と、複数ターンの修正シナリオにより適したきめ細かい報酬を特徴としています。これにより、モデルは初期応答の品質を向上させると同時に、自己修正を通じて大幅な改善を達成することができます。10億規模のモデルを使用した場合、CoCoSはベースラインと比較して、MBPPで35.8％、HumanEvalで27.7％の改善を達成しています。

&lt;img src="https://arxiv.org/html/2505.23060v1/x1.png"/&gt;&lt;p&gt;Deokhyung Kang, Engineering Pohang University of Science, Gary Geunbae Lee Graduate School of Artificial Intelligence Department of Computer Science, Hyounghun Kim, Jeonghun Cho, Technology (POSTECH)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23060v1</guid><pubDate>Thu, 29 May 2025 04:04:44 +0000</pubDate></item><item><title>Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery</title><link>http://arxiv.org/abs/2505.23106v1</link><description>注意機構は、自然言語処理やコンピュータビジョンといった主要なAI分野において、変革をもたらすツールとして登場しました。しかし、複雑な物理システムをモデル化するための、そのほとんど活用されていない潜在能力は、魅力的なフロンティアを提示しています。このようなシステムを学習するには、関数ペアの限られたインスタンスを使用して、関数空間間のマッピングを行う演算子を発見することがしばしば必要となります。これは、一般的に、著しく不良設定な逆偏微分方程式（PDE）問題として捉えられます。本研究では、予測精度と計算効率の両面でNonlocal Attention Operators（NAO）を基盤とし、それを強化する新しいニューラルオペレータアーキテクチャであるNeural Interpretable PDEs（NIPS）を紹介します。NIPSは、スケーラブルな学習を可能にする線形注意機構を採用し、フーリエ空間でチャネル独立な畳み込みとして機能する学習可能なカーネルネットワークを統合します。その結果、NIPSは、大規模なペアワイズインタラクションを明示的に計算して保存する必要がなくなり、空間インタラクションの処理コストをフーリエ変換に効果的に分散させます。実証的な評価により、NIPSは、多様なベンチマークにおいて、NAOや他のベースラインを一貫して上回り、スケーラブルで解釈可能、かつ効率的な物理学習における大きな飛躍を告げています。本論文に付随するコードとデータは、https://github.com/fishmoon1234/Nonlocal-Attention-Operator で入手できます。

&lt;img src="https://arxiv.org/html/2505.23106v1/x1.png"/&gt;&lt;p&gt;Ning Liu, Yue Yu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23106v1</guid><pubDate>Thu, 29 May 2025 05:18:30 +0000</pubDate></item><item><title>DINGO: Constrained Inference for Diffusion LLMs</title><link>http://arxiv.org/abs/2505.23061v1</link><description>拡散LLMは、従来の自己回帰LLMに代わる有望な選択肢として登場しており、実行時効率の向上に大きな可能性を秘めています。しかし、既存の拡散モデルは、正規表現などのユーザー指定の形式的な制約を確実に適用する能力に欠けており、固定スキーマJSON生成など、構造化された出力を必要とするタスクには信頼性がありません。トークンを逐次的に生成する自己回帰モデルとは異なり、拡散LLMはトークンのブロックを並列に予測します。この並列性により、逐次的なトークン予測のために設計された従来の制約付きデコードアルゴリズムは、真の出力分布を維持する上で効果がありません。この制限に対処するために、我々はDINGOという、動的計画法に基づいた制約付きデコード戦略を提案します。DINGOは、効率的であると同時に、確率分布を確実に保持します。DINGOは、モデルが予測した分布の下で最も高い確率を持つ出力文字列をサンプリングすることを可能にし、ユーザーが指定した正規表現を厳密に満たします。標準的な記号数学およびJSON生成ベンチマークにおいて、DINGOは制約なし推論と比較して最大68パーセントポイントの改善を達成しました。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23061v1</guid><pubDate>Thu, 29 May 2025 04:04:54 +0000</pubDate></item><item><title>Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?</title><link>http://arxiv.org/abs/2505.23749v1</link><description>事前学習後、大規模言語モデルは、ペアワイズ比較に基づいて人間の好みに合わせられます。最先端のアライメント手法（PPOベースのRLHFやDPOなど）は、ユーザーが多様な好みを持つ環境に展開されているにもかかわらず、単一の好みモデルに合わせるという前提に基づいて構築されています。その結果、これらのアライメント手法が、多元的なアライメントの最小限の要件である、平均的にユーザーを満足させるモデルを生成するかどうかさえ明らかではありません。社会選択理論に基づき、ユーザーの比較を個々のブラッドリー・テリー（BT）モデルを通じてモデル化することで、アライメント手法の歪みを導入します。これは、達成可能な最適な平均効用と、学習されたポリシーの平均効用との間の最悪ケースの比率です。

歪みの概念は、アライメント手法間の明確な区別を可能にします。人間のフィードバックからのナッシュ学習は、（BT温度$\beta$に対して）$(\frac{1}{2} + o(1)) \cdot \beta$のミニマックス最適な歪みを達成し、効用分布、比較ペアの分布、および参照ポリシーからの許容可能なKLダイバージェンスにわたってロバストです。対照的に、RLHFとDPOは、KL制約なしでも$\geq (1 - o(1)) \cdot \beta$の歪みを被り、比較ペアのサンプリング方法によっては、完全な設定では$e^{\Omega(\beta)}$または無制限の歪みを被ります。

&lt;img src="https://arxiv.org/html/2505.23749v1/x1.png"/&gt;&lt;p&gt;Kunhe Yang UC Berkeley, Nika Haghtalab UC Berkeley, Paul Gölz Cornell University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23749v1</guid><pubDate>Thu, 29 May 2025 17:59:20 +0000</pubDate></item><item><title>Are Reasoning Models More Prone to Hallucination?</title><link>http://arxiv.org/abs/2505.23646v1</link><description>最近進化を遂げた大規模推論モデル（LRM）は、長い思考連鎖（CoT）推論能力により、複雑なタスクを解決する上で強力な性能を示しています。これらのLRMは主に形式的な推論タスクに関するポストトレーニングによって開発されているため、その推論能力が事実探索タスクにおけるハルシネーション（幻覚）を減らすのに役立つように一般化されるかどうかは不明確であり、議論の余地があります。例えば、DeepSeek-R1は事実探索ベンチマークであるSimpleQAでパフォーマンスが向上したと報告していますが、OpenAI-o3はさらに深刻なハルシネーションを観察しています。この矛盾は当然、次の研究課題を提起します。推論モデルはハルシネーションを起こしやすいのか？本論文では、この疑問に3つの視点から取り組みます。（1）まず、LRMにおけるハルシネーションについて包括的な評価を行います。私たちの分析により、コールドスタートの教師ありファインチューニング（SFT）と検証可能な報酬RLを用いた完全なポストトレーニングパイプラインを経たLRMは、一般的にハルシネーションを軽減することが明らかになりました。対照的に、蒸留のみ、およびコールドスタートのファインチューニングなしのRLトレーニングは、より微妙なハルシネーションを引き起こします。（2）異なるポストトレーニングパイプラインがLRMにおけるハルシネーションへの影響を変化させる理由を探るために、行動分析を行います。LRMの事実性に直接影響を与える2つの重要な認知行動を特徴づけます。それは、表面的な推論が同じ根本的な欠陥のあるロジックを繰り返し追う「欠陥の繰り返し」と、最終的な答えが以前のCoTプロセスに忠実に一致しない「思考と答えの不一致」です。（3）さらに、モデルの不確実性の観点から、LRMのハルシネーションのメカニズムを調査します。LRMのハルシネーションの増加は、通常、モデルの不確実性と事実の正確さの間のずれに関連していることがわかりました。私たちの研究は、LRMにおけるハルシネーションの初期的な理解を提供します。

&lt;img src="https://arxiv.org/html/2505.23646v1/x1.png"/&gt;&lt;p&gt;National University of Singapore, Technology, Tsinghua University School of Computing, Zijun Yao Yantao Liu Yanxu Chen Jianhui Chen Department of Computer Science&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23646v1</guid><pubDate>Thu, 29 May 2025 16:53:41 +0000</pubDate></item><item><title>MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment</title><link>http://arxiv.org/abs/2505.23634v1</link><description>モデルコンテキストプロトコル（MCP）は、生成AIエージェントのシームレスな統合を可能にするオープンスタンダードとして広く採用されています。しかし、最近の研究では、MCPが検索ベースの「偽装された良性」攻撃（FBA）を受けやすく、悪意のあるシステムアクセスや認証情報の盗難を許容するものの、ユーザーが侵害されたファイルを直接システムにダウンロードする必要があることが示されています。本稿では、MCPベースの攻撃の脅威モデルが、これまで考えられていたよりも大幅に広いことを示します。つまり、攻撃者は悪意のあるコンテンツをオンラインに投稿するだけで、MCPエージェントを欺き、警戒心の薄い被害者のシステムに対する攻撃を実行させることができます。

このような攻撃に対するアライメントガードレールを改善するために、FBAと（真に）良性のサンプルからなる新しいMCPデータセットを導入し、大規模言語モデル（LLM）の拒否学習における直接選好最適化（DPO）の有効性を検証します。DPOはこのような攻撃に対するモデルのガードレールを改善しますが、拒否学習の有効性は、モデルの元のポストトレーニングアライメントスキームによって大きく異なることを示します。例えば、GRPOベースのLLMは拒否を学習するのが非常に苦手です。そこで、FBAの拒否をさらに改善するために、RAG（Retrieval Augmented Generation）に基づく新しい選好アライメント戦略であるRetrieval Augmented Generation for Preference alignment（RAG-Pref）を導入します。RAG-Prefは、特にDPOアライメントと組み合わせることで、LLMがFBAを拒否する能力を大幅に向上させ、MCPベースの攻撃に対するガードレールを劇的に改善することを示します。

&lt;img src="https://arxiv.org/html/2505.23634v1/x1.png"/&gt;&lt;p&gt;John T.  Halloran Leidos&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23634v1</guid><pubDate>Thu, 29 May 2025 16:44:29 +0000</pubDate></item><item><title>LLM Performance for Code Generation on Noisy Tasks</title><link>http://arxiv.org/abs/2505.23598v1</link><description>本論文では、大規模言語モデル（LLM）が、認識できないほど難読化されたタスクを認識し、解決する能力について調査します。競技プログラミングとベンチマークタスク（LeetCodeとMATH）に焦点を当て、複数のモデルと、ノイズやリダクションなどの難読化手法における性能を比較します。評価したすべてのLLMが、テキストが人間の読者には理解できず、指示やコンテキストの重要な部分が含まれていないレベルまで難読化されたタスクを解決できることを示します。この挙動を説明するために、モデルの知識カットオフ日以降に公開されたタスクでは観察されない「積極的なパターンマッチング」という概念を導入します。これは、提示された問題に対する正当な推論というよりも、トレーニングデータに対する強力な記憶または過剰適合を示唆しています。汚染されたデータセットと未見のデータセットの間で、異なる性能低下パターンに関する経験的証拠を報告します。標準的なデータセットを使用した実験を設計する際には注意が必要であると主張し、モデルの挙動のベンチマークと評価に対する影響について議論します。また、データセットの汚染を検出し、自動化されたソフトウェアシステムの潜在的な安全上のリスクと解釈可能性の問題を強調するための可能な戦略として、難読化下での性能低下の測定を提案します。

&lt;img src="https://arxiv.org/html/2505.23598v1/extracted/6481777/images/math/math.json_multi_model.png"/&gt;&lt;p&gt;Andrei Paleyes, Christian Cabrera, Diana Robinson, Neil Lawrence University of Cambridge, Radzim Sendyka&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23598v1</guid><pubDate>Thu, 29 May 2025 16:11:18 +0000</pubDate></item><item><title>DiffER: Categorical Diffusion for Chemical Retrosynthesis</title><link>http://arxiv.org/abs/2505.23721v1</link><description>自動化学逆合成のための手法は、主にトランスフォーマーニューラルネットワークを通じて、自然言語処理のために伝統的に構築されたモデルの応用によって、近年成功を収めています。これらのモデルは、化学製品と反応物のSMILESエンコーディング間の翻訳において著しい能力を示していますが、自己回帰的な性質の結果として制約を受けています。我々は、カテゴリカル拡散の形で、逆合成予測のための代替のテンプレートフリー手法であるDiffERを提案します。これにより、出力SMILESシーケンス全体を同時に予測できます。我々は、トップ1精度で最先端の性能を達成し、テンプレートフリー手法の中でトップ3、トップ5、およびトップ10精度で競争力のある性能を達成する拡散モデルのアンサンブルを構築します。DiffERが、実験室環境で使用されるさまざまな合成技術を学習し、トップk精度メトリックで他のさまざまなテンプレートフリー手法を上回ることができる、新しいクラスのテンプレートフリーモデルの強力なベースラインであることを証明します。分散を持つ新しい長さ予測コンポーネントを備えたカテゴリカル拡散モデルのアンサンブルを構築することにより、我々の手法は、反応物の事後分布から近似的にサンプリングし、信頼性と尤度の強力なメトリックを備えた結果を生成することができます。さらに、我々の分析は、SMILESシーケンス長の正確な予測が、カテゴリカル拡散モデルのパフォーマンスをさらに向上させるための鍵であることを示しています。

&lt;img src="https://arxiv.org/html/2505.23721v1/extracted/6494269/images/diffusion_schema.png"/&gt;&lt;p&gt;Daniel Adu-Ampratwum, Srinivasan Parthasarathy, Ziqi Chen&lt;/p&gt;&lt;p&gt;[&lt;/p&gt;</description><guid isPermaLink="false">2505.23721v1</guid><pubDate>Thu, 29 May 2025 17:53:37 +0000</pubDate></item><item><title>VERINA: Benchmarking Verifiable Code Generation</title><link>http://arxiv.org/abs/2505.23135v1</link><description>大規模言語モデル（LLM）はソフトウェア開発への統合が進んでいますが、LLMが生成したコードの正確性を保証することは依然として難しく、多くの場合、コストのかかる手動レビューが必要です。検証可能なコード生成（コード、仕様、およびコードと仕様のアラインメントの証明を共同で生成すること）は、この制限に対処し、コーディングにおけるLLMの利点をさらに引き出すための有望な道筋を提供します。しかし、評価には大きなギャップが存在します。現在のベンチマークは、エンドツーエンドの検証可能なコード生成のサポートを欠いていることが多いのです。本論文では、コード、仕様、および証明の生成、ならびにそれらの組み合わせの包括的かつモジュール式の評価を可能にする高品質なベンチマークであるVerina（Verifiable Code Generation Arena）を紹介します。Verinaは、詳細な問題記述、参照実装、形式仕様、および広範なテストスイートを備えた、Leanにおける189個の手動でキュレーションされたコーディングタスクで構成されています。最先端のLLMに対する広範な評価により、検証可能なコード生成、特に証明生成において重大な課題があることが明らかになり、検証ドメインにおけるLLMベースの定理証明器の改善の必要性が強調されています。最高のモデルであるOpenAI o4-miniでさえ、タスクごとに1回の試行で、正しいコードを61.4%、健全かつ完全な仕様を51.0%、成功した証明を3.6%しか生成できません。Verinaが、厳密かつ包括的なベンチマークを提供することにより、検証可能なコード生成の進歩を促進することを願っています。データセットはhttps://huggingface.co/datasets/sunblaze-ucb/verina、評価コードはhttps://github.com/sunblaze-ucb/verinaで公開しています。

&lt;img src="https://arxiv.org/html/2505.23135v1/x4.png"/&gt;&lt;p&gt;Berkeley, Dawn Song University of California, Jingxuan He, Kaiyu Yang, Meta FAIR, Timothe Kasriel, Zhe Ye, Zhengxu Yan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23135v1</guid><pubDate>Thu, 29 May 2025 06:12:52 +0000</pubDate></item><item><title>ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering</title><link>http://arxiv.org/abs/2505.23723v1</link><description>大規模言語モデル（LLM）ベースのエージェントの登場は、自律的な機械学習（ML）エンジニアリングの発展を大きく前進させました。しかし、既存のアプローチのほとんどは、手動によるプロンプトエンジニアリングに大きく依存しており、多様な実験経験に基づいて適応・最適化することができていません。この点に着目し、今回初めて、オンライン強化学習（RL）を用いてMLタスクに関するインタラクティブな実験を通してLLMエージェントが学習する、学習ベースのエージェント型MLというパラダイムを探求します。これを実現するために、以下の3つの主要なコンポーネントを備えた、新しいエージェント型MLトレーニングフレームワークを提案します。（1）探索を強化したファインチューニング：LLMエージェントが多様なアクションを生成し、RLの探索を強化できるようにします。（2）ステップワイズRL：単一のアクションステップでトレーニングを可能にし、経験収集を加速し、トレーニング効率を向上させます。（3）エージェント型MLに特化した報酬モジュール：多様なMLフィードバック信号を、RL最適化のための一貫した報酬に統合します。このフレームワークを活用し、7BサイズのQwen-2.5 LLMによって駆動される、自律的なMLのためのML-Agentをトレーニングします。驚くべきことに、わずか9つのMLタスクでトレーニングされたにもかかわらず、私たちの7BサイズのML-Agentは、671BサイズのDeepSeek-R1エージェントを上回る性能を発揮します。さらに、継続的なパフォーマンスの向上を達成し、卓越したタスク間の汎化能力を示しています。

&lt;img src="https://arxiv.org/html/2505.23723v1/x1.png"/&gt;&lt;p&gt;Shanghai Jiao Tong University Shanghai AI Laboratory Equal Contribution Corresponding Author&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23723v1</guid><pubDate>Thu, 29 May 2025 17:54:44 +0000</pubDate></item><item><title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title><link>http://arxiv.org/abs/2505.23671v1</link><description>高性能ソフトウェアの開発は複雑な作業であり、専門的な知識が必要です。本稿では、言語モデルの高性能ソフトウェア開発能力を評価するためのベンチマークであるGSOを紹介します。多様なドメインとプログラミング言語にまたがる10のコードベースから、リポジトリのコミット履歴を分析し、102の困難な最適化タスクを特定するための自動化パイプラインを開発し、パフォーマンステストを生成・実行します。エージェントには、コードベースとパフォーマンステストが正確な仕様として提供され、実行時の効率を改善するよう指示されます。その効率は、熟練した開発者による最適化と比較して測定されます。定量的な評価の結果、主要なSWE-Agentは苦戦しており、成功率は5%未満であり、推論時のスケーリングを行っても改善は限定的であることが明らかになりました。定性的な分析では、低レベル言語の扱いの難しさ、安易な最適化戦略の実行、ボトルネックの正確な特定における課題など、主要な失敗要因が特定されました。今後の研究を可能にするため、ベンチマークのコードと成果物、およびエージェントの軌跡を公開します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23671v1</guid><pubDate>Thu, 29 May 2025 17:14:55 +0000</pubDate></item><item><title>Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering</title><link>http://arxiv.org/abs/2505.23604v1</link><description>言語モデル（LM）は標準化されたコーディングベンチマークでは優れた性能を発揮しますが、SWE-BenchにおけるGitHubのissue解決など、現実世界のソフトウェアエンジニアリングタスクでは苦戦します。特にモデルのパラメータが100B未満の場合に顕著です。計算コストが低いため、より小さなモデルが実際には好ましいのですが、その性能向上は依然として困難です。既存のアプローチは主に、高品質なデータを用いた教師ありファインチューニング（SFT）に依存していますが、これを大規模にキュレーションするにはコストがかかります。代替案として、テスト時のスケーリングがあります。これは、複数の出力を生成し、検証器を使ってスコアリングし、最良のものを選択するというものです。効果的ではあるものの、この戦略は過剰なサンプリングと高価なスコアリングを必要とし、その実用性を制限します。そこで、我々は進化的テスト時スケーリング（EvoScale）を提案します。これは、生成を進化的なプロセスとして扱う、サンプル効率の良い手法です。選択と突然変異を通じて出力を反復的に洗練することで、EvoScaleは出力分布をより高いスコアの領域にシフトさせ、正解を見つけるために必要なサンプル数を減らします。繰り返しサンプリングと選択によるオーバーヘッドを削減するために、強化学習（RL）を用いてモデルを自己進化させるように訓練します。推論時に外部の検証器に頼るのではなく、モデルは反復処理を通じて、自身の世代のスコアを自己改善することを学習します。SWE-Bench-Verifiedで評価した結果、EvoScaleにより、我々の32BモデルであるSatori-SWE-32Bは、わずかなサンプル数で100Bを超えるパラメータを持つモデルの性能に匹敵するか、それを上回ることができました。コード、データ、およびモデルは完全にオープンソース化されます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23604v1</guid><pubDate>Thu, 29 May 2025 16:15:36 +0000</pubDate></item><item><title>SWE-bench Goes Live!</title><link>http://arxiv.org/abs/2505.23419v1</link><description>大規模言語モデル（LLM）の能力を評価する上で、現実世界のバグを修正するためのパッチをモデルが生成する課題解決タスクが重要なベンチマークとして登場しました。SWE-benchとその派生版がこの分野で標準となっていますが、初期リリース以降更新されていない、対象リポジトリが限られている、インスタンスの構築と環境設定に手作業が大きく依存しているといった重要な制約があります。これらの要因は、スケーラビリティを阻害し、過学習やデータ汚染のリスクをもたらします。本研究では、これらの課題を克服するために設計された、\textit{ライブ更新可能}なベンチマークである\textbf{SWE-bench-Live}を提案します。初期リリースでは、2024年以降に作成された実際のGitHub issueから派生した1,319のタスクで構成され、93のリポジトリに及びます。各タスクには、再現可能な実行を保証するための専用のDockerイメージが付属しています。当社のベンチマークの中心となるのは、インスタンスの作成から環境設定までのプロセス全体を効率化し、手作業によるボトルネックを取り除き、スケーラビリティと継続的な更新を可能にする自動キュレーションパイプラインである\methodです。SWE-bench-Liveで最先端のエージェントフレームワークとLLMを評価した結果、制御された評価条件下であっても、SWE-benchのような静的なベンチマークと比較して、大きなパフォーマンスのギャップがあることが明らかになりました。この乖離をより深く理解するために、リポジトリの起源、issueの最新性、タスクの難易度について詳細な分析を行いました。ライブリポジトリのアクティビティに基づいた、新鮮で多様で実行可能なベンチマークであるSWE-bench-Liveを提供することで、動的な現実世界のソフトウェア開発環境におけるLLMとエージェントの厳密で汚染耐性のある評価を促進します。

&lt;img src="https://arxiv.org/html/2505.23419v1/x1.png"/&gt;&lt;p&gt;Linghao Zhang Shilin He Chaoyun Zhang Yu Kang Bowen Li Microsoft Shanghai Artificial Intelligence Laboratory, Linghao Zhang  Shilin He  Chaoyun Zhang  Yu Kang  Bowen Li Microsoft&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23419v1</guid><pubDate>Thu, 29 May 2025 13:09:44 +0000</pubDate></item><item><title>Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2505.23187v1</link><description>大規模言語モデルに基づいたマルチエージェントシステム（MAS）は、協調的な推論とエージェント間の批判を通じて、複雑なタスクを解決する上で目覚ましい進歩を示しています。しかし、既存のアプローチは通常、各タスクを個別に扱うため、冗長な計算が発生し、構造的に類似したタスク間での汎化が制限されます。これに対処するため、我々はマルチエージェントクロスタスク経験学習（MAEL）という、LLM駆動のエージェントに明示的なクロスタスク学習と経験蓄積を付与する新しいフレームワークを導入します。タスク解決のワークフローをグラフ構造化されたマルチエージェント協調ネットワークとしてモデル化し、エージェントは明示的な接続性を介して情報を伝播し、連携します。経験学習フェーズでは、タスク解決ワークフローの各ステップの品質を定量化し、結果として得られる報酬を対応する入出力とともに各エージェントの個々の経験プールに保存します。推論時には、エージェントは高報酬でタスクに関連する経験を少数ショットの例として検索し、各推論ステップの有効性を高め、より正確で効率的なマルチエージェント協調を可能にします。多様なデータセットでの実験結果は、MAELがエージェントに以前のタスク経験から効果的に学習する能力を与え、より速い収束を達成し、現在のタスクでより高品質なソリューションを生み出すことを示しています。

&lt;img src="https://arxiv.org/html/2505.23187v1/x1.png"/&gt;&lt;p&gt;Peking University Shanghai Jiao Tong University Tsinghua University Beijing University of Posts, Telecommunications Siemens Tencent Robotics X liyilong@stu.pku.edu.cn qianc@sjtu.edu.cn liuzy@tsinghua.edu.cn sms@tsinghua.edu.cn&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23187v1</guid><pubDate>Thu, 29 May 2025 07:24:37 +0000</pubDate></item><item><title>Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization</title><link>http://arxiv.org/abs/2505.23387v1</link><description>大規模言語モデル（LLM）は機能的に正しい解決策を生成しますが、コード効率においてはしばしば不十分であり、これは実世界での展開における重大なボトルネックとなります。本論文では、この問題に対処するために、新しいテスト時の反復最適化フレームワークを導入します。これは、LLMが実行サンドボックスからの経験的なパフォーマンスフィードバックに基づいてコードを反復的に改良する閉ループシステムを採用しています。我々は、教師ありファインチューニング（SFT）、直接選好最適化（DPO）、およびグループ相対ポリシー最適化（GRPO）の3つのトレーニング戦略を検討します。我々のVenusデータセットとAPPSベンチマークでの実験は、SFTとDPOが効率の向上において急速に飽和することを示しています。対照的に、実行フィードバックを用いた強化学習（RL）を使用するGRPOは、コードのパフォーマンスを継続的に最適化し、pass@1（47％から62％へ）と、効率において人間の提出物を上回る可能性（31％から45％へ）の両方を大幅に向上させます。我々の研究は、効果的なテスト時のコード効率の改善を実証し、LLMにコード効率を真に自己改善させるためのRLの力を明らかにします。

&lt;img src="https://arxiv.org/html/2505.23387v1/x1.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23387v1</guid><pubDate>Thu, 29 May 2025 12:14:29 +0000</pubDate></item><item><title>Representing local protein environments with atomistic foundation models</title><link>http://arxiv.org/abs/2505.23354v1</link><description>タンパク質の局所構造は、その機能や他の分子との相互作用に強く影響します。したがって、タンパク質の局所環境を簡潔かつ情報豊かに表現することは、タンパク質や生体分子の相互作用をモデル化し設計する上で不可欠です。しかし、これらの環境の広範な構造的および化学的変動性により、モデル化が困難であり、そのような表現は十分に研究されていません。本研究では、原子レベルの基盤モデル（AFM）の中間特徴量から導出された、タンパク質の局所環境のための新しい表現を提案します。この埋め込みが、局所構造（例：二次モチーフ）と化学的特徴（例：アミノ酸の同一性およびプロトン化状態）の両方を効果的に捉えることを示します。さらに、AFM由来の表現空間が意味のある構造を示し、生体分子環境の分布に対するデータ駆動型の事前分布の構築を可能にすることを示します。最後に、生体分子NMR分光法の文脈において、提案された表現が、最先端の精度を達成する、この種のものとしては初めての物理情報に基づいた化学シフト予測器を可能にすることを示します。我々の結果は、原子レベルの基盤モデルとその創発的な表現が、従来の分子シミュレーションを超えて、タンパク質モデリングに驚くほど効果的であることを示しています。これは、タンパク質環境のための効果的な機能表現を構築する上で、新たな研究分野を開拓すると信じています。

&lt;img src="https://arxiv.org/html/2505.23354v1/x1.png"/&gt;&lt;p&gt;Advaith Maddipatla, IST Austria Technion, Israel University of Oxford, Meital Bojan ∗, Nadav Bojan Sellam, Sanketh Vedula ∗, UK Equal contribution, \, \dagger, \dagger} start_FLOATSUPERSCRIPT ∗, \ddagger} start_FLOATSUPERSCRIPT ∗, †, † end_FLOATSUPERSCRIPT, † † {}^{*, ‡ end_FLOATSUPERSCRIPT, ‡ † ‡ {}^{*&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23354v1</guid><pubDate>Thu, 29 May 2025 11:25:47 +0000</pubDate></item><item><title>Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking</title><link>http://arxiv.org/abs/2505.23250v1</link><description>我々は、CLEF CheckThat! 2025コンペティションのサブタスク4bにおけるDeep Retrievalチームの手法と結果を発表する。このサブタスクは、与えられたソーシャルメディアの投稿に対して関連する科学文献を検索することに焦点を当てている。このタスクに取り組むため、我々は、字句的な精度、意味的な汎化、および深層文脈再ランキングを組み合わせたハイブリッド検索パイプラインを提案し、インフォーマルな言語とフォーマルな言語のギャップを埋める堅牢な検索を可能にする。具体的には、BM25ベースのキーワードマッチングと、密な意味検索のためにファインチューニングされたINF-Retriever-v1モデルを使用したFAISSベクトルストアを組み合わせる。BM25は上位30件の候補を返し、意味検索は100件の候補を生成し、それらをマージして大規模言語モデル（LLM）ベースのクロスエンコーダを介して再ランキングする。我々のアプローチは、開発セットで76.46％、隠されたテストセットで66.43％の平均逆ランク（MRR@5）を達成し、開発リーダーボードで1位、テストリーダーボードで3位（31チーム中）を獲得し、トップランクのシステムと比較してわずか2パーセントポイントの相対的なパフォーマンスギャップにとどまった。我々は、オープンソースモデルをローカルで実行し、外部のトレーニングデータを使用せずに、この強力なパフォーマンスを達成しており、注意深く設計され、ファインチューニングされた検索パイプラインの有効性を強調している。

&lt;img src="https://arxiv.org/html/2505.23250v1/extracted/6492478/figures/motivation.png"/&gt;&lt;p&gt;Ashwini Kamaraj, Benjamin F. Grewe, Pascal J. Sager, Thilo Stadelmann&lt;/p&gt;&lt;p&gt;Centre for Artificial Intelligence, Zurich University of Applied Sciences, Technikumstrasse 71, 8401 Winterthur, Switzerland
European Centre for Living Technology, Dorsoduro 3246, 30123 Venice, Italy
Institute of Neuroinformatics, ETH Zurich and University of Zurich, Winterthurerstrasse 190, 8057 Zurich, Switzerland
University of Zurich, Rämistrasse 71, 8006 Zurich, Switzerland&lt;/p&gt;</description><guid isPermaLink="false">2505.23250v1</guid><pubDate>Thu, 29 May 2025 08:55:39 +0000</pubDate></item><item><title>OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software</title><link>http://arxiv.org/abs/2505.23239v1</link><description>ユーザビリティ評価は、オープンソースソフトウェア（OSS）のインパクトと普及にとって非常に重要ですが、人間の評価者に依存する従来の方法は、コストが高く、拡張性に限界があります。これらの限界に対処するため、私たちは、オープンソースソフトウェア専用に設計された、自動化され、設定可能で、インタラクティブなエージェントベースのユーザビリティ評価フレームワークであるOSS-UAgentを紹介します。私たちのフレームワークは、大規模言語モデル（LLM）を搭載したインテリジェントエージェントを採用し、さまざまな経験レベル（ジュニアからエキスパートまで）の開発者がプログラミングタスクを実行する様子をシミュレートします。プラットフォーム固有の知識ベースを動的に構築することで、OSS-UAgentは正確でコンテキストを意識したコード生成を保証します。生成されたコードは、コンプライアンス、正確性、可読性など、複数の側面から自動的に評価され、ソフトウェアのユーザビリティを包括的に測定します。さらに、私たちのデモンストレーションでは、グラフ分析プラットフォームの評価におけるOSS-UAgentの実用的な応用を紹介し、ユーザビリティ評価の自動化におけるその有効性を強調しています。

&lt;img src=""/&gt;&lt;p&gt;Jingren Zhou, Lingkai Meng, Long Yuan, Longbin Lai, Peng Cheng, Wenjie Zhang, Wenyuan Yu, Xuemin Lin, Yu Shao&lt;/p&gt;&lt;p&gt;Alibaba Group
East China Normal University
Shanghai Jiao Tong University
Tongji University
University of New South Wales
Wuhan University of Technology&lt;/p&gt;</description><guid isPermaLink="false">2505.23239v1</guid><pubDate>Thu, 29 May 2025 08:40:10 +0000</pubDate></item><item><title>Equivariant Spherical Transformer for Efficient Molecular Modeling</title><link>http://arxiv.org/abs/2505.23086v1</link><description>SE(3)同変グラフニューラルネットワーク（GNN）は、群表現を用いることで分子システムモデリングを大きく進歩させてきました。しかし、テンソル積に基づく畳み込みに依存するメッセージパッシングプロセスは、不十分な非線形性と不完全な群表現によって制限され、表現力が制限されています。これらの制限を克服するために、我々はEquivariant Spherical Transformer（EST）という新しいフレームワークを導入します。これは、フーリエ変換後の群表現の空間領域内でTransformer構造を活用するものです。我々は、ESTがテンソル積の関数空間を包含し、より優れた表現力を達成できることを理論的および経験的に示します。さらに、ESTの同変帰納的バイアスは、フーリエ変換に対する一様なサンプリング戦略によって保証されます。我々の実験では、OC20やQM9を含む様々な分子ベンチマークにおいて、ESTが最先端の性能を示すことを実証しています。

&lt;img src="https://arxiv.org/html/2505.23086v1/x1.png"/&gt;&lt;p&gt;Junyi An SAIS &amp;Xinyu Lu Shanghai Innovation Institute Xiamen University &amp;Chao Qu INFTech &amp;Yunfei Shi SAIS &amp;Peijia Lin † † \dagger † Sun Yat-sen University Shanghai Innovation Institute &amp;Qianwei Tang † † \dagger † Nanjing University &amp;Licheng Xu SAIS &amp;Fenglei Cao SAIS &amp;Yuan Qi ‡ ‡ \ddagger ‡ SAIS Fudan University Zhongshan Hospital&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23086v1</guid><pubDate>Thu, 29 May 2025 04:43:07 +0000</pubDate></item><item><title>SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents</title><link>http://arxiv.org/abs/2505.23559v1</link><description>大規模言語モデル（LLM）エージェントの最近の進歩は、科学的発見の自動化を著しく加速させましたが、同時に重大な倫理的および安全性の懸念も引き起こしました。これらの課題に体系的に対処するために、AI主導の科学的探求における安全性と倫理的責任を強化するために明示的に設計された革新的なAI科学者フレームワークである\textbf{SafeScientist}を紹介します。SafeScientistは、倫理的に不適切または高リスクなタスクを積極的に拒否し、研究プロセス全体を通して安全性を厳格に重視します。包括的な安全性の監視を実現するために、プロンプト監視、エージェント間連携監視、ツール使用監視、および倫理的レビュー担当者コンポーネントを含む、複数の防御メカニズムを統合します。SafeScientistを補完するものとして、科学的文脈におけるAIの安全性を評価するために特別に設計された新しいベンチマークである\textbf{SciSafetyBench}を提案します。これは、6つのドメインにわたる240の高リスク科学タスク、30の特別に設計された科学ツール、および120のツール関連リスクタスクで構成されています。広範な実験により、SafeScientistは、科学的成果の質を損なうことなく、従来のAI科学者フレームワークと比較して、安全性能を35％大幅に向上させることが実証されています。さらに、多様な敵対的攻撃手法に対する安全パイプラインの堅牢性を厳密に検証し、統合されたアプローチの有効性をさらに確認します。コードとデータはhttps://github.com/ulab-uiuc/SafeScientistで入手できます。
\textcolor{red}{警告：この論文には、不快または有害な可能性のあるサンプルデータが含まれています。}

&lt;img src="https://arxiv.org/html/2505.23559v1/x1.png"/&gt;&lt;p&gt;Jiaxun Zhang, Kunlun Zhu, Nuoxing Shang, University of Illinois Urbana-Champaign kunlunz2@illinois.edu, Ziheng Qi&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.23559v1</guid><pubDate>Thu, 29 May 2025 15:35:58 +0000</pubDate></item><item><title>A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors</title><link>http://arxiv.org/abs/2505.23344v1</link><description>非断熱結合（NAC）は、広く使用されている最小スイッチ表面ホッピング（FSSH）などの手法を用いた光化学および光物理プロセスのモデリングにおいて重要な役割を果たします。したがって、シミュレーションを加速するためにNACを機械学習することには強い動機があります。しかし、NACのベクトル的、二価的な性質、および円錐交差シーム付近の特異性のため、これは困難です。今回初めて、我々はドメインの専門知識に基づいてNAC固有の記述子を設計し、それらが0.99を超えるR^2という、これまで報告されたことのない精度でNACを学習できることを示します。成功の鍵は、我々の新しいML位相補正手順にもあります。フルベンのSA-2-CASSCF(6,6)電子構造レベルをターゲットとした、完全にML駆動のFSSHシミュレーションの典型的な例で、我々のアプローチの効率とロバスト性を示します。このML-FSSHダイナミクスは、$S_1$崩壊の正確な記述につながり、多数の軌跡の実行を可能にすることで誤差範囲を縮小します。我々の実装は、オープンソースのMLatomで利用可能です。

&lt;img src="https://arxiv.org/html/2505.23344v1/extracted/6477814/figures/chart.png"/&gt;&lt;p&gt;Jakub Martinka, Jiří Pittner, Lina Zhang, Mario Barbatti, Mikołaj Martyka, Pavlo O. Dral, Yi-Fan Hou&lt;/p&gt;&lt;p&gt;Aix Marseille University, CNRS, ICR, Marseille, France
J. Heyrovský Institute of Physical Chemistry, Academy of Sciences of the Czech Republic, v.v.i. , Dolejškova 3, 18223 Prague 8, Czech Republic
[&lt;/p&gt;</description><guid isPermaLink="false">2505.23344v1</guid><pubDate>Thu, 29 May 2025 11:08:49 +0000</pubDate></item><item><title>Optical Controllable Spin-Polarization in Two Dimensional Altermagnets via Robust Spin-Momentum Locking Excitons</title><link>http://arxiv.org/abs/2505.23237v1</link><description>二次元半導体におけるスピン-運動量ロッキング（SML）励起子は、超高速スピントロニクスにおけるスピン偏極キャリアのプログラム可能な光制御に魅力的である。ゼロ外部磁場安定性と室温でのスピン偏極を備えた長寿命励起子に対する現在の需要に応えるため、本研究では、巨大な非相対論的スピン分裂（&gt; 1.2 eV）によって駆動されるオルター磁性V$_2 X_2$O ($X=$ S, Se)における本質的なSML励起子の存在を予測する。第一原理計算により、単層では1400 meV以上、ファンデルワールスヘテロ二層では430 meVを超える結合エネルギーを持つSML励起子が明らかになり、積層依存の光選択則により調整可能な層間励起子も示される。これらの注目すべき物理的特性は、長い放射寿命と相まって、室温でロバストなスピン偏極を持つSML励起子の実現可能性を強く示唆している。本研究は、新規なオルター磁性を介したSML励起子物理学の新たなパラダイムを提供し、高度なオプトスピントロニクスにおける全光操作の新たな可能性を切り開く。

&lt;img src="https://arxiv.org/html/2505.23237v1/extracted/6491419/Figure1.jpg"/&gt;&lt;p&gt;Erjun Kan, Jinzhe Han, Jiuyu Sun, Yongping Du&lt;/p&gt;&lt;p&gt;MIIT Key Laboratory of Semiconductor Microstructure and Quantum Sensing, Nanjing University of Science and Technology, Nanjing 210094, China&lt;/p&gt;</description><guid isPermaLink="false">2505.23237v1</guid><pubDate>Thu, 29 May 2025 08:39:07 +0000</pubDate></item></channel></rss>