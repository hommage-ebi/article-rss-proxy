<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Tue, 20 May 2025 03:18:00 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-20</title><link>https://arxiv.org/2025-05-20</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12858v1"&gt;Breaking Sensitivity Barriers in Luminescence Thermometry: Synergy Between Structural Phase Transition and Luminescence Thermal Quenching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12914v1"&gt;Mechanistic Insights into the Early Stages of Oxidation at Copper Terrace: The Role of O-O Repulsion and Substrate-mediated Effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12956v1"&gt;Variability analysis in memristors based on electrodeposited prussian blue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13001v1"&gt;Competing Magnetic States in the Candidate Altermagnet GdAlGe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13107v1"&gt;Electronic and optical and topological properties of defects in bismuthene&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13415v1"&gt;Optical signatures of bulk g-wave altermagnetism in MnTe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12819v1"&gt;Theory of charge-to-spin conversion under quantum confinement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13245v1"&gt;Ab initio study of strain-driven vacancy clustering in aluminum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13352v1"&gt;Emergent High-Entropy Phases in Geometrically Frustrated Pyrochlore Magnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12724v1"&gt;Josephson Junctions in the Age of Quantum Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12645v1"&gt;An improved guess for the variational calculation of charge-transfer excitations in large systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12962v1"&gt;The Importance of Layer-Dependent Molecular Twisting for the Structural Anisotropy of Interfacial Water&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12953v1"&gt;Hydrogen Bond Topology Reveals Layering of LDL-like and HDL-like Water at its Liquid/Vapor Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13365v1"&gt;Mixed Quantum-Classical Methods for Polaron Spectral Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13394v1"&gt;Multireference Embedding and Fragmentation Methods for Classical and Quantum Computers: from Model Systems to Realistic Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13435v1"&gt;Optical signatures of coherence in molecular dimers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13042v1"&gt;An introduction to Neural Networks for Physicists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13283v1"&gt;Accelerating Bayesian Optimal Experimental Design via Local Radial Basis Functions: Application to Soft Material Characterization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13305v1"&gt;Kinematic dynamos and resolution limits for Smoothed Particle Magnetohydrodynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13086v1"&gt;Coupled integral equations method with open boundary conditions for calculation the characteristics of structured waveguides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13374v1"&gt;Structure-preserving schemes conserving entropy and kinetic energy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12944v1"&gt;CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12651v1"&gt;$\texttt{DIAMONDs}$: A Dataset for $\mathbb{D}$ynamic $\mathbb{I}$nformation $\mathbb{A}$nd $\mathbb{M}$ental modeling $\mathbb{O}$f $\mathbb{N}$umeric $\mathbb{D}$iscussions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12731v1"&gt;Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12741v1"&gt;Dense Communication between Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12744v1"&gt;Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12746v1"&gt;Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12762v1"&gt;IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12767v1"&gt;Language Models That Walk the Talk: A Framework for Formal Fairness Certificates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12788v1"&gt;Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12822v1"&gt;Emergent Specialization: Rare Token Neurons in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12845v1"&gt;Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13011v1"&gt;Unveiling and Steering Connectome Organization with Interpretable Latent Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13031v1"&gt;MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13175v1"&gt;Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13180v1"&gt;ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13195v1"&gt;Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13355v1"&gt;Multi-Armed Bandits Meet Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13372v1"&gt;Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13406v1"&gt;AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12623v1"&gt;Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12630v1"&gt;Degradation-Aware Feature Perturbation for All-in-One Image Restoration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12641v1"&gt;Single Image Reflection Removal via inter-layer Complementarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12654v1"&gt;Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12655v1"&gt;Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12662v1"&gt;Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12692v1"&gt;Bullying the Machine: How Personas Increase LLM Vulnerability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12701v1"&gt;Counterfactual Explanations for Continuous Action Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12711v1"&gt;Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12716v1"&gt;Shadow-FT: Tuning Instruct via Base&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12737v1"&gt;Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12745v1"&gt;PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12761v1"&gt;Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12781v1"&gt;A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12795v1"&gt;FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12805v1"&gt;FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12814v1"&gt;PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12815v1"&gt;Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12821v1"&gt;SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12837v1"&gt;The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12843v1"&gt;Bias Fitting to Mitigate Length Bias of Reward Model in RLHF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12844v1"&gt;AGI-Elo: How Far Are We From Mastering A Task?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12851v1"&gt;FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12869v1"&gt;Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12882v1"&gt;PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12891v1"&gt;TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12894v1"&gt;HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12903v1"&gt;Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12909v1"&gt;Sinusoidal Initialization, Time for a New Start&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12910v1"&gt;SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12923v1"&gt;The Traitors: Deception and Trust in Multi-Agent Language Model Simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12951v1"&gt;DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12966v1"&gt;Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12983v1"&gt;An Empirical Study of Many-to-Many Summarization with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12996v1"&gt;ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13025v1"&gt;LiBOG: Lifelong Learning for Black-Box Optimizer Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13026v1"&gt;Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13033v1"&gt;TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13036v1"&gt;KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13043v1"&gt;A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13044v1"&gt;CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13053v1"&gt;SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13079v1"&gt;Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13087v1"&gt;Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13101v1"&gt;ARIW-Framework: Adaptive Robust Iterative Watermarking Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13116v1"&gt;Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13126v1"&gt;Zero-Shot Iterative Formalization and Planning in Partially Observable Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13130v1"&gt;Adaptive Image Restoration for Video Surveillance: A Real-Time Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13156v1"&gt;Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13157v1"&gt;Role-Playing Evaluation for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13182v1"&gt;Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13191v1"&gt;Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13201v1"&gt;MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13210v1"&gt;Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13211v1"&gt;MAGI-1: Autoregressive Video Generation at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13232v1"&gt;StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13253v1"&gt;Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13273v1"&gt;Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13287v1"&gt;Level Generation with Quantum Reservoir Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13292v1"&gt;Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13339v1"&gt;OPA-Pack: Object-Property-Aware Robotic Bin Packing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13346v1"&gt;J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13358v1"&gt;One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13379v1"&gt;Thinkless: LLM Learns When to Think&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13380v1"&gt;CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13381v1"&gt;How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13408v1"&gt;CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13425v1"&gt;Learnware of Language Models: Specialized Small Language Models Can Do Big&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13427v1"&gt;MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13437v1"&gt;FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13445v1"&gt;Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13448v1"&gt;CIE: Controlling Language Model Text Generations Using Continuous Signals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12626v1"&gt;scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12632v1"&gt;Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12638v1"&gt;ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12664v1"&gt;Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12669v1"&gt;Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12684v1"&gt;Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12705v1"&gt;DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12707v1"&gt;PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12738v1"&gt;EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12748v1"&gt;TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12750v1"&gt;Malware families discovery via Open-Set Recognition on Android manifest permissions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12751v1"&gt;Structure-based Anomaly Detection and Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12763v1"&gt;Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12774v1"&gt;UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12800v1"&gt;OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12811v1"&gt;Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12863v1"&gt;Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12864v1"&gt;LEXam: Benchmarking Legal Reasoning on 340 Law Exams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12871v1"&gt;Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12872v1"&gt;From Grunts to Grammar: Emergent Language from Cooperative Foraging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12880v1"&gt;AdS-GNN -- a Conformally Equivariant Graph Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12884v1"&gt;TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12886v1"&gt;Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12904v1"&gt;The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12908v1"&gt;Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12920v1"&gt;PyFCG: Fluid Construction Grammar in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12925v1"&gt;CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12929v1"&gt;Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12938v1"&gt;Leveraging LLM Inconsistency to Boost Pass@k Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12942v1"&gt;A3 : an Analytical Low-Rank Approximation Framework for Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12960v1"&gt;Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12963v1"&gt;Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12981v1"&gt;From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12992v1"&gt;Fractured Chain-of-Thought Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13010v1"&gt;To Bias or Not to Bias: Detecting bias in News with bias-detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13023v1"&gt;Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13028v1"&gt;Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13077v1"&gt;Advancing Sequential Numerical Prediction in Autoregressive Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13082v1"&gt;MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13094v1"&gt;Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13098v1"&gt;LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13102v1"&gt;Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13109v1"&gt;FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13118v1"&gt;Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13122v1"&gt;When majority rules, minority loses: bias amplification of gradient descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13123v1"&gt;Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13124v1"&gt;$μ$PC: Scaling Predictive Coding to 100+ Layer Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13136v1"&gt;ModernGBERT: German-only 1B Encoder Model Trained from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13144v1"&gt;Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13188v1"&gt;When a Reinforcement Learning Agent Encounters Unknown Unknowns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13192v1"&gt;True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13196v1"&gt;A Physics-Inspired Optimizer: Velocity Regularized Adam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13208v1"&gt;Efficient Generation of Parameterised Quantum Circuits from Large Texts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13227v1"&gt;Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13257v1"&gt;WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13268v1"&gt;Representation of perceived prosodic similarity of conversational feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13280v1"&gt;FlowPure: Continuous Normalizing Flows for Adversarial Purification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13307v1"&gt;RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13308v1"&gt;Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13315v1"&gt;KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13316v1"&gt;Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13329v1"&gt;Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13338v1"&gt;Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13344v1"&gt;RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13388v1"&gt;R3: Robust Rubric-Agnostic Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13391v1"&gt;Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13393v1"&gt;IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13417v1"&gt;AdaptThink: Reasoning Models Can Learn When to Think&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13438v1"&gt;Optimizing Anytime Reasoning via Budget Relative Policy Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13439v1"&gt;VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12734v1"&gt;SounDiT: Geo-Contextual Soundscape-to-Landscape Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12900v1"&gt;AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13115v1"&gt;Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13264v1"&gt;Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13324v1"&gt;From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12628v1"&gt;Dual-Agent Reinforcement Learning for Automated Feature Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12647v1"&gt;Spiking Neural Network: a low power solution for physical layer authentication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12672v1"&gt;TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12682v1"&gt;RoFL: Robust Fingerprinting of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12683v1"&gt;DimGrow: Memory-Efficient Field-level Embedding Dimension Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12709v1"&gt;Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12736v1"&gt;Deep Unfolding with Kernel-based Quantization in MIMO Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12754v1"&gt;ProDS: Preference-oriented Data Selection for Instruction Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12759v1"&gt;Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12809v1"&gt;Koopman Autoencoders Learn Neural Representation Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12917v1"&gt;Temporal Query Network for Efficient Multivariate Time Series Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12988v1"&gt;Optimal Formats for Weight Quantisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13027v1"&gt;Unpacking Positional Encoding in Transformers: A Spectral Analysis of Content-Position Coupling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13047v1"&gt;PPTNet: A Hybrid Periodic Pattern-Transformer Architecture for Traffic Flow Prediction and Congestion Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13060v1"&gt;Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13071v1"&gt;OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13092v1"&gt;Treatment Effect Estimation for Optimal Decision-Making&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13100v1"&gt;Time series saliency maps: explaining models across multiple domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13111v1"&gt;Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13138v1"&gt;Neurosymbolic Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13150v1"&gt;Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13169v1"&gt;RIFLES: Resource-effIcient Federated LEarning via Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13249v1"&gt;RN-F: A Novel Approach for Mitigating Contaminated Data in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13317v1"&gt;Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13326v1"&gt;Thinking Short and Right Over Thinking Long: Serving LLM Reasoning Efficiently and Accurately&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13342v1"&gt;Detect and Correct: A Selective Noise Correction Method for Learning with Noisy Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13343v1"&gt;MRM3: Machine Readable ML Model Metadata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13377v1"&gt;Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13413v1"&gt;Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13421v1"&gt;Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13446v1"&gt;Unlocking Non-Invasive Brain-to-Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12629v1"&gt;Enhancing Latent Computation in Transformers with Latent Tokens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12642v1"&gt;Two out of Three (ToT): using self-consistency to make robust predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12681v1"&gt;On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12710v1"&gt;Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12758v1"&gt;It's not you, it's me -- Global urban visual perception varies across demographics and personalities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12791v1"&gt;Unlearning for Federated Online Learning to Rank: A Reproducibility Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12801v1"&gt;Testing Identifiability and Transportability with Observational and Experimental Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12803v1"&gt;Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12808v1"&gt;Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12825v1"&gt;Theoretical Investigation on Inductive Bias of Isolation Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12842v1"&gt;GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12879v1"&gt;Spline Dimensional Decomposition with Interpolation-based Optimal Knot Selection for Stochastic Dynamic Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12906v1"&gt;Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12952v1"&gt;LoD: Loss-difference OOD Detection by Intentionally Label-Noisifying Unlabeled Wild Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12967v1"&gt;Augmented Regression Models using Neurochaos Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12982v1"&gt;Multi-parameter Control for the (1+($λ$,$λ$))-GA on OneMax via Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13012v1"&gt;Asymptotic Performance of Time-Varying Bayesian Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13021v1"&gt;The role of data partitioning on the performance of EEG-based deep learning models in supervised cross-subject analysis: a preliminary study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13055v1"&gt;Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio Channels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13072v1"&gt;Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13081v1"&gt;Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13085v1"&gt;Universal Semantic Disentangled Privacy-preserving Speech Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13088v1"&gt;Cross-modal feature fusion for robust point cloud registration with ambiguous geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13112v1"&gt;Attention-based clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13142v1"&gt;Parallel Layer Normalization for Universal Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13186v1"&gt;Interpretable Robotic Friction Learning via Symbolic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13231v1"&gt;Investigating Active Sampling for Hardness Classification with Vision-Based Tactile Sensors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13235v1"&gt;WriteViT: Handwritten Text Generation with Vision Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13241v1"&gt;Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13243v1"&gt;Conformalized Decision Risk Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13244v1"&gt;JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13289v1"&gt;RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13334v1"&gt;Measuring Social Influence with Networked Synthetic Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13345v1"&gt;Occult: Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13357v1"&gt;Introducing Instruction-Accurate Simulators for Performance Estimation of Autotuning Workloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13398v1"&gt;A Minimum Description Length Approach to Regularization in Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13405v1"&gt;A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13418v1"&gt;Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13432v1"&gt;Synthetic-Powered Predictive Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13447v1"&gt;Mean Flows for One-step Generative Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12625v1"&gt;R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12674v1"&gt;Few-Step Diffusion via Score identity Distillation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12686v1"&gt;RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12836v1"&gt;The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12868v1"&gt;Causality-Inspired Robustness for Nonlinear Models via Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12896v1"&gt;On the Thinking-Language Modeling Gap in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12902v1"&gt;Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13058v1"&gt;A Path to Universal Neural Cellular Automata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13189v1"&gt;A Malliavin-Gamma calculus approach to Score Based Diffusion Generative models for random fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13197v1"&gt;Inferring stochastic dynamics with growth from cross-sectional data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13230v1"&gt;Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13299v1"&gt;Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13318v1"&gt;VesselGPT: Autoregressive Modeling of Vascular Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13353v1"&gt;Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13397v1"&gt;Learning by solving differential equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13416v1"&gt;Gluon: Making Muon &amp; Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13422v1"&gt;Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13430v1"&gt;Fine-tuning Quantized Neural Networks with Zeroth-order Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12713v1"&gt;Identifiability of Nonnegative Tucker Decompositions -- Part I: Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12919v1"&gt;RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13052v1"&gt;Model Selection for Gaussian-gated Gaussian Mixture of Experts Using Dendrograms of Mixing Measures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12636v1"&gt;Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12717v1"&gt;ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12768v1"&gt;ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12792v1"&gt;EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12831v1"&gt;Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12859v1"&gt;Re-identification of De-identified Documents with Autoregressive Infilling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12888v1"&gt;GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12949v1"&gt;Neural Morphological Tagging for Nguni Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12950v1"&gt;GuRE:Generative Query REwriter for Legal Passage Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12964v1"&gt;MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12969v1"&gt;Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12970v1"&gt;A Structured Literature Review on Traditional Approaches in Current Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12973v1"&gt;Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13006v1"&gt;Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13034v1"&gt;topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13089v1"&gt;Systematic Generalization in Language Models Scales with Information Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13090v1"&gt;The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13141v1"&gt;Understanding Cross-Lingual Inconsistency in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13147v1"&gt;What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13171v1"&gt;Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13173v1"&gt;A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13204v1"&gt;Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13220v1"&gt;SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13251v1"&gt;Stronger Together: Unleashing the Social Impact of Hate Speech Research&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13252v1"&gt;Natural Language Planning via Coding and Inference Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13254v1"&gt;HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13258v1"&gt;Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13271v1"&gt;CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13282v1"&gt;$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13302v1"&gt;I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13312v1"&gt;GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13348v1"&gt;Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13403v1"&gt;MR. Judge: Multimodal Reasoner as a Judge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13434v1"&gt;SMOTExT: SMOTE meets Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12621v1"&gt;Think Before You Attribute: Improving the Performance of LLMs Attribution Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12718v1"&gt;Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12835v1"&gt;FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13360v1"&gt;What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13404v1"&gt;Granary: Speech Recognition and Translation Dataset in 25 European Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13444v1"&gt;ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.12727v1"&gt;What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13032v1"&gt;MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13069v1"&gt;Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13181v1"&gt;Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.13237v1"&gt;SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-20</guid><pubDate>Tue, 20 May 2025 12:15:08 +0900</pubDate></item><item><title>Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges</title><link>http://arxiv.org/abs/2505.13328v1</link><description>既存の言語モデル（LM）を言語エージェント（LA）としてツール利用を評価するベンチマークは、主にステートレスな単発のやり取りや、単発でのツール選択など部分的な評価に焦点を当てており、複数ターンのアプリケーションにおけるインタラクションに内在するステートフルな性質を見過ごしています。このギャップを埋めるために、私たちは\texttt{DialogTool}を提案します。これは、ツール利用のライフサイクル全体を考慮した、ステートフルなツールインタラクションを伴う複数ターンの対話データセットであり、3つの段階における6つの主要なタスクを対象としています。1）\textit{ツール作成}、2）\textit{ツール利用}：ツール認識、ツール選択、ツール実行、そして3）\textit{役割一貫性のある応答}：応答生成とロールプレイです。さらに、API呼び出しをシミュレートし、作成されたAPIの堅牢性を評価するために、具現化された仮想モバイル評価環境である\texttt{VirtualMobile}を構築します\footnote{本稿では、ツールとAPIを代替的に使用しますが、両者の間に大きな違いはありません。}。これらの成果物を活用して、13種類のオープンソースおよびクローズドソースのLLMに対して包括的な評価を行い、各段階で詳細な分析を提供し、既存の最先端のLLMは、長期にわたってツールをうまく利用することがまだできないことを明らかにします。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13328v1</guid><pubDate>Mon, 19 May 2025 16:36:13 +0000</pubDate></item><item><title>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</title><link>http://arxiv.org/abs/2505.13259v1</link><description>大規模言語モデル（LLM）は、科学的発見におけるパラダイムシフトを触媒しており、タスク特化型の自動化ツールから、ますます自律的なエージェントへと進化し、研究プロセスと人間とAIの協調を根本的に再定義しています。本調査は、この急成長している分野を体系的に図示し、科学におけるLLMの役割の変化と能力の向上に焦点を当てています。科学的方法の観点から、研究ライフサイクルにおけるLLMの自律性と責任の進化を明確にするために、基礎となる3段階の分類（ツール、アナリスト、科学者）を導入します。さらに、ロボット自動化、自己改善、倫理的ガバナンスなど、重要な課題と将来の研究の方向性を特定します。全体として、本調査は、AI主導の科学的発見の未来をナビゲートし、形成するための概念アーキテクチャと戦略的洞察を提供し、迅速なイノベーションと責任ある進歩の両方を促進します。Githubリポジトリ：https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery。

&lt;img src="https://arxiv.org/html/2505.13259v1/x1.png"/&gt;&lt;p&gt;China https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery, Department of Computer Science, Engineering, HKUST, Hong Kong SAR, Hong Ting Tsang, Tianshi Zheng, Weiqi Wang, Zheye Deng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13259v1</guid><pubDate>Mon, 19 May 2025 15:41:32 +0000</pubDate></item><item><title>EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code</title><link>http://arxiv.org/abs/2505.13004v1</link><description>既存のコード生成ベンチマークは、主に機能的な正しさを評価しており、コード効率への注目は限られており、多くの場合Pythonのような単一の言語に限定されています。このギャップに対処するため、LLMが生成したコードの効率を測定するために設計された初の多言語ベンチマークであるEffiBench-Xを導入します。EffiBench-Xは、Python、C++、Java、JavaScript、Ruby、Golangをサポートしています。これには、効率のベースラインとして人間の専門家による解答が用意された競技プログラミングのタスクが含まれています。最先端のLLMをEffiBench-Xで評価した結果、モデルは機能的に正しいコードを生成するものの、効率においては一貫して人間の専門家を下回ることが明らかになりました。最も効率的なLLM生成ソリューション（Qwen3-32B）でさえ、平均して人間の効率の約\textbf{62％}しか達成しておらず、言語固有のばらつきも大きいです。LLMは、Java、C++、GolangよりもPython、Ruby、JavaScriptでより優れた効率を示しています。たとえば、DeepSeek-R1のPythonコードは、Javaコードよりも大幅に効率的です。これらの結果は、多様な言語にわたってコード効率を向上させるためのLLM最適化技術の研究が不可欠であることを強調しています。データセットと評価インフラストラクチャは提出済みであり、https://github.com/EffiBench/EffiBench-X.git および https://huggingface.co/datasets/EffiBench/effibench-x で入手できます。

&lt;img src="https://arxiv.org/html/2505.13004v1/x1.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 HKU 2 UCL 3 NTU 4 NUS 5 HKUST (GZ) 6 HKUST 7 Monash University 8 CSIRO’s Data61 9 KCL&lt;/p&gt;</description><guid isPermaLink="false">2505.13004v1</guid><pubDate>Mon, 19 May 2025 11:43:37 +0000</pubDate></item><item><title>On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding</title><link>http://arxiv.org/abs/2505.12723v1</link><description>大規模言語モデル（LLM）は、コード生成タスクにおいて目覚ましい性能を発揮します。しかし、一般的なプログラミング言語（例：Python、C++）とそれ以外の言語との間には、依然として大きな性能差が存在します。この能力のギャップに対処するため、我々はコード翻訳タスクを活用してLLMを訓練し、多様なプログラミング言語間でのコーディング能力の転移を促進します。さらに、オンポリシーとオフポリシー戦略を統合した新しい強化学習（RL）フレームワークであるOORLを訓練に導入します。OORLでは、コード翻訳中にオンポリシーRLが適用され、ユニットテストから導出されるルールベースの報酬信号によって誘導されます。この粗いルールベースの報酬を補完するために、我々は新しい選好最適化手法であるGroup Equivalent Preference Optimization（GEPO）を提案します。具体的には、GEPOは中間表現（IR）グループを使用してLLMを訓練します。LLMは、ソースコードと同等のIRとそうでないものを識別するように誘導されると同時に、グループ内のIR間の相互同等性に関する信号も利用できます。このプロセスにより、LLMはコード機能の微妙な側面を捉えることができます。コード翻訳タスクを用いたOORLによる訓練を行うことで、LLMはコード機能の認識と、異なる言語で実装されたコード間の関係の理解を向上させます。広範な実験により、コード翻訳タスクを用いたLLM訓練のためのOORLが、複数のプログラミング言語にわたるコードベンチマークにおいて、大幅な性能向上を達成することが示されています。

&lt;img src="https://arxiv.org/html/2505.12723v1/x1.png"/&gt;&lt;p&gt;Haoyuan Wu Rui Ming Jilong Gao Hangyu Zhao Xueyi Chen The Chinese University of Hong Kong Shanghai Artificial Intelligent Laboratory ChatEDA Tech&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12723v1</guid><pubDate>Mon, 19 May 2025 05:25:29 +0000</pubDate></item><item><title>Multi-Level Monte Carlo Training of Neural Operators</title><link>http://arxiv.org/abs/2505.12940v1</link><description>オペレータ学習は、ニューラルオペレータを用いて偏微分方程式（PDE）に関連する非線形オペレータを近似することを目的とした、急速に成長している分野です。これらは入力関数と出力関数の離散化に依存しており、通常、大規模な問題を高解像度で学習させるにはコストがかかります。この動機から、関数離散化の解像度の階層を利用してニューラルオペレータを学習させるための、マルチレベルモンテカルロ（MLMC）アプローチを提案します。私たちのフレームワークは、高精度を維持しながら学習の計算コストを削減するために、高解像度データの少ないサンプルからの勾配補正を使用することに依存しています。提案されたMLMC学習手順は、多重解像度データを受け入れる任意のアーキテクチャに適用できます。最先端のモデルとテストケースを用いた数値実験により、従来の単一解像度学習アプローチと比較して計算効率が向上することが示され、解像度ごとのサンプル数に関連する、精度と計算時間の間のパレート曲線が存在することが強調されています。

&lt;img src="https://arxiv.org/html/2505.12940v1/x1.png"/&gt;&lt;p&gt;CB3 0FD UK &amp;Carola-Bibiane Schönlieb Department of Applied Mathematics, CB3 0WA UK &amp;Stefania Fresca MOX Lab - Department of Mathematics Politecnico di Milano Milano, CB3 0WA UK Nicolas Boullé Department of Mathematics Imperial College London London, I-20133 Pietro Lio Department of Computer Science, James Rowbottom Department of Applied Mathematics, SW7 2AZ UK, Technology University of Cambridge Cambridge, Theoretical Physics University of Cambridge Cambridge&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12940v1</guid><pubDate>Mon, 19 May 2025 10:26:28 +0000</pubDate></item><item><title>Minimum-Excess-Work Guidance</title><link>http://arxiv.org/abs/2505.13375v1</link><description>我々は、熱力学的仕事に触発された正則化フレームワークを提案する。これは、過剰な仕事量を最小化することで、事前学習済みの確率フロー生成モデル（例えば、連続正規化フローや拡散モデル）を誘導するものである。過剰な仕事量とは、統計力学に根ざし、最適輸送と強い概念的なつながりを持つ概念である。我々のアプローチは、科学的応用によく見られるスパースデータ領域において、限られたターゲットサンプルや部分的な密度制約しか利用できない場合でも、効率的な誘導を可能にする。我々は、2つの戦略を導入する。1つは、ユーザー定義のサブセットに確率質量を集中させることで、希少な遷移状態をサンプリングするためのパス誘導、もう1つは、エントロピーを維持しながら、生成された分布を実験的な観測量に整合させるための観測量誘導である。粗視化されたタンパク質モデルにおいて、折り畳まれた状態と折り畳まれていない状態の間の遷移構造をサンプリングし、実験データを用いて系統的なバイアスを修正することで、フレームワークの汎用性を示す。この方法は、熱力学的原理と最新の生成アーキテクチャを結びつけ、データが不足している領域において、標準的なファインチューニングに代わる、原理的で効率的、かつ物理学に触発された代替手段を提供する。実験結果は、サンプル効率の向上とバイアスの低減を強調し、分子シミュレーションおよびそれ以外の分野への適用可能性を強調している。

&lt;img src="https://arxiv.org/html/2505.13375v1/x1.png"/&gt;&lt;p&gt;Cambridge, Chalmers University of Technology, Gothenburg, MA, Munich Max Planck Institute for Intelligent Systems, Research Laboratory of Electronics, Sweden Massachusetts Institute of Technology, Tübingen, USA Technical University of Munich Helmholtz AI, University of Gothenburg&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13375v1</guid><pubDate>Mon, 19 May 2025 17:19:43 +0000</pubDate></item><item><title>Diffusion Models with Double Guidance: Generate with aggregated datasets</title><link>http://arxiv.org/abs/2505.13213v1</link><description>高性能な生成モデルを訓練するための大規模なデータセットを作成することは、特に属性やアノテーションを付与する必要がある場合、しばしば非常にコストがかかります。そのため、既存のデータセットを統合することが一般的な戦略となっています。しかし、データセット間で属性のセットが一致しないことが多く、単純に連結すると、ブロック状に条件が欠落してしまうことがよくあります。これは、複数の属性を条件として同時に使用する条件付き生成モデリングにおいて大きな課題となり、モデルの制御性と適用性を制限します。この問題に対処するため、我々は、すべての条件を同時に含む訓練サンプルが存在しない場合でも、正確な条件付き生成を可能にする、新しい生成アプローチであるDouble Guidance付き拡散モデルを提案します。我々の手法は、共同アノテーションを必要とせずに、複数の条件に対する厳密な制御を維持します。分子生成および画像生成タスクにおいてその有効性を示し、ターゲットの条件付き分布との整合性、および条件が欠落している設定下での制御性において、既存のベースラインを上回る性能を発揮します。

&lt;img src="https://arxiv.org/html/2505.13213v1/x1.png"/&gt;&lt;p&gt;Yanfeng Yang Graduate University of Advanced Studies (SOKENDAI) The Institute of Statistical Mathematics &amp;Kenji Fukumizu The Institute of Statistical Mathematics&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13213v1</guid><pubDate>Mon, 19 May 2025 14:59:35 +0000</pubDate></item><item><title>Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching</title><link>http://arxiv.org/abs/2505.13007v1</link><description>深層生成モデルは科学と工学において有望なツールですが、豊富で高品質なデータへの依存が適用範囲を制限しています。本稿では、限られた、疎な、間接的なデータを補完するために、ドメイン知識を組み込んだランダムフィールド（連続関数上の確率分布）の生成モデリングのための新しいフレームワークを提案します。このアプローチの基礎は、事前学習された変分オートエンコーダ（VAE）の潜在空間における圧縮された関数表現上で生成モデリングを行う潜在フローマッチングです。革新的な点として、VAE内での関数デコーダの採用と、物理的/統計的制約のVAEトレーニングプロセスへの統合が含まれます。これにより、データが限られた状況でも、復号化時にドメイン固有の制約を満たす連続的なランダムフィールドサンプルを生成する潜在関数表現が学習されます。有効性は、疎なセンサーからの風速場再構成と、限られた数の間接的な測定からの材料特性推論という、2つの困難なアプリケーションで実証されています。結果は、提案されたフレームワークが制約のない方法と比較して再構成精度において大幅な改善を達成し、制約なしでは扱いにくい比較的小さなトレーニングデータセットで効果的な推論を可能にすることを示しています。

&lt;img src="https://arxiv.org/html/2505.13007v1/x1.png"/&gt;&lt;p&gt;Geoffrey F. Bomarito NASA Langley Research Center Hampton, James E. Warner NASA Langley Research Center Hampton, Joshua D. Pribe Analytical Mechanics Associates Hampton, Michael C. Stanley Analytical Mechanics Associates Hampton, Patrick E. Leser NASA Langley Research Center Hampton, TX 79409, Tristan A. Shah Texas Tech. University Lubbock, VA 23681&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13007v1</guid><pubDate>Mon, 19 May 2025 11:47:44 +0000</pubDate></item><item><title>Active Learning on Synthons for Molecular Design</title><link>http://arxiv.org/abs/2505.12913v1</link><description>網羅的なバーチャルスクリーニングは非常に有益だが、現代の創薬に関わる高コストな目的関数に対してはしばしば手に負えない。この問題は、分子空間が急速に超巨大化する多ベクトル展開のような組み合わせ的状況下ではさらに悪化する。そこで、シンソン獲得によるスケーラブルなアクティブラーニング（SALSA）を紹介する。これは、プールベースのアクティブラーニングを、シンソンまたはフラグメントの選択におけるモデリングと獲得を分解することで、非可算空間に拡張する、多ベクトル展開に適用可能なシンプルなアルゴリズムである。リガンドベースおよび構造ベースの目的関数に関する実験を通して、SALSAのサンプル効率と、数兆個の化合物空間へのスケーリング能力を強調する。さらに、3つのタンパク質ターゲットにおける多パラメータ目的設計タスクへの応用を実証し、SALSAによって生成された分子が既知の生理活性物質と同等の化学的特性プロファイルを持ち、業界をリードする生成アプローチよりも高い多様性とスコアを示すことを明らかにする。

&lt;img src="https://arxiv.org/html/2505.12913v1/extracted/6426098/figures/2_SALSA.png"/&gt;&lt;p&gt;Tom George Grigg Mason Burlage Oliver Brook Scott Recursion&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12913v1</guid><pubDate>Mon, 19 May 2025 09:48:02 +0000</pubDate></item><item><title>A Comprehensive Benchmarking Platform for Deep Generative Models in Molecular Design</title><link>http://arxiv.org/abs/2505.12848v1</link><description>新規医薬品の開発は、現代科学における重要な課題であり、多大なコストと時間投資を必要とします。深層生成モデルは、広大な化学空間を効率的に探索することで、創薬を加速させる有望なツールとして登場しました。しかし、この急速に進化する分野には標準化された評価プロトコルが欠けており、アプローチ間の公平な比較を妨げています。本研究では、分子設計における深層生成モデルの評価を標準化するために設計された包括的なベンチマークフレームワークであるMolecular Sets (MOSES)プラットフォームの広範な分析を紹介します。リカレントニューラルネットワーク、変分オートエンコーダ、敵対的生成ネットワークを含む複数の生成アーキテクチャの厳密な評価を通じて、特定の化学的特性を維持しながら、有効でユニークかつ新規な分子構造を生成する能力を検証します。我々の発見は、異なるアーキテクチャが様々な指標において相補的な強みを示すことを明らかにし、化学空間における探索と活用の間の複雑なトレードオフを強調しています。本研究は、分子生成における最先端技術に関する詳細な洞察を提供し、AI主導の創薬における将来の進歩のための基盤を確立します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12848v1</guid><pubDate>Mon, 19 May 2025 08:34:38 +0000</pubDate></item><item><title>Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates</title><link>http://arxiv.org/abs/2505.13275v1</link><description>近年、ニューラルネットワークやオペレーター学習を基盤とした、ニューラルPDE代用モデルのための多くのアーキテクチャが提案されてきました。本研究では、関数からスカラーへのマッピングを学習する新しいアーキテクチャであるNeural Functionalを導出し、提案します。その実装は、オペレーター学習とニューラルフィールドからの洞察を活用しており、Neural Functionalが関数微分を暗黙的に学習する能力を示すことができます。これにより、ハミルトニアン関数を学習し、その関数微分を最適化することで、ハミルトニアン力学をニューラルPDE代用モデルに拡張することが初めて可能になります。ハミルトニアンNeural Functionalが、1Dおよび2DのPDEにおいて、安定性の向上とエネルギーのような量の保存を通じて、効果的な代用モデルとなり得ることを実証します。PDE以外にも、関数は物理学において広く普及しています。関数近似とその勾配を用いた学習は、分子動力学や設計最適化など、他の用途にも応用できる可能性があります。

&lt;img src="https://arxiv.org/html/2505.13275v1/extracted/6446621/figures/overview.png"/&gt;&lt;p&gt;Anthony Zhou Carnegie Mellon University &amp;Amir Barati Farimani Carnegie Mellon University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13275v1</guid><pubDate>Mon, 19 May 2025 15:55:38 +0000</pubDate></item><item><title>Robin: A multi-agent system for automating scientific discovery</title><link>http://arxiv.org/abs/2505.13400v1</link><description>科学的発見は、背景調査、仮説生成、実験、データ分析という反復的なプロセスによって推進される。人工知能を科学的発見に応用する最近の進歩にもかかわらず、単一のワークフローでこれらの段階すべてを自動化したシステムはまだ存在しない。本稿では、科学的プロセスの主要な知的ステップを完全に自動化できる初のマルチエージェントシステムであるRobinを紹介する。文献検索エージェントとデータ分析エージェントを統合することで、Robinは仮説を生成し、実験を提案し、実験結果を解釈し、更新された仮説を生成し、科学的発見への半自律的なアプローチを実現する。このシステムを適用することで、先進国における失明の主な原因である加齢黄斑変性症（dAMD）の新しい治療法を特定することができた。Robinは、治療戦略として網膜色素上皮の食作用を促進することを提案し、有望な治療候補であるリパスジルを特定し検証した。リパスジルは、臨床で使用されているRhoキナーゼ（ROCK）阻害剤であり、これまでdAMDの治療薬として提案されたことはなかった。リパスジルによる食作用のアップレギュレーションのメカニズムを解明するために、RobinはRNA-seq追跡実験を提案し分析し、重要な脂質排出ポンプであり、新たな標的となる可能性のあるABCA1のアップレギュレーションを明らかにした。本報告書の本文におけるすべての仮説、実験計画、データ分析、およびデータ図は、Robinによって作成されたものである。反復的なラボインザループの枠組みの中で、新しい治療候補を自律的に発見し検証した最初のAIシステムとして、RobinはAI主導の科学的発見のための新しいパラダイムを確立する。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13400v1</guid><pubDate>Mon, 19 May 2025 17:36:17 +0000</pubDate></item><item><title>Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities</title><link>http://arxiv.org/abs/2505.12680v1</link><description>LLMベースの形式的証明支援系（例えばLean）は、数学的発見の自動化に大きな可能性を秘めている。しかし、構文的な正しさだけでなく、これらのシステムは人間のように数学的構造を真に理解しているのだろうか？我々は、多くの分野で基本的なツールである数学的不等式というレンズを通して、この疑問を調査する。現代の証明器は基本的な不等式を解くことができるが、我々は人間の直感的な構成性に対処する能力を調査する。変数複製、代数的書き換え、多段階構成を含む体系的な変換を通じて、基本的な不等式から構築されたベンチマークIneq-Compを導入する。これらの問題は人間にとっては容易なままだが、Goedel、STP、Kimina-7Bを含むほとんどの証明器が著しく苦戦することがわかった。DeepSeek-Prover-V2-7Bは比較的堅牢性を示す（おそらく問題をサブ問題に分解するように訓練されているため）が、それでも20％の性能低下（pass@32）が見られる。驚くべきことに、構成要素の形式的な証明が文脈で提供された場合でも、すべてのモデルで性能が低いままであり、弱点の源泉が実際に構成的推論にあることを明らかにしている。我々の結果は、現在のAI証明器の汎化行動と人間の数学的直感との間に依然として存在するギャップを露呈している。

&lt;img src="https://arxiv.org/html/2505.12680v1/extracted/6452223/figures/fig-problem.png"/&gt;&lt;p&gt;Haoyu Zhao   &amp;Yihan Geng &amp;Shange Tang &amp;Yong Lin &amp;Bohan Lyu &amp;Hongzhou Lin &amp;Chi Jin &amp;Sanjeev Arora&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12680v1</guid><pubDate>Mon, 19 May 2025 03:56:05 +0000</pubDate></item><item><title>TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents</title><link>http://arxiv.org/abs/2505.13291v1</link><description>我々は、時系列機械学習エンジニアリングの課題において、人工知能（AI）エージェントを評価するためのスケーラブルなベンチマークフレームワークであるTimeSeriesGymを紹介します。既存のベンチマークはスケーラビリティに欠け、明確に定義された設定でのモデル構築に狭く焦点を当て、限られた研究成果物（例：CSV提出ファイル）のみを評価します。AIエージェントのベンチマークを機械学習エンジニアリングの実践により関連付けるために、我々のフレームワークは2つの重要な側面でスケールします。第一に、効果的なMLエンジニアリングには多様なスキルが必要であることを認識し、TimeSeriesGymは複数のドメインとタスクにまたがる多様なソースからの課題を組み込んでいます。我々は、データ処理、研究リポジトリの理解、コード翻訳を含む、孤立した能力とその組み合わせの両方を評価するための課題を設計し、各課題に個別に対処するのではなく、複数の課題を大規模に設計するためのツールを開発します。第二に、正確な数値測定と、より柔軟なLLMベースの評価アプローチの両方を使用して、提出ファイル、コード、モデルを含む複数の研究成果物に対する評価メカニズムを実装します。この二重戦略は、客観的な評価と文脈的な判断のバランスを取ります。我々の最初の焦点は時系列アプリケーションですが、我々のフレームワークは他のデータモダリティにも容易に拡張でき、エージェントAI評価の包括性と実用性を広範囲に向上させます。AIエージェントのMLエンジニアリング能力に関する今後の研究を促進するために、我々のベンチマークフレームワークをオープンソース化します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13291v1</guid><pubDate>Mon, 19 May 2025 16:11:23 +0000</pubDate></item><item><title>Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems</title><link>http://arxiv.org/abs/2505.13246v1</link><description>科学文献の指数関数的な増加は、研究者が複雑な知識の状況を乗り越える上で大きな課題となっています。そこで私たちは、論文をインタラクティブな知識システムに変換することで、従来の出版を補完する、新しいLLM駆動のフレームワーク「Agentic Publications」を提案します。私たちのアーキテクチャは、検索拡張生成とマルチエージェント検証を通じて、構造化データと非構造化コンテンツを統合します。このフレームワークは、人間と機械の両方のためのインターフェースを提供し、物語的な説明と機械可読な出力を組み合わせながら、自動検証と透明性のあるガバナンスを通じて倫理的な考慮事項に対処します。主な機能には、継続的な知識の更新、新しい発見の自動統合、カスタマイズ可能な詳細レベルが含まれます。私たちの概念実証は、多言語インタラクション、APIアクセス性、ベクトルデータベース、知識グラフ、検証エージェントを通じた構造化された知識表現を示しています。このアプローチは、学際的な科学コミュニケーションを強化し、効率とコラボレーションを向上させながら、従来の出版経路を維持します。特に、知識統合が依然として困難な学際的な分野において価値があります。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13246v1</guid><pubDate>Mon, 19 May 2025 15:28:10 +0000</pubDate></item><item><title>ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models</title><link>http://arxiv.org/abs/2505.13176v1</link><description>大規模言語モデル（LLM）に外部ツールを統合することで、リアルタイムの情報や特定の分野のサービスへのアクセス能力が向上しますが、既存のアプローチは、ユーザーの指示に従った機能的なツールの選択に狭く焦点を当てており、ツール選択におけるコンテキストに応じたパーソナライズが見過ごされています。この見落としは、特に重複するツールセットがコンテキスト要因に基づいた微妙な選択を必要とする場合に、ユーザー満足度の低下や非効率的なツールの利用につながります。このギャップを埋めるために、LLMのパーソナライズされたツール利用能力を評価するために設計されたベンチマークであるToolSpectrumを紹介します。具体的には、パーソナライズの2つの重要な側面、ユーザープロファイルと環境要因を形式化し、ツール利用に対するそれらの個別および相乗的な影響を分析します。ToolSpectrumでの広範な実験を通じて、パーソナライズされたツール利用が多様なシナリオでユーザーエクスペリエンスを大幅に向上させることを示します。しかし、最先端のLLMでさえ、ユーザープロファイルと環境要因を総合的に推論する能力は限られており、一方の側面を優先して他方を犠牲にすることがよくあります。私たちの調査結果は、ツール拡張されたLLMにおけるコンテキストに応じたパーソナライズの必要性を強調し、現在のモデルの重大な限界を明らかにしています。私たちのデータとコードは、https://github.com/Chengziha0/ToolSpectrum で入手できます。

&lt;img src="https://arxiv.org/html/2505.13176v1/x1.png"/&gt;&lt;p&gt;Hongru Wang, Yuhang Guo, Zeming Liu, Zihao Cheng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13176v1</guid><pubDate>Mon, 19 May 2025 14:30:46 +0000</pubDate></item><item><title>The Hidden Dangers of Browsing AI Agents</title><link>http://arxiv.org/abs/2505.13076v1</link><description>大規模言語モデル（LLM）を搭載した自律型ブラウジングエージェントは、ウェブベースのタスクを自動化するためにますます使用されています。しかし、動的なコンテンツ、ツール実行、およびユーザー提供のデータへの依存は、広範な攻撃対象領域にさらします。本稿では、そのようなエージェントの包括的なセキュリティ評価を行い、複数のアーキテクチャ層にわたるシステム的な脆弱性に焦点を当てます。私たちの研究は、ブラウジングエージェントに対する最初のエンドツーエンドの脅威モデルを概説し、実際の環境での展開を保護するための実用的なガイダンスを提供します。発見された脅威に対処するために、入力サニタイズ、プランナー実行者の隔離、形式的アナライザー、およびセッション保護を組み込んだ多層防御戦略を提案します。これらの対策は、初期アクセスと攻撃後の両方の攻撃ベクトルから保護します。一般的なオープンソースプロジェクトであるBrowser Useのホワイトボックス分析を通じて、信頼できないウェブコンテンツがエージェントの動作をどのようにハイジャックし、重大なセキュリティ侵害につながるかを示します。私たちの調査結果には、プロンプトインジェクション、ドメイン検証のバイパス、および認証情報の漏洩が含まれており、開示されたCVEと動作する概念実証エクスプロイトによって証明されています。

&lt;img src="https://arxiv.org/html/2505.13076v1/extracted/6453505/images/benchmark_image.png"/&gt;&lt;p&gt;Grzegorz Marcin Wójcik, Markiyan Chaklosh, Mykyta Mudryi&lt;/p&gt;&lt;p&gt;
ARIMLABS.AI
Maria Curie-Sklodowska University in Lublin
Polish-Japanese Academy of Information Technology
University of the National Education Commission in Kraków&lt;/p&gt;</description><guid isPermaLink="false">2505.13076v1</guid><pubDate>Mon, 19 May 2025 13:10:29 +0000</pubDate></item><item><title>Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion</title><link>http://arxiv.org/abs/2505.13073v1</link><description>大規模言語モデルに基づくコード補完技術は、プログラマーの開発効率を大幅に向上させています。しかし、実際の応用においては、現在一般的に使用されているコード補完の評価指標と、ユーザーの実際の認識との間に依然としてギャップが存在します。この問題に対処するため、確率的モデリングの観点から、コード補完タスクに対する2つの評価指標、LCPとROUGE-LCPを提案します。さらに、リポジトリレベルのコード補完シナリオにおけるLLMにおける効果的な構造的意味モデリングとクロスモジュール依存性情報の欠如に対処するため、構造を保持し意味的に並べ替えられたコードグラフ（SPSR-Graph）に基づくデータ処理方法を提案します。理論的分析と実験的検証を通じて、提案された評価指標がユーザーの認識の一貫性において優れていること、およびデータ処理方法がモデルのパフォーマンス向上に効果的であることを示します。

&lt;img src="https://arxiv.org/html/2505.13073v1/x1.png"/&gt;&lt;p&gt;Changsha, China jucaizhai@gmail.com, Hunan, Intelligent System Department, Zhongxing Telecom Equipment(ZTE), liudengfeng9912@gmail.com&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.13073v1</guid><pubDate>Mon, 19 May 2025 13:09:32 +0000</pubDate></item><item><title>AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use</title><link>http://arxiv.org/abs/2505.12650v1</link><description>機械学習に基づく原子間ポテンシャルと力場は、正確な原子構造に大きく依存するが、実験的に決定された結晶の入手可能性が限られているため、そのようなデータは不足している。原子分解能電子顕微鏡は構造データの潜在的な供給源となるが、これらの画像をシミュレーション可能な形式に変換するには依然として多くの労力と誤りが生じやすく、モデルのトレーニングと検証のボトルネックとなっている。我々は、走査透過型電子顕微鏡（STEM）画像を自動的に原子結晶構造に変換し、その物理的特性を予測する、エンドツーエンドのエージェント支援パイプラインであるAutoMatを紹介する。AutoMatは、パターン適応型ノイズ除去、物理学に基づいたテンプレート検索、対称性を考慮した原子再構成、MatterSimによる高速緩和と特性予測、およびすべての段階にわたる協調的なオーケストレーションを組み合わせている。このタスクのために最初の専用STEM2Mat-Benchを提案し、格子RMSD、生成エネルギーMAE、および構造マッチング成功率を使用してパフォーマンスを評価する。AutoMatは、外部ツール呼び出しをオーケストレーションすることにより、テキストのみのLLMがこのドメインでビジョン言語モデルを上回り、パイプライン全体でクローズドループ推論を実現することを可能にする。450を超える構造サンプルでの大規模な実験において、AutoMatは既存のマルチモーダル大規模言語モデルおよびツールを大幅に上回る。これらの結果は、AutoMatとSTEM2Mat-Benchの両方を検証し、材料科学における顕微鏡観察と原子シミュレーションを結びつける重要な一歩となる。コードとデータセットは、https://github.com/yyt-2378/AutoMat および https://huggingface.co/datasets/yaotianvector/STEM2Mat で公開されている。

&lt;img src="https://arxiv.org/html/2505.12650v1/extracted/6452132/figures/teaser_overview.jpg"/&gt;&lt;p&gt;Yaotian Yang &amp;Yiwen Tang &amp;Yizhe Chen &amp;Xiao Chen &amp;Jiangjie Qiu &amp;Hao Xiong &amp;Haoyu Yin &amp;Zhiyao Luo &amp;Yifei Zhang &amp;Sijia Tao &amp;Wentao Li &amp;Qinghua Zhang &amp;Yuqiang Li &amp;Wanli Ouyang &amp;Bin Zhao &amp;Xiaonan Wang &amp;Fei Wei&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12650v1</guid><pubDate>Mon, 19 May 2025 03:04:50 +0000</pubDate></item><item><title>Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs</title><link>http://arxiv.org/abs/2505.12833v1</link><description>多くの現実世界の科学的および産業的応用では、コストのかかるブラックボックス関数の最適化が必要です。ベイズ最適化（BO）は、そのような問題に対して効果的なフレームワークを提供します。しかし、従来のBO手法は局所最適解に陥りやすく、解釈可能な洞察に欠けることがよくあります。この問題に対処するため、本論文では、推論モデルを活用してBOにおけるサンプリングプロセスを誘導し、マルチエージェントシステムと知識グラフを組み込んでオンラインでの知識蓄積を行う、新しいフレームワークであるReasoning BOを設計します。大規模言語モデル（LLM）の推論能力と文脈理解能力を統合することで、BOプロセスを強化するための強力なガイダンスを提供できます。最適化が進むにつれて、Reasoning BOは、妥当な科学理論に基づいた重要な洞察とともに、リアルタイムのサンプリング推奨を提供し、探索空間内で優れたソリューションの発見を支援します。合成数学関数と複雑な現実世界のアプリケーションを含む10種類の多様なタスクにわたって、我々のアプローチを体系的に評価します。このフレームワークは、リアルタイムの洞察と仮説の進化を通じてサンプリング戦略を段階的に洗練し、集中的な探索のために探索空間のより高性能な領域を効果的に特定する能力を示しています。このプロセスは、最適化シナリオにおけるLLMの強力な推論能力と文脈学習能力を強調しています。たとえば、直接アリール化タスクでは、我々の手法は収率を60.7％に向上させましたが、従来のBOではわずか25.2％の収率しか達成できませんでした。さらに、我々の調査により、強化学習を通じてファインチューニングされたより小さなLLMは、より大きなLLMと同等のパフォーマンスを達成できることが明らかになりました。この強化された推論能力は、計算上の実現可能性を維持しながら、より効率的な自動化された科学実験への道を開きます。

&lt;img src="https://arxiv.org/html/2505.12833v1/extracted/6452615/figures/multi-round_v4.png"/&gt;&lt;p&gt;Telecommunications &amp;Tianfan Fu Nanjing University Shanghai Artificial Intelligence Laboratory &amp;Yuqiang Li Shanghai Artificial Intelligence Laboratory, Zhuo Yang Shanghai Artificial Intelligence Laboratory Xidian University &amp;Lingli Ge Shanghai Artificial Intelligence
Laboratory Fudan University &amp;Dong
Han Shanghai Artificial Intelligence Laboratory Beijing University of
Posts&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.12833v1</guid><pubDate>Mon, 19 May 2025 08:20:40 +0000</pubDate></item><item><title>The Enduring Relevance of Semiempirical Quantum Mechanics</title><link>http://arxiv.org/abs/2505.13424v1</link><description>原子系の量子力学的記述を簡略化するための半経験的モデルの開発は、量子力学の発見直後に始まり、今日まで続いている慣習です。現在、原子シミュレーションのための多くの手法が存在し、多くのソフトウェア実装と多くのユーザーが存在し、ソフトウェア市場と見なせるほど大規模になっています。半経験的モデルは、この市場の初期には大きなシェアを占めていましたが、原子シミュレーションの研究活動は、過去30年間で、汎用的だが高価な第一原理量子力学的手法と、高速だが特殊用途の分子力学的手法へと着実に二極化してきました。半経験的モデリングという中間の立場から、原子シミュレーションにおける最近の動向について考察し、その過去の成功から学び、将来の成長の可能性を探ります。特に、半経験的量子力学と機械学習モデルを組み合わせる研究活動が活発に行われており、より柔軟な理論的枠組みとよりモジュール化されたソフトウェアコンポーネントを用いて、第一原理量子力学と半経験的量子力学をより緊密に統合する、まだ実現されていない可能性も存在します。

&lt;img src="https://arxiv.org/html/2505.13424v1/x2.png"/&gt;&lt;p&gt;Jonathan E. Moussa&lt;/p&gt;&lt;p&gt;Molecular Sciences Software Institute, Virginia Tech, Blacksburg, VA 24060&lt;/p&gt;</description><guid isPermaLink="false">2505.13424v1</guid><pubDate>Mon, 19 May 2025 17:53:45 +0000</pubDate></item><item><title>Lattice thermal conductivity of 16 elemental metals from molecular dynamics simulations with a unified neuroevolution potential</title><link>http://arxiv.org/abs/2505.13179v1</link><description>金属は集積回路などの電子デバイスにおける熱管理において重要な役割を果たしており、基礎的な金属や合金における熱輸送を理解することが不可欠です。本研究では、効率的な均一非平衡分子動力学（HNEMD）法と、最近開発された16種類の金属とその合金に対する統一ニューロエボリューションポテンシャルバージョン1（UNEP-v1）を用いて、16種類の金属におけるフォノン熱輸送を系統的に研究します。我々は、ボルツマン輸送方程式（BTE）に基づく既存の結果と比較し、我々のHNEMDの結果が、フォノン-フォノン散乱のみを考慮して得られたBTEの結果とよく一致することを見出しました。対照的に、従来の埋め込み原子法ポテンシャルに基づくHNEMDの結果は、BTEの結果との一致度が低いことが示されました。様々な金属合金で実証されたUNEP-v1モデルの高い精度を考慮すると、HNEMD法とUNEP-v1モデルの組み合わせは、高エントロピー合金のような複雑な系におけるフォノン熱輸送特性を探索するための有望なツールになると期待されます。

&lt;img src="https://arxiv.org/html/2505.13179v1/x1.png"/&gt;&lt;p&gt;Ao Wang, Hua Bao, Ping Qian, Shuo Cao, Ye Su, Yu Yan, Zheyong Fan&lt;/p&gt;&lt;p&gt;Beijing Advanced Innovation Center for Materials Genome Engineering, University of Science and Technology Beijing, Beijing 100083, P. R. China
College of Physical Science and Technology, Bohai University, Jinzhou, P. R. China
Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai 200240, P. R. China
Institute of Condensed Matter and Nanosciences, Université catholique de Louvain, Chemin des Étoiles 8, 1348 Louvain-la-Neuve, Belgium&lt;/p&gt;</description><guid isPermaLink="false">2505.13179v1</guid><pubDate>Mon, 19 May 2025 14:35:00 +0000</pubDate></item><item><title>Nanoindentation simulations for copper and tungsten with adaptive-precision potentials</title><link>http://arxiv.org/abs/2505.12958v1</link><description>我々は、代表的な面心立方金属である銅と体心立方金属であるタングステンに対して、異なる精度と計算コストを含む相互作用ポテンシャルの新しい適応精度記述を用いてナノインデンテーションシミュレーションを実行する。計算効率の高い埋め込み原子法（EAM）ポテンシャルと、原子クラスター展開（ACE）に基づく高精度だが計算効率の低い機械学習ポテンシャルを組み合わせて、ナノインデンテーション用に調整された適応精度（AP）ポテンシャルを作成する。数値計算コストの高いACEポテンシャルは、高い精度が要求される計算セル内の領域でのみ選択的に使用される。純粋なEAMおよび純粋なACEシミュレーションとの比較により、Cuの場合、すべてのポテンシャルが圧子下の同様の転位形態をもたらし、定量的な差はわずかであることが示される。対照的に、Wの場合、中心力EAMポテンシャルを用いたシミュレーションでは、ACEポテンシャルを用いて得られた結果と比較して、著しく異なる塑性メカニズムが観察される。ACEポテンシャルは、半充填dバンドにより、Wにおける結合の角度特性を正確に記述できる。すべてのACE固有のメカニズムは、APナノインデンテーションシミュレーションで再現されるが、純粋なACEシミュレーションと比較して20〜30倍の大幅な高速化が実現される。したがって、APポテンシャルは、高精度なACEと高速なEAMポテンシャルの両方の利点を組み合わせることで、両ポテンシャル間のパフォーマンスギャップを克服する。

&lt;img src="https://arxiv.org/html/2505.12958v1/x1.png"/&gt;&lt;p&gt;David Immel 0000-0001-5143-8043, Godehard Sutmann 0000-0002-9004-604X, Matous Mrovec 0000-0001-8216-2254, Ralf Drautz 0000-0001-7101-8804&lt;/p&gt;&lt;p&gt;Interdisciplinary Centre for Advanced Materials Simulations (ICAMS), Ruhr Universität Bochum, Bochum, Germany
Jülich Supercomputing Centre (JSC), Institute for Advanced Simulation (IAS), Forschungszentrum Jülich, Jülich, Germany
Jülich Supercomputing Centre (JSC), Institute for Advanced Simulation (IAS), Forschungszentrum Jülich, Jülich, Germany Interdisciplinary Centre for Advanced Materials Simulations (ICAMS), Ruhr Universität Bochum, Bochum, Germany&lt;/p&gt;</description><guid isPermaLink="false">2505.12958v1</guid><pubDate>Mon, 19 May 2025 10:53:56 +0000</pubDate></item><item><title>Ultrafast Laser Induces Macroscopic Symmetry-Breaking of Diamond Color Centers</title><link>http://arxiv.org/abs/2505.12989v1</link><description>我々は、負に帯電した窒素空孔中心（NV$^{-}$）における電子-フォノン-スピン相関ダイナミクスを調査し、包括的な動的描像を構築するために、リアルタイム時間依存密度汎関数理論（RT-TDDFT）を用いる。レーザー励起は、100 fs以内に少数スピン電子を促進し、3回回転対称性の破れ（3RSB）電荷秩序を確立する。その後、励起電子のポテンシャルエネルギー面上でのイオン運動は、2つの異なる動的モードを生成する。（1）炭素-窒素結合の対称振動、および（2）3RSBを伴う動的ヤーン・テラー歪み（DJT）。これらの歪みは、ダイヤモンド格子内に非局所的なコヒーレントフォノンを誘起し、音速（$\sim$2~\AA/fs）で3RSBを伴って伝播する。さらに、NV$^{-}$のスピン状態は光励起中も保持されるが、増強されたスピン-軌道-フォノン結合を介して100 fs以内に急速な再配向を受ける。我々のRT-TDDFTシミュレーションは、これらのプロセスの直接的な時間分解可視化を提供し、NV$^{-}$中心における電子、フォノン、およびスピンの微視的な相互作用に関する新たな洞察を提供する。これらの結果は、固体量子システムにおける動的メカニズムの基本的な理解を深め、NV$^{-}$ベースの量子センシング技術の最適化に影響を与える。

&lt;img src="https://arxiv.org/html/2505.12989v1/extracted/6453584/Figure1.png"/&gt;&lt;p&gt;Chao Lian, Chao-Bo Liu, Qi Xiao, Qi-Zheng Ji, Yang Gao&lt;/p&gt;&lt;p&gt;Beijing Institute of Spacecraft Environment Engineering,100094 Beijing, People’s Republic of China
Beijing National Laboratory for Condensed Matter
Physics and Institute of Physics, Chinese Academy of Sciences,
Beijing, 100190, P. R. China
Songshan Lake Materials Laboratory, Dongguan City, Guangdong Province, P. R. China&lt;/p&gt;</description><guid isPermaLink="false">2505.12989v1</guid><pubDate>Mon, 19 May 2025 11:27:37 +0000</pubDate></item><item><title>A Deep Learning Potential for Accurate Shock Response Simulations in Tin</title><link>http://arxiv.org/abs/2505.12698v1</link><description>錫（Sn）は、衝撃荷重下における延性金属の動的機械応答の研究において重要な役割を果たします。原子シミュレーションは、動的応答の重要な挙動に関するナノスケールのメカニズムを解明するのに役立ちます。しかし、既存のSnの経験的ポテンシャルは、そのようなシミュレーションに適用した場合、十分な精度を欠くことがよくあります。特に、Snの固相-固相転移挙動は、原子間ポテンシャルの精度にとって大きな課題となります。これらの課題に対処するため、本研究では、特に衝撃応答シミュレーション用に最適化された、Snの機械学習ポテンシャルモデルを導入します。このモデルは、同時学習フレームワークを通じて構築されたデータセットを用いてトレーニングされており、0〜100 GPaおよび0〜5000 Kの熱力学的条件下での分子シミュレーション用に設計されており、固体相と液体相の両方、および自由表面を持つ構造を包含します。密度汎関数理論（DFT）から導出された基本的な特性、実験的な融解曲線、固相-固相境界、および衝撃Hugoniotの結果を正確に再現します。これは、このモデルがSnの大規模動的シミュレーションにおいて、第一原理計算の精度と橋渡しをする可能性を示しています。

&lt;img src="https://arxiv.org/html/2505.12698v1/extracted/6441280/slip_sys.png"/&gt;&lt;p&gt;Han Wang, Mohan Chen, Wanghui Li, Xiaoyang Wang, Yixin Chen&lt;/p&gt;&lt;p&gt;AI for Science Institute, Beijing 100084, People’s Republic of China
HEDPS, CAPT, School of Physics and College of Engineering, Peking University, Beijing 100871, People’s Republic of China
Institute of Applied Physics and Computational Mathematics,
Beijing 100094, People’s Republic of China
Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore 138632, Republic of Singapore&lt;/p&gt;</description><guid isPermaLink="false">2505.12698v1</guid><pubDate>Mon, 19 May 2025 04:39:39 +0000</pubDate></item></channel></rss>