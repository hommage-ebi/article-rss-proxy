<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Wed, 28 May 2025 03:19:20 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-28</title><link>https://arxiv.org/2025-05-28</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20695v1"&gt;Highly Enhanced robust room temperature ferromagnetism in CVD-grown nano-dimensional MoS2 flakes by modifying edges and defect engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21096v1"&gt;Ultrafast atomic dimerization of Peierls distortion in semimetal molybdenum ditelluride&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21174v1"&gt;Atomistic and experimental study of microstructural evolution in nanocrystalline iron subjected to irradiation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21211v1"&gt;Large strain contribution to the laser-driven magnetization response of magnetostrictive TbFe$_{2}$&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21234v1"&gt;Charge Carrier Mobilities in gamma-Graphynes: A computational approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21440v1"&gt;Tuning Ultra-Narrow Direct Bandgap in alpha-Sn Nanocrystals: A CMOS-Compatible Approach for THz Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20790v1"&gt;Defects and acceptor removal in 60Co γ-irradiated p-type silicon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21143v1"&gt;Optically detected magnetic resonance of wafer-scale hexagonal boron nitride thin films&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21235v1"&gt;From Polyhedra to Crystals: A Graph-Theoretic Framework for Crystal Structure Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21300v1"&gt;Low-energy, ultrafast spin reorientation at competing hybrid interfaces with tunable operating temperature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21434v1"&gt;Degradation and SEI Evolution in Alloy Anodes Revealed by Correlative Liquid-Cell Electrochemistry and Cryogenic Microscopy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20843v1"&gt;Ferroelastic Altermagnetism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21293v1"&gt;Strain controlled g- to d-wave transition in altermagnetic CrSb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21436v1"&gt;Superconducting Acoustogalvanic Effect in Twisted Transition Metal Dichalcogenides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20823v1"&gt;Quantum Dynamics Predicts Coherent Oscillatory Behavior in the Early-times of a Photoisomerization Reaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21469v1"&gt;PropMolFlow: Property-guided Molecule Generation with Geometry-Complete Flow Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21009v1"&gt;The non-equilibrium thermodynamics of active suspensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21247v1"&gt;Kernel Ridge Regression for conformer ensembles made easy with Structured Orthogonal Random Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21302v1"&gt;Reduced Density Matrices and Phase-Space Distributions in Thermofield Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21031v1"&gt;Reactive molecular dynamics approach to PFAS plasma oxidation in water&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21203v1"&gt;Quantum Optimal Control Using MAGICARP: Combining Pontryagin's Maximum Principle and Gradient Ascent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21308v1"&gt;Dissipative Preparation of Many-Body Quantum States: Towards Practical Quantum Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21424v1"&gt;A Hyperbolic Approximation of the Nonlinear Schrödinger Equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20642v1"&gt;CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20662v1"&gt;AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20670v1"&gt;MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20672v1"&gt;GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20728v1"&gt;Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20733v1"&gt;E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20737v1"&gt;RRO: LLM Agent Optimization Through Rising Reward Trajectories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20740v1"&gt;MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20869v1"&gt;Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20889v1"&gt;Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20948v1"&gt;Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21045v1"&gt;Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21055v1"&gt;Agent-Environment Alignment via Automated Interface Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21067v1"&gt;Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21106v1"&gt;Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21212v1"&gt;Interpretable DNFs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21279v1"&gt;XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21281v1"&gt;RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21291v1"&gt;Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21322v1"&gt;Assured Autonomy with Neuro-Symbolic Perception&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21486v1"&gt;Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20637v1"&gt;TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20643v1"&gt;Can Past Experience Accelerate LLM Reasoning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20644v1"&gt;HCQA-1.5 @ Ego4D EgoSchema Challenge 2025&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20648v1"&gt;Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20653v1"&gt;RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20654v1"&gt;Chinese Cyberbullying Detection: Dataset, Method, and Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20664v1"&gt;Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20666v1"&gt;Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20671v1"&gt;LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20674v1"&gt;Pretraining Language Models to Ponder in Continuous Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20686v1"&gt;Accelerating RL for LLM Reasoning with Optimal Advantage Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20691v1"&gt;Evidential Deep Active Learning for Semi-Supervised Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20718v1"&gt;VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20734v1"&gt;Adversarial bandit optimization for approximately linear functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20751v1"&gt;Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20753v1"&gt;Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20759v1"&gt;PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20767v1"&gt;CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20771v1"&gt;Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20783v1"&gt;FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20793v1"&gt;Rendering-Aware Reinforcement Learning for Vector Graphics Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20813v1"&gt;RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20824v1"&gt;MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20853v1"&gt;Cooperation of Experts: Fusing Heterogeneous Information with Large Margin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20875v1"&gt;Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20881v1"&gt;Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20888v1"&gt;EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20890v1"&gt;Frequency Composition for Compressed and Domain-Adaptive Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20901v1"&gt;A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20921v1"&gt;Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20925v1"&gt;Multi-objective Large Language Model Alignment with Hierarchical Experts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20947v1"&gt;Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20949v1"&gt;Streamlining Knowledge Graph Creation with PyRML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20963v1"&gt;Context-Aware Content Moderation for German Newspaper Comments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20973v1"&gt;Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20993v1"&gt;Who Reasons in the Large Language Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21027v1"&gt;TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21036v1"&gt;RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21040v1"&gt;FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21061v1"&gt;LPOI: Listwise Preference Optimization for Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21077v1"&gt;Efficient Large Language Model Inference with Neural Block Linearization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21092v1"&gt;BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21140v1"&gt;HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21160v1"&gt;STEB: In Search of the Best Evaluation Approach for Synthetic Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21170v1"&gt;Quantum AIXI: Universal Intelligence via Quantum Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21171v1"&gt;M-Wanda: Improving One-Shot Pruning for Multilingual LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21180v1"&gt;Latent label distribution grid representation for modeling uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21182v1"&gt;Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21190v1"&gt;Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21218v1"&gt;Pretrained LLMs Learn Multiple Types of Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21219v1"&gt;Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21265v1"&gt;Multilingual Pretraining for Pixel Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21288v1"&gt;GSAT: Graph Structure Attention Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21301v1"&gt;How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21317v1"&gt;A Cross Modal Knowledge Distillation &amp; Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21327v1"&gt;MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21339v1"&gt;An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21344v1"&gt;The Multilingual Divide and Its Impact on Global AI Safety&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21363v1"&gt;Subgroups Matter for Robust Bias Mitigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21364v1"&gt;Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21393v1"&gt;Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21398v1"&gt;A Structured Unplugged Approach for Foundational AI Literacy in Primary Education&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21419v1"&gt;Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21420v1"&gt;Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21427v1"&gt;Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21432v1"&gt;Hume: Introducing System-2 Thinking in Visual-Language-Action Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21457v1"&gt;Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21478v1"&gt;Policy Optimized Text-to-Image Pipeline Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21499v1"&gt;AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21505v1"&gt;How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20633v1"&gt;Test-Time Learning for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20635v1"&gt;Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20646v1"&gt;Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20650v1"&gt;FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20692v1"&gt;Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20697v1"&gt;Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20707v1"&gt;Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20714v1"&gt;Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20730v1"&gt;What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20776v1"&gt;SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20794v1"&gt;VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20866v1"&gt;Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20868v1"&gt;Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20872v1"&gt;In Context Learning with Vision Transformers: Case Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20896v1"&gt;How Do Transformers Learn Variable Binding in Symbolic Programs?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20897v1"&gt;Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20918v1"&gt;Humble AI in the real-world: the case of algorithmic hiring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20922v1"&gt;Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20956v1"&gt;Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20961v1"&gt;Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20979v1"&gt;MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21012v1"&gt;Federated Instrumental Variable Analysis via Federated Generalized Method of Moments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21025v1"&gt;Text-Queried Audio Source Separation via Hierarchical Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21026v1"&gt;Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21032v1"&gt;FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21038v1"&gt;Fixed-Point Traps and Identity Emergence in Educational Feedback Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21087v1"&gt;Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21091v1"&gt;Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21097v1"&gt;Thinker: Learning to Think Fast and Slow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21119v1"&gt;Universal Value-Function Uncertainties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21136v1"&gt;SageAttention2++: A More Efficient Implementation of SageAttention2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21154v1"&gt;GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21184v1"&gt;PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21189v1"&gt;Exploring the Latent Capacity of LLMs for One-Step Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21228v1"&gt;Is Hyperbolic Space All You Need for Medical Anomaly Detection?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21230v1"&gt;PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21236v1"&gt;Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21277v1"&gt;Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21355v1"&gt;Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21362v1"&gt;Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21388v1"&gt;DeSocial: Blockchain-based Decentralized Social Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21391v1"&gt;Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21399v1"&gt;Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21409v1"&gt;RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21410v1"&gt;MRSD: Multi-Resolution Skill Discovery for HRL Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21414v1"&gt;A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21441v1"&gt;Autoencoding Random Forests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21445v1"&gt;VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21488v1"&gt;Be Decisive: Noise-Induced Layouts for Multi-Subject Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21500v1"&gt;ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21503v1"&gt;Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21046v1"&gt;A domain adaptation neural network for digital twin-supported fault diagnosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21074v1"&gt;Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21109v1"&gt;A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21156v1"&gt;Model as Loss: A Self-Consistent Training Paradigm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21329v1"&gt;Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21335v1"&gt;Structure from Collision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21426v1"&gt;Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21459v1"&gt;LazyVLM: Neuro-Symbolic Approach to Video Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20659v1"&gt;An Optimisation Framework for Unsupervised Environment Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20698v1"&gt;Sparsified State-Space Models are Efficient Highway Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20716v1"&gt;Are Data Embeddings effective in time series forecasting?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20742v1"&gt;'Hello, World!': Making GNNs Talk with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20765v1"&gt;Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20774v1"&gt;TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20807v1"&gt;Simple yet Effective Graph Distillation via Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20815v1"&gt;Interpretable Credit Default Prediction with Ensemble Learning and SHAP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20839v1"&gt;FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20840v1"&gt;Aggregation Buffer: Revisiting DropEdge with a New Parameter Block&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20892v1"&gt;One-Time Soft Alignment Enables Resilient Learning without Weight Transport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20930v1"&gt;MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20934v1"&gt;NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20938v1"&gt;Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20970v1"&gt;Understanding the behavior of representation forgetting in continual learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21095v1"&gt;Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21141v1"&gt;A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21147v1"&gt;Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21201v1"&gt;Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21226v1"&gt;Why Do More Experts Fail? A Theoretical Analysis of Model Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21251v1"&gt;Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21321v1"&gt;Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21330v1"&gt;UGCE: User-Guided Incremental Counterfactual Exploration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21347v1"&gt;OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21360v1"&gt;CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21366v1"&gt;PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21382v1"&gt;DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21395v1"&gt;Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21422v1"&gt;When Shift Happens - Confounding Is to Blame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21430v1"&gt;Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21438v1"&gt;Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21444v1"&gt;Can Large Reasoning Models Self-Train?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20628v1"&gt;Position: Adopt Constraints Over Penalties in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20629v1"&gt;Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20634v1"&gt;Explaining Concept Shift with Interpretable Feature Attribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20694v1"&gt;Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20705v1"&gt;Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20723v1"&gt;LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20725v1"&gt;A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20731v1"&gt;Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20732v1"&gt;SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20739v1"&gt;Detecting Informative Channels: ActionFormer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20754v1"&gt;Stationary MMD Points for Cubature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20755v1"&gt;Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20761v1"&gt;Practical estimation of the optimal classification error with soft labels and calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20764v1"&gt;ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20772v1"&gt;MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20781v1"&gt;STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20788v1"&gt;Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20789v1"&gt;Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20797v1"&gt;Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20802v1"&gt;Leaner Transformers: More Heads, Less Depth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20804v1"&gt;Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20817v1"&gt;Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20836v1"&gt;HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20863v1"&gt;Leveraging Diffusion Models for Parameterized Quantum Circuit Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20882v1"&gt;Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20885v1"&gt;Improved Bounds for Swap Multicalibration and Swap Omniprediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20924v1"&gt;Label Leakage in Federated Inertial-based Human Activity Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20955v1"&gt;Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20980v1"&gt;Identifying Super Spreaders in Multilayer Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20992v1"&gt;Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21010v1"&gt;Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21019v1"&gt;Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21020v1"&gt;NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21024v1"&gt;Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21073v1"&gt;Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21101v1"&gt;Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21133v1"&gt;Robust and Computation-Aware Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21191v1"&gt;Unveiling Instruction-Specific Neurons &amp; Experts: An Analytical Framework for LLM's Instruction-Following Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21204v1"&gt;Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21215v1"&gt;Transfer learning for multifidelity simulation-based inference in cosmology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21220v1"&gt;Wavelet Flow For Extragalactic Foreground Simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21285v1"&gt;Learnable Kernel Density Estimation for Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21289v1"&gt;LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21354v1"&gt;Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21357v1"&gt;AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21421v1"&gt;A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21423v1"&gt;Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21468v1"&gt;Causal Posterior Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21475v1"&gt;Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21487v1"&gt;Hardware-Efficient Attention for Fast Decoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21493v1"&gt;Reinforcing General Reasoning without Verifiers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20647v1"&gt;Moment Expansions of the Energy Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20679v1"&gt;SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20688v1"&gt;A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20693v1"&gt;Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20721v1"&gt;Recurrent Neural Operators: Stable Long-Term PDE Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20745v1"&gt;Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20775v1"&gt;Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20894v1"&gt;DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20964v1"&gt;Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21039v1"&gt;Scalable and adaptive prediction bands with kernel sum-of-squares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21135v1"&gt;Learning Single Index Models with Diffusion Priors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21139v1"&gt;Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21173v1"&gt;Topological Deep Learning for Speech Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21208v1"&gt;Input Convex Kolmogorov Arnold Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21304v1"&gt;Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21336v1"&gt;Joint Learning in the Gaussian Single Index Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21356v1"&gt;Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21460v1"&gt;High-Dimensional Calibration from Swap Regret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21496v1"&gt;UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20943v1"&gt;Efficient Spectral Control of Partially Observed Linear Dynamical Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21331v1"&gt;Scheduling with Uncertain Holding Costs and its Application to Content Moderation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20950v1"&gt;Scattering Networks on Noncommutative Finite Groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21400v1"&gt;A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20624v1"&gt;POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20625v1"&gt;Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20645v1"&gt;STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20658v1"&gt;Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20700v1"&gt;Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20738v1"&gt;Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20809v1"&gt;Improved Representation Steering for Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20816v1"&gt;Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20819v1"&gt;Tracing and Reversing Rank-One Model Edits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20825v1"&gt;Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20826v1"&gt;AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20841v1"&gt;Concealment of Intent: A Game-Theoretic Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20871v1"&gt;Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20874v1"&gt;Can LLMs Learn to Map the World from Local Descriptions?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20880v1"&gt;MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20903v1"&gt;Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20910v1"&gt;Automated Privacy Information Annotation in Large Language Model Interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20933v1"&gt;Information-Theoretic Complementary Prompts for Improved Continual Text Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20937v1"&gt;On VLMs for Diverse Tasks in Multimodal Meme Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20976v1"&gt;Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20977v1"&gt;Evaluating and Steering Modality Preferences in Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20995v1"&gt;Articulatory strategy in vowel production as a basis for speaker discrimination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21003v1"&gt;Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21011v1"&gt;LLMs are Frequency Pattern Learners in Natural Language Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21033v1"&gt;Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21072v1"&gt;Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21082v1"&gt;LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21115v1"&gt;Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21172v1"&gt;TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21178v1"&gt;Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21224v1"&gt;A Representation Level Analysis of NMT Model Robustness to Grammatical Errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21239v1"&gt;LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21242v1"&gt;Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21250v1"&gt;ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21315v1"&gt;Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21324v1"&gt;Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21342v1"&gt;PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21378v1"&gt;Analyzing values about gendered language reform in LLMs' revisions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21380v1"&gt;PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21397v1"&gt;DecisionFlow: Advancing Large Language Model as Principled Decision Maker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21411v1"&gt;Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21451v1"&gt;Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21458v1"&gt;Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21467v1"&gt;Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21479v1"&gt;Are Language Models Consequentialist or Deontological Moral Reasoners?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20630v1"&gt;SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20715v1"&gt;MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20959v1"&gt;Research Community Perspectives on "Intelligence" and Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20966v1"&gt;Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21043v1"&gt;Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21068v1"&gt;Predicting Implicit Arguments in Procedural Video Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21439v1"&gt;Towards Better Instruction Following Retrieval Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21465v1"&gt;ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21472v1"&gt;Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20899v1"&gt;Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.20981v1"&gt;RefAV: Towards Planning-Centric Scenario Mining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21137v1"&gt;Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21138v1"&gt;Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21148v1"&gt;Assessment of L2 Oral Proficiency using Speech Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.21162v1"&gt;Leveraging GANs for citation intent classification and its impact on citation network analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-28</guid><pubDate>Wed, 28 May 2025 12:16:26 +0900</pubDate></item><item><title>Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2505.21471v1</link><description>推論と情報探索のためのポストトレーニング技術の急速な進歩により、大規模言語モデル（LLM）は、大量の検索された知識を取り込んで複雑なタスクを解決できるようになりました。しかし、LLMの限られたコンテキストウィンドウは、外部知識の入力量を拡大することを妨げ、特に大量の外部知識を必要とするタスクにおいて、さらなる改善を阻害しています。既存のコンテキストウィンドウ拡張手法は、必然的に情報損失を引き起こします。LLMベースのマルチエージェント手法は、分散的な方法で大量の入力を処理するための新しいパラダイムとして登場しており、既存の知識同期および推論プロセスにおける2つの主要なボトルネックを特定しました。本研究では、これらのボトルネックを克服し、より長いコンテキストでのトレーニングなしに、推論時の知識統合におけるより優れたスケーラビリティを可能にするマルチエージェントフレームワーク$\textbf{ExtAgents}$を開発します。強化されたマルチホップ質問応答テストである$\textbf{$\boldsymbol{\infty}$Bench+}$、および長文サーベイ生成を含む他の公開テストセットでベンチマークした結果、ExtAgentsは、$\textit{コンテキストウィンドウの内外}$にかかわらず、同じ量の外部知識入力で、既存の非トレーニング手法よりも大幅にパフォーマンスを向上させます。さらに、この手法は高い並列性により、高い効率を維持します。外部知識入力の増加に関するLLMエージェントの連携に関するさらなる研究は、現実世界のアプリケーションに役立つ可能性があります。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21471v1</guid><pubDate>Tue, 27 May 2025 17:45:04 +0000</pubDate></item><item><title>AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs</title><link>http://arxiv.org/abs/2505.21389v1</link><description>マルチモーダル大規模言語モデル（MLLM）の評価は、ベンチマークの規模拡大と異種モーダル間の複雑さが増すにつれて、スコアリングに多大な労力を要するため、ますます高価になっています。この困難に対処するため、我々は、この高騰するコストに対処する、MLLMの効率的かつ適応的なベンチマークのためのエージェント駆動型フレームワークであるAutoJudgerを導入します。AutoJudgerは、項目反応理論（IRT）を用いて質問の難易度を推定し、自律的な評価エージェントを用いて、モデルのリアルタイムのパフォーマンスに基づいて最も有益なテスト質問を動的に選択します。具体的には、AutoJudgerは2つの重要なコンポーネントを組み込んでいます。1つは、選択された質問が視覚と言語の両方のモダリティにわたって多様で困難なシナリオを確実にカバーするための意味認識型検索メカニズム、もう1つは、評価プロセス全体を通して一貫性があり、グローバルな情報に基づいた質問選択を導くために、以前に評価された質問の文脈統計を維持する動的メモリです。4つの代表的なマルチモーダルベンチマークに関する広範な実験により、我々のアダプティブフレームワークが評価コストを劇的に削減することが実証されています。すなわち、AutoJudgerはMMT-Benchでの完全なベンチマーク評価で90%以上のランキング精度を達成するために、わずか4%のデータを使用します。

&lt;img src="https://arxiv.org/html/2505.21389v1/x1.png"/&gt;&lt;p&gt;Chengjun Pan, China, China University of Southern California, Fudan University, Jiwen Zhang, Los Angeles, Shanghai, USA Shanghai Innovation Institute, Xuanwen Ding, Zejun Li&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21389v1</guid><pubDate>Tue, 27 May 2025 16:17:15 +0000</pubDate></item><item><title>rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset</title><link>http://arxiv.org/abs/2505.21297v1</link><description>大規模言語モデル（LLM）におけるコード推論の進歩は、本質的に高難易度データセットの不足、特に大規模なソリューション検証に必要な検証可能な入出力テストケースを持つデータセットの不足によって制限されています。我々は、418Kの競技レベルのコード問題、580Kの長文推論ソリューション、および様々な難易度の豊富なテストケースからなる大規模な検証済みデータセットを構築することで、LLMのコード推論能力を大幅に向上させるrStar-Coderを紹介します。これは、以下の3つの主要な貢献によって達成されます。（1）新しい解決可能な問題を合成するために、競技プログラミングのコード問題と正解ソリューションを厳選します。（2）効果的な出力ラベリングのために、生成を3段階の入力生成法と相互検証メカニズムに分離する、信頼性の高い入出力テストケース合成パイプラインを導入します。（3）高品質でテストケース検証済みの長文推論ソリューションで問題を拡張します。様々なコード推論ベンチマークにおけるQwenモデル（1.5B-14B）に関する広範な実験は、rStar-Coderデータセットの優位性を示しており、はるかに小さいモデルサイズで最先端の推論LLMに匹敵する優れたパフォーマンスを達成しています。LiveCodeBenchでは、rStar-CoderはQwen2.5-7Bを17.4%から57.3%という驚異的な数値に、Qwen2.5-14Bを23.3%から62.5%に向上させ、o3-mini（low）を3.1%上回っています。より困難なUSA Computing Olympiadでは、当社の7Bモデルは平均pass@1精度16.15%を達成し、最先端レベルのQWQ-32Bを上回っています。コードとデータセットはhttps://github.com/microsoft/rStarで公開されます。

&lt;img src="https://arxiv.org/html/2505.21297v1/extracted/6483672/teaser.png"/&gt;&lt;p&gt;Yifei Liu Li Lyna Zhang Yi Zhu Microsoft Research Asia Dalian University of Technology Shanghai Jiao Tong University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21297v1</guid><pubDate>Tue, 27 May 2025 15:00:57 +0000</pubDate></item><item><title>CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature</title><link>http://arxiv.org/abs/2505.20779v1</link><description>人間の革新の特徴は、既存のメカニズムや概念の要素を統合して独創的なアイデアを生み出す、組み換えのプロセスです。本研究では、科学文献を自動的にマイニングし、組み換えの事例に関する大規模な知識ベース（KB）であるCHIMERAを構築します。CHIMERAは、科学者がどのように概念を組み換え、異なる分野からインスピレーションを得ているかを大規模に実証的に探求したり、新しい創造的な異分野融合の方向性を予測することを学習する教師あり機械学習モデルを訓練したりするために使用できます。このKBを構築するために、科学論文のアブストラクトから組み換えを抽出するという新しい情報抽出タスクを提示し、手作業でアノテーションされた数百のアブストラクトからなる高品質なコーパスを収集し、それを使用してLLMベースの抽出モデルを訓練します。このモデルをAI分野の論文の大規模なコーパスに適用し、28,000件以上の組み換え事例からなるKBを生成します。CHIMERAを分析して、AIのさまざまなサブエリアにおける組み換えの特性を探求します。最後に、KBを使用して科学的な仮説生成モデルを訓練し、現実の研究者が刺激を受ける新しい組み換えの方向性を予測します。データとコードは、https://github.cs.huji.ac.il/tomhope-lab/CHIMERA で入手できます。

&lt;img src="https://arxiv.org/html/2505.20779v1/x1.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;School of Computer Science and Engineering, The Hebrew University of Jerusalem
The Allen Institute for AI (AI2)&lt;/p&gt;</description><guid isPermaLink="false">2505.20779v1</guid><pubDate>Tue, 27 May 2025 06:36:04 +0000</pubDate></item><item><title>Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling</title><link>http://arxiv.org/abs/2505.21452v1</link><description>環状ペプチドは医薬品において固有の利点を提供する。例えば、環状ペプチドは線状ペプチドと比較して酵素的加水分解に対する耐性が高く、通常、優れた安定性と親和性を示す。深層生成モデルは線状ペプチドの設計において大きな成功を収めているが、いくつかの課題が多様な種類の環状ペプチドを設計するための計算手法の開発を妨げている。これらの課題には、標的タンパク質と関連する環状ペプチドリガンドの3D構造データの不足、環化が課す幾何学的制約、および環化における非標準アミノ酸の関与が含まれる。上記の課題に対処するために、我々はCpSDEを導入する。これは、調和SDEに基づく生成構造予測モデルであるAtomSDEと、残基タイプ予測器であるResRouterという2つの主要なコンポーネントで構成されている。これらの2つのモデルを交互に使用して配列と構造を反復的に更新するルーティングされたサンプリングアルゴリズムを利用することで、CpSDEは環状ペプチドの生成を促進する。明示的な全原子および結合モデリングを採用することにより、CpSDEは既存のデータ制限を克服し、多種多様な環状ペプチドの設計に熟達している。我々の実験結果は、我々の手法によって設計された環状ペプチドが信頼できる安定性と親和性を示すことを実証している。

&lt;img src="https://arxiv.org/html/2505.21452v1/x5.png"/&gt;&lt;p&gt;Dongyu Xue, Jiahan Li, Jianzhu Ma, Mingyu Li, Quanquan Gu, Xiangxin Zhou, Yi Xiao, Zaixiang Zheng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21452v1</guid><pubDate>Tue, 27 May 2025 17:24:12 +0000</pubDate></item><item><title>Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2505.21404v1</link><description>自然勾配法は、物理情報ニューラルネットワーク（PINN）の学習を著しく加速するが、そのガウス・ニュートン更新はパラメータ空間で解く必要があり、ネットワークの学習可能な重みの数 $n$ に対して、計算量が $O(n^3)$ となり、非常に高コストになる。我々は、全く同じステップが、一般的に小さい残差空間（サイズ $m = \sum_{\gamma} N_{\gamma} d_{\gamma}$）で定式化できることを示す。ここで、各残差クラス $\gamma$ （例えば、PDE内部、境界、初期データ）は、$N_{\gamma}$ 個の配置点と出力次元 $d_{\gamma}$ を持つ。

この洞察に基づき、我々は\textit{Dual Natural Gradient Descent} (D-NGD) を導入する。D-NGD は、残差空間でガウス・ニュートンステップを計算し、それを無視できるほどの追加コストで測地線加速度補正で拡張し、適度な $m$ に対しては密な直接ソルバー、より大きな $m$ に対しては Nystrom 前処理共役勾配ソルバーを提供する。

実験的に、D-NGD は、2 次の PINN 最適化を最大 1280 万個のパラメータを持つネットワークにスケールさせ、1 次法（Adam、SGD）および準ニュートン法よりも 1 桁から 3 桁低い最終誤差 $L^2$ を実現し、そして何よりも重要なことに、このスケールでの PINN の自然勾配学習を単一の GPU で可能にする。

&lt;img src="https://arxiv.org/html/2505.21404v1/extracted/6485346/figures/training_l2_error_final_poisson.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21404v1</guid><pubDate>Tue, 27 May 2025 16:27:23 +0000</pubDate></item><item><title>LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms</title><link>http://arxiv.org/abs/2505.21034v1</link><description>ベイズ最適化（BO）は、コストのかかるブラックボックス関数を最適化するための強力なアルゴリズム群ですが、効果的なBOアルゴリズムの設計は依然として、専門知識に基づいた手作業のタスクです。大規模言語モデル（LLM）の最近の進歩は、最適化アルゴリズムの自動設計を含む、科学的発見を自動化するための新たな道を開きました。先行研究では、最適化ループ内でLLMを使用したり、BO以外のアルゴリズムを生成したりしていましたが、私たちは新たな課題に取り組みます。それは、LLMを使用して、完全なBOアルゴリズムのコードを自動的に生成することです。私たちのフレームワークは、進化戦略を使用して、BOアルゴリズムの主要な構成要素（初期設計、サロゲートモデル、獲得関数）を保持するPythonコードをLLMに生成させます。LLMは、複数の候補アルゴリズムを生成するように促され、それらはCOmparing Continuous Optimizers（COCO）プラットフォームの確立されたBlack-Box Optimization Benchmarking（BBOB）テストスイートで評価されます。そのパフォーマンスに基づいて、上位の候補が選択され、制御されたプロンプトのバリエーションを介して結合および変異され、反復的な改良が可能になります。追加のファインチューニングなしにもかかわらず、LLMによって生成されたアルゴリズムは、次元5の19個（24個中）のBBOB関数で最先端のBOベースラインを上回り、より高い次元や、（Bayesmarkフレームワークからの）異なるタスクにもうまく一般化されます。この研究は、LLMがアルゴリズムの共同設計者として機能し、BO開発を自動化し、新しいアルゴリズムの組み合わせの発見を加速するための新しいパラダイムを提供できることを示しています。ソースコードはhttps://github.com/Ewendawi/LLaMEA-BOで提供されています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21034v1</guid><pubDate>Tue, 27 May 2025 11:13:14 +0000</pubDate></item><item><title>Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models</title><link>http://arxiv.org/abs/2505.21005v1</link><description>スコアベース拡散モデル（SBDM）は、ボルツマン分布に対する強力な償却サンプラーですが、不完全なスコア推定は、その後のモンテカルロ推定にバイアスをもたらします。古典的な重点サンプリング（IS）はこのバイアスを修正できますが、正確な尤度を計算するには、確率フロー常微分方程式（PF-ODE）を解く必要があり、これは非常にコストがかかり、次元とともにスケールしません。本稿では、Variance-Tuned Diffusion Importance Sampling（VT-DIS）という、事前学習済みSBDMのステップごとのノイズ共分散を、その順方向拡散と逆方向ノイズ除去軌跡間の$\alpha$-ダイバージェンス（$\alpha=2$）を最小化することで適応させる、軽量な事後学習手法を提案します。VT-DISは、順方向-逆方向プロセス全体に対して単一の軌跡ごとの重点重みを割り当てることで、標準的なサンプリングと比較して無視できるオーバーヘッドで、テスト時に不偏な期待値推定を実現します。DW-4、LJ-13、およびアラニンジペプチドのベンチマークにおいて、VT-DISは、それぞれ約80％、35％、および3.5％の有効サンプルサイズを達成し、バニラ拡散+ ISまたはPF-ODEベースのISに必要な計算予算のごく一部しか使用しません。

&lt;img src="https://arxiv.org/html/2505.21005v1/x1.png"/&gt;&lt;p&gt;Fengzhe Zhang University of Cambridge &amp;Laurence I. Midgley Ångström AI University of Cambridge &amp;José Miguel Hernández-Lobato Ångström AI University of Cambridge&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21005v1</guid><pubDate>Tue, 27 May 2025 10:37:48 +0000</pubDate></item><item><title>BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design</title><link>http://arxiv.org/abs/2505.21241v1</link><description>タンパク質結合体の設計は、バックプロパゲーションを通じて構造予測の信頼性指標（例えば、界面予測TMスコア（ipTM））を最適化する、ハルシネーションに基づく手法によって変革されてきました。しかし、これらの指標は、学習された分布における結合体-標的複合体の統計的な尤度を反映しておらず、最適化のための疎な勾配しか生成しません。本研究では、構造予測器の信頼性出力をエネルギーベースモデル（EBM）として再解釈することにより、そのような尤度を構造予測器から抽出する方法を提案します。Joint Energy-based Modeling（JEM）フレームワークを活用することで、予測された残基間誤差分布から導出される統計的なエネルギー関数であるpTMEnergyを導入します。pTMEnergyをBindEnergyCraft（BECraft）に組み込みます。BECraftは、BindCraftと同じ最適化フレームワークを維持しながら、ipTMをエネルギーベースの目的関数に置き換えます。BECraftは、複数の困難な標的にわたってBindCraft、RFDiffusion、ESM3を上回り、構造的な衝突を減らしながら、より高いインシリコ結合体成功率を達成します。さらに、pTMEnergyは、ミニタンパク質およびRNAアプタマー結合体の構造ベースのバーチャルスクリーニングタスクにおいて、新たな最先端技術を確立します。

&lt;img src="https://arxiv.org/html/2505.21241v1/extracted/6484888/figures/method.png"/&gt;&lt;p&gt;Divya Nori &amp;Anisha Parsan Caroline Uhler &amp;Wengong Jin&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21241v1</guid><pubDate>Tue, 27 May 2025 14:21:35 +0000</pubDate></item><item><title>Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers</title><link>http://arxiv.org/abs/2505.21497v1</link><description>学術ポスターの作成は、科学コミュニケーションにおいて重要でありながら困難な課題であり、長文の相互に関連する文書を、視覚的に一貫性のある単一のページに圧縮する必要があります。この課題に対処するため、ポスター作成のための最初のベンチマークとメトリックスイートを導入します。これは、最近の会議論文と著者によってデザインされたポスターを組み合わせ、出力を以下の点で評価します。（i）視覚的品質 - 人間のポスターとの意味的整合性、（ii）テキストの整合性 - 言語の流暢さ、（iii）全体的な評価 - VLM（視覚言語モデル）を審査員として、6つの詳細な美的および情報的基準で評価、（iv）PaperQuiz - 生成されたクイズにVLMが回答することで測定される、ポスターが論文の核心的な内容を伝える能力。このベンチマークに基づいて、トップダウン型の視覚ループ型マルチエージェントパイプラインであるPosterAgentを提案します。（a）Parserは、論文を構造化されたアセットライブラリに抽出します。（b）Plannerは、テキストと視覚のペアを、読み順と空間的なバランスを維持する二分木レイアウトに配置します。（c）Painter-Commenterループは、レンダリングコードを実行し、VLMからのフィードバックを使用してオーバーフローを排除し、整合性を確保することで、各パネルを洗練します。包括的な評価の結果、GPT-4oの出力は、一見視覚的に魅力的であるものの、ノイズの多いテキストや低いPaperQuizスコアを示すことが多く、人間のデザインしたポスターは意味を伝えるために視覚的なセマンティクスに大きく依存しているため、読者のエンゲージメントが主要な美的ボトルネックであることがわかりました。完全にオープンソースのバリアント（例：Qwen-2.5シリーズに基づくもの）は、既存の4o駆動のマルチエージェントシステムをほぼすべてのメトリックで上回り、トークン使用量は87％削減されています。22ページの論文を、最終的でありながら編集可能な.pptx形式のポスターに変換し、費用はわずか0.005ドルです。これらの発見は、次世代の完全自動ポスター作成モデルの明確な方向性を示しています。コードとデータセットは、https://github.com/Paper2Poster/Paper2Poster で入手できます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21497v1</guid><pubDate>Tue, 27 May 2025 17:58:49 +0000</pubDate></item><item><title>Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science</title><link>http://arxiv.org/abs/2505.21396v1</link><description>大規模言語モデル（LLM）における最近の進歩は、斬新な研究アイデアを生み出す上で有望性を示しています。しかし、これらのアイデアはしばしば、実現可能性や期待される有効性に関連する課題に直面します。本論文では、アイデア生成プロセス中にLLMに関連データを提供することで、生成されるアイデアの質をどのように向上させることができるかを探求します。データの組み込み方として、(1)アイデア生成段階でメタデータを提供し、LLMを現実的な方向に導くこと、(2)アイデア内の仮説の経験的な妥当性を評価するために、アイデア選択段階で自動検証を追加すること、の2つを紹介します。社会科学の分野、特に気候変動交渉のトピックで実験を行い、メタデータによって生成されたアイデアの実現可能性が20%向上し、自動検証によって選択されたアイデアの全体的な質が7%向上することを発見しました。人間による研究では、LLMによって生成されたアイデアが、関連データや検証プロセスとともに、研究者がより質の高い研究アイデアを提案するきっかけになることが示されています。私たちの研究は、データ駆動型の研究アイデア生成の可能性を強調し、現実世界の学術環境におけるLLM支援によるアイデア創出の実用的な有用性を強調しています。

&lt;img src="https://arxiv.org/html/2505.21396v1/x1.png"/&gt;&lt;p&gt;Xiao Liu, Xinyang Gao, Xinyi Dong, Xun Pang Wangxuan Institute of Computer Technology Yuanpei College School of Government School of International Studies Institute for Carbon Neutrality Analytics Lab for Global Risk Politics Peking University, Yansong Feng&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21396v1</guid><pubDate>Tue, 27 May 2025 16:23:42 +0000</pubDate></item><item><title>Large Language Models Miss the Multi-Agent Mark</title><link>http://arxiv.org/abs/2505.21298v1</link><description>大規模言語モデルのマルチエージェントシステム（MAS LLM）への関心の高まりにより、複数のLLMを活用して複雑なタスクに取り組むフレームワークが増加しています。しかし、この文献の多くは、MASの基礎原則に関与することなく、MASの用語を流用しています。本ポジションペーパーでは、MAS理論と現在のMAS LLMの実装との間の重要な相違点を、エージェンシーの社会的側面、環境設計、協調および通信プロトコル、そして創発的行動の測定という4つの主要分野に焦点を当てて強調します。私たちの立場は、多くのMAS LLMが自律性、社会的相互作用、構造化された環境などのマルチエージェント特性を欠いており、しばしば単純化されたLLM中心のアーキテクチャに依存しているということです。この分野は、MASの文献がすでに扱ってきた問題を再検討することで、減速し、勢いを失う可能性があります。したがって、私たちはこの問題を体系的に分析し、関連する研究機会を概説します。誤った特徴付けや機会の逸失を避けるために、確立されたMASの概念をより適切に統合し、より正確な用語を使用することを提唱します。

&lt;img src="https://arxiv.org/html/2505.21298v1/extracted/6485263/img/environments-survey.png"/&gt;&lt;p&gt;Emanuele La Malfa Gabriele La Malfa Samuele Marro Department of Computer Science, King’s College London Department of Engineering, University of Oxford Department of Informatics, University of Oxford University of Sussex&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21298v1</guid><pubDate>Tue, 27 May 2025 15:01:06 +0000</pubDate></item><item><title>Creativity in LLM-based Multi-Agent Systems: A Survey</title><link>http://arxiv.org/abs/2505.21116v1</link><description>大規模言語モデル（LLM）駆動型マルチエージェントシステム（MAS）は、人間とAIが共同でアイデアや成果物を生成する方法を大きく変革しています。既存の調査はMASインフラストラクチャの包括的な概要を提供していますが、新規なアウトプットがどのように生成・評価されるか、創造性がエージェントのペルソナにどのように影響を与えるか、創造的なワークフローがどのように調整されるかなど、\emph{創造性}の側面を大きく見落としています。本調査は、MASにおける創造性に特化した初の調査です。テキストおよび画像生成タスクに焦点を当て、以下を提示します。（1）エージェントの積極性とペルソナ設計の分類、（2）発散的探索、反復的洗練、協調的合成を含む生成技術の概要、および関連するデータセットと評価指標、（3）一貫性のない評価基準、不十分なバイアス軽減、調整の衝突、統一されたベンチマークの欠如など、主要な課題に関する考察。本調査は、創造的なMASの開発、評価、標準化を推進するための構造化されたフレームワークとロードマップを提供します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21116v1</guid><pubDate>Tue, 27 May 2025 12:36:14 +0000</pubDate></item><item><title>An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks</title><link>http://arxiv.org/abs/2505.20854v1</link><description>大規模言語モデル（LLM）やその他の自動化技術は、コードスニペット、パッチ、コメントなどのソフトウェア成果物を生成することで、ソフトウェア開発者を支援するためにますます使用されています。しかし、これらの生成された成果物の正確性を正確に評価することは、依然として大きな課題です。一方、人間による評価は高い精度を提供しますが、労働集約的であり、拡張性に欠けます。他方、既存の他の自動評価指標はスケーラブルであり、人間の労力を最小限に抑えますが、生成されたソフトウェア成果物の実際の正確性を正確に反映できないことがよくあります。

本稿では、生成されたソフトウェア成果物の正確性を正確に評価するために特別に設計された、LLM-as-Ensemble-Judgeのための最初の評価指標であるSWE-Judgeを紹介します。SWE-Judgeは、まず5つの異なる評価戦略を定義し、それぞれが独立した審査員として実装されます。次に、動的なチーム選択メカニズムが、アンサンブルを通じて最終的な正確性スコアを生成するための最適な審査員のサブセットを特定します。CoNaLa、Card2Code、HumanEval-X、APPS、APR-Assess、Summary-Assessなど、多様なソフトウェアエンジニアリング（SE）ベンチマークにわたってSWE-Judgeを評価します。これらのベンチマークは、コード生成、自動プログラム修復、コード要約という3つのSEタスクに及びます。実験結果は、SWE-Judgeが既存の自動指標よりも5.9％から183.8％の改善で、人間による判断とのより高い相関を一貫して達成することを示しています。さらに、SWE-Judgeは、コード生成およびプログラム修復タスクにおける注釈者間の合意と同等のレベルで、人間の注釈者との合意に達しています。これらの発見は、SWE-Judgeが人間による評価に代わるスケーラブルで信頼性の高い代替手段としての可能性を強調しています。

&lt;img src="https://arxiv.org/html/2505.20854v1/x1.png"/&gt;&lt;p&gt;David Lo, Guang Yang, Kisub Kim, Luís F. Gomes, Martin Weyssow, Ting Zhang, Xin Zhou&lt;/p&gt;&lt;p&gt;Carnegie Mellon University USA
Independent Researcher Hong Kong
Nanjing University of Aeronautics and Astronautics China
Singapore Management University Singapore&lt;/p&gt;</description><guid isPermaLink="false">2505.20854v1</guid><pubDate>Tue, 27 May 2025 08:04:34 +0000</pubDate></item><item><title>TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research</title><link>http://arxiv.org/abs/2505.20663v1</link><description>テルペノイドは150年以上にわたって研究されてきた重要な天然物の一種ですが、その学際的な性質（化学、薬理学、生物学にまたがる）が知識の統合を複雑にしています。この問題に対処するため、著者らは、20年分のテルペノイド文献から構築されたキュレーションされた知識ベース（KB）であるTeroSeekと、AI搭載の質問応答チャットボットおよびウェブサービスを開発しました。検索拡張生成（RAG）フレームワークを活用することで、TeroSeekは構造化された高品質な情報を提供し、テルペノイド関連のクエリにおいて汎用的な大規模言語モデル（LLM）よりも優れた性能を発揮します。これは、学際的な研究のためのドメイン固有のエキスパートツールとして機能し、http://teroseek.qmclab.com で一般公開されています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20663v1</guid><pubDate>Tue, 27 May 2025 03:17:30 +0000</pubDate></item><item><title>RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation</title><link>http://arxiv.org/abs/2505.21413v1</link><description>ツールは、複雑な問題解決タスクにおいて大規模言語モデル（LLM）の推論能力を向上させるが、すべてのタスクで利用可能なツールがあるわけではない。定義済みのツールがない場合、先行研究ではLLMにツールを独自に生成させることを試みてきた。しかし、そのようなアプローチはモデルの内部知識に大きく依存し、LLMの知識範囲を超える領域では失敗するだろう。この制限に対処するため、我々はRefToolという、教科書のような構造化された外部資料を活用した、参照誘導型の自動ツール作成フレームワークを提案する。RefToolは、(1)ツール作成モジュール（LLMが参照コンテンツから実行可能なツールを生成し、例を用いて検証し、階層的にツールボックスに整理する）と、(2)ツール利用モジュール（LLMがツールボックスの構造をナビゲートして、適切なツールを選択し、問題を解決するために適用する）の2つのモジュールで構成される。因果関係、物理学、化学のベンチマークに関する実験では、RefToolは既存のツール作成およびドメイン固有の推論手法よりも平均で11.3%高い精度を示し、コスト効率が高く、広く一般化可能であることが示された。分析の結果、参照に基づいてツールを作成することで、正確で信頼性の高いツールが生成され、階層構造が効果的なツール選択を促進することが明らかになった。RefToolはLLMが知識の限界を克服することを可能にし、強化された汎用的な推論のために、外部参照に基づいてツールを作成することの価値を実証している。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21413v1</guid><pubDate>Tue, 27 May 2025 16:41:19 +0000</pubDate></item><item><title>Improving LLM-based Global Optimization with Search Space Partitioning</title><link>http://arxiv.org/abs/2505.21372v1</link><description>大規模言語モデル（LLM）は近年、高コストなブラックボックス関数のためのグローバル最適化フレームワークにおいて、効果的な代替モデルおよび候補生成器として登場しました。有望な結果にもかかわらず、LLMベースの手法は、高次元の探索空間や、ドメイン固有の事前知識が不足している場合に苦戦することが多く、疎な、または情報量の少ない提案につながります。これらの制限を克服するために、我々はHOLLMという新しいグローバル最適化アルゴリズムを提案します。これは、探索空間を有望なサブリージョンに分割することで、LLM駆動のサンプリングを強化します。各サブリージョンは「メタアーム」として機能し、バンディットに着想を得たスコアリングメカニズムによって選択され、探索と活用のバランスを効果的に取ります。選択された各サブリージョン内で、LLMは明示的なドメイン知識なしに、高品質な候補点を提案します。標準的な最適化ベンチマークでの実証的な評価では、HOLLMが一貫して主要なベイズ最適化および信頼領域法と同等またはそれ以上の性能を示し、グローバルなLLMベースのサンプリング戦略を大幅に上回ることが示されています。

&lt;img src="https://arxiv.org/html/2505.21372v1/x1.png"/&gt;&lt;p&gt;Andrej Schwanke Lyubomir Ivanov David Salinas University of Freiburg, ELLIS Institute Tübingen, Prior Labs, ScaDS.AI, University of Leipzig&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21372v1</guid><pubDate>Tue, 27 May 2025 16:01:49 +0000</pubDate></item><item><title>BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks</title><link>http://arxiv.org/abs/2505.20997v1</link><description>二値 (0-1) 整数計画法 (BIP) は、離散的な意思決定を必要とする科学分野において極めて重要です。AIコンピューティングの進歩に伴い、近年の研究では整数線形計画法 (ILP) 問題に対するニューラルネットワークベースのソルバーが模索されています。しかし、それらは非線形の課題に取り組むためのスケーラビリティに欠けています。非線形性に対処するため、最先端の Branch-and-Cut ソルバーは線形緩和を利用しますが、補助変数の指数関数的な増加と深刻な計算上の制約につながります。これらの制限を克服するために、我々は BIPNN (Binary Integer Programming Neural Network) を提案します。これは、ハイパーグラフニューラルネットワーク (HyperGNN) を介して非線形 BIP 問題を解決するための教師なし学習フレームワークです。具体的には、BIPNN は、制約付き、離散的、非線形 (sin, log, exp) な最適化問題である BIP を、制約なし、微分可能、多項式損失関数に再構成します。この再構成は、多項式 BIP 目的関数とハイパーグラフ構造との間に正確な一対一対応があるという観察に基づいています。これにより、HyperGNN の教師なし学習が可能になり、BIP 問題をエンドツーエンドで最適化できます。この基盤の上に、GPU で加速され、連続アニーリングで強化された BIPNN のトレーニングパイプラインを提案します。このパイプラインにより、BIPNN は、単純な勾配降下法を通じて BIP 内の大規模な非線形項を完全に並行して最適化できるため、トレーニングコストを大幅に削減しながら、離散的で高品質なソリューションの生成を保証します。合成データセットと実世界のデータセットに関する広範な実験により、我々のアプローチの優位性が強調されています。

&lt;img src="https://arxiv.org/html/2505.20997v1/x1.png"/&gt;&lt;p&gt;China, China &amp;Chunqi Yang Changchun University of Science, China &amp;Zhengang Jiang Changchun University of Science, China Xin Bai Huawei Technologies Co. Ltd China &amp;Xin Zhang Changchun University of Science, Sen Bai Changchun University of Science, Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20997v1</guid><pubDate>Tue, 27 May 2025 10:31:52 +0000</pubDate></item><item><title>Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs</title><link>http://arxiv.org/abs/2505.20972v1</link><description>AIコンピューティングが科学的発見で輝きを放つとともに、近年では組み合わせ最適化（CO）領域におけるその可能性も現れてきました。しかし、既存の教師なしニューラルネットワークソルバーは、計算フレームワークの制約により、大規模なグラフやハイパーグラフにおけるk-グループ化問題（例：彩色、分割）を解くのに苦戦しています。本研究では、教師なし学習に基づくCOフレームワークであるDeep $k$-groupingを提案します。具体的には、以下の貢献をします。グラフおよびハイパーグラフ上のk-グループ化問題をモデル化するための新しいone-hotエンコードされた多項式制約なし二値最適化（OH-PUBO）定式化（例：グラフ/ハイパーグラフ彩色および分割）。大規模なk-グループ化CO問題のためのGPUアクセラレーションされたアルゴリズム。Deep $k$-groupingは、大規模なOH-PUBO目的関数の緩和を微分可能な損失関数として利用し、教師なしで最適化するように学習します。スケーラビリティを確保するために、GPUアクセラレーションされたアルゴリズムを活用してトレーニングパイプラインを統合します。解の離散性を強制しつつ、局所最適解への収束を防ぐための、ジニ係数に基づく連続緩和アニーリング戦略。実験結果は、Deep $k$-groupingが既存のニューラルネットワークソルバーや、SCIPやTabuなどの古典的なヒューリスティクスよりも優れていることを示しています。

&lt;img src="https://arxiv.org/html/2505.20972v1/x1.png"/&gt;&lt;p&gt;China, China &amp;Chunqi Yang Changchun University of Science, China &amp;Zhengang Jiang Changchun University of Science, China Xin Bai Huawei Technologies Co. Ltd China &amp;Xin Zhang Changchun University of Science, Sen Bai Changchun University of Science, Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20972v1</guid><pubDate>Tue, 27 May 2025 10:04:54 +0000</pubDate></item><item><title>Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA</title><link>http://arxiv.org/abs/2505.20971v1</link><description>LLM（大規模言語モデル）は、複雑な推論タスクにおいて目覚ましい能力を発揮していますが、しばしばハルシネーション（幻覚）を起こし、信頼できる事実に基づいた根拠を欠いています。一方、知識グラフ（KG）は構造化された事実知識を提供しますが、LLMのような柔軟な推論能力を持ちません。本論文では、KGQA（知識グラフ質問応答）のために、LLMの推論と知識グラフを体系的に統合する新しいフレームワークであるReason-Align-Respond（RAR）を提案します。我々のアプローチは、人間のような推論チェーンを生成するReasoner（推論器）、これらのチェーンを有効なKGパスにマッピングするAligner（アライナー）、そして最終的な答えを合成するResponser（応答器）という3つの主要なコンポーネントで構成されています。このプロセスを確率モデルとして定式化し、期待値最大化アルゴリズムを用いて最適化します。このアルゴリズムは、推論チェーンと知識パスを反復的に洗練します。複数のベンチマークにおける広範な実験により、RARの有効性が実証され、WebQSPとCWQでそれぞれ93.3%と91.0%のHit@1スコアで最先端の性能を達成しました。人間による評価では、RARがKGパスとよく整合した、高品質で解釈可能な推論チェーンを生成することが確認されています。さらに、RARは強力なゼロショット汎化能力を示し、推論中の計算効率を維持します。

&lt;img src="https://arxiv.org/html/2505.20971v1/x1.png"/&gt;&lt;p&gt;China, Engineering, Fanfan Wang, Nanjing University of Science, Rui Xia School of Computer Science, Technology, Xiangqing Shen&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20971v1</guid><pubDate>Tue, 27 May 2025 10:04:53 +0000</pubDate></item><item><title>Can Agents Fix Agent Issues?</title><link>http://arxiv.org/abs/2505.20749v1</link><description>LLMベースのエージェントシステムは、新たなソフトウェアパラダイムとして登場し、医療、ロボット工学、プログラミングなど、多様な分野で広く採用されています。しかし、これらのシステムの維持には多大な労力が必要であり、バグが発生しやすく、変化する外部要件に対応するために常に進化し続けるためです。したがって、エージェントの問題（バグレポートや機能リクエストなど）を自動的に解決することは、重要かつ困難な課題です。最近のソフトウェアエンジニアリング（SE）エージェント（SWE-agentなど）は、従来のソフトウェアシステムの問題に対処する上で有望な結果を示していますが、従来型ソフトウェアとは大きく異なるエージェントシステムにおいて、現実世界の問題をどれだけ効果的に解決できるかは不明なままです。このギャップを埋めるために、まず201件の現実世界のエージェントの問題を手動で分析し、エージェントの問題の一般的なカテゴリを特定します。次に、500人時を費やして、50件のエージェント問題解決タスク（それぞれ実行可能な環境と失敗をトリガーするテストを含む）で構成される、再現可能なベンチマークであるAGENTISSUE-BENCHを構築します。さらに、AGENTISSUE-BENCHで最先端のSEエージェントを評価し、その効果が限定的であること（解決率はわずか3.33%〜12.67%）を明らかにします。これらの結果は、従来型ソフトウェアと比較して、エージェントシステムの維持における固有の課題を強調しており、エージェントの問題を解決するための高度なSEエージェントを開発するためのさらなる研究の必要性を示しています。データとコードは、https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ で入手できます。

&lt;img src="https://arxiv.org/html/2505.20749v1/extracted/6482784/pic/methodology.png"/&gt;&lt;p&gt;Alfin Wijaya Rahardja Fudan University Junwei Liu Fudan University Weitong Chen Fudan University Zhenpeng Chen Nanyang Technological University Yiling Lou Fudan University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20749v1</guid><pubDate>Tue, 27 May 2025 05:45:03 +0000</pubDate></item><item><title>BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism</title><link>http://arxiv.org/abs/2505.20660v1</link><description>グラフィカルユーザーインターフェース（GUI）エージェントは、GUI環境内での複数のインタラクションを通じてタスクを完了させる優れた能力により、大きな注目を集めています。しかし、既存のエージェントは主に個々のアクションの精度向上に焦点を当てており、エラーを検出し、そこから回復するための効果的なメカニズムを欠いていることがよくあります。これらの欠点に対処するため、タスク完了効率を向上させるバックトラッキングメカニズムを組み込んだ堅牢なフレームワークであるBacktrackAgentを提案します。BacktrackAgentには、エラー検出と回復のためのモジュールとして、検証器、判定器、およびリフレクターのコンポーネントが含まれており、エージェントのパフォーマンスをさらに向上させるために判定報酬も適用します。さらに、アクション実行後の結果ページを考慮した、バックトラッキングメカニズム専用のトレーニングデータセットを開発します。実験結果は、BacktrackAgentがMobile3MおよびAuto-UIベンチマークにおいて、タスク成功率とステップ精度の両方でパフォーマンスの向上を達成したことを示しています。データとコードは、採択され次第公開予定です。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20660v1</guid><pubDate>Tue, 27 May 2025 03:09:06 +0000</pubDate></item><item><title>Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations</title><link>http://arxiv.org/abs/2505.21318v1</link><description>大規模言語モデル（LLM）は、思考の連鎖（CoT）推論を用いることで、数学やコーディングにおいて優れた能力を発揮しますが、創薬や反応工学といった現実世界のタスクにおいて厳密な構造解析が求められる化学分野における体系的な推論能力は、まだ十分に活用されていません。現在のベンチマークは、単純な知識検索に焦点を当てており、分子最適化や反応予測といった複雑なタスクに必要な段階的な推論を無視しています。この問題に対処するため、分子構造の理解と、足し算、削除、置換といった算術的な操作を結びつけ、化学的な問題解決を透明性の高い段階的なワークフローに形式化する推論フレームワークであるChemCoTBenchを導入します。分子変換をモジュール化された「化学操作」として扱うことで、このフレームワークは、数学的な証明の論理を反映しつつ、現実世界の化学的制約に基づいた解決策を提供する、ゆっくりとした思考による推論を可能にします。分子特性の最適化と化学反応予測という、影響力の大きい2つのタスクでモデルを評価します。これらのタスクは、現実世界の課題を反映しつつ、構造化された評価可能性を提供します。注釈付きデータセット、推論分類、およびベースライン評価を提供することで、ChemCoTBenchは、抽象的な推論手法と実用的な化学的発見との間のギャップを埋め、AI主導の科学的イノベーションのためのツールとしてLLMを進歩させるための基盤を確立します。

&lt;img src="https://arxiv.org/html/2505.21318v1/x1.png"/&gt;&lt;p&gt;International Digital Economy Academy, Peking University, Yale University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.21318v1</guid><pubDate>Tue, 27 May 2025 15:15:44 +0000</pubDate></item><item><title>MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization</title><link>http://arxiv.org/abs/2505.20820v1</link><description>大規模言語モデル（LLM）は、外部の化学ツールを集約し、協調的な相互作用を可能にして分子候補を反復的に改良できるため、分子最適化において大きな可能性を秘めています。しかし、この可能性は、特に構造化された推論、解釈可能性、および包括的なツールに基づいた分子最適化の文脈においては、未だ十分に探求されていません。このギャップに対処するため、ツールガイドされた推論と役割特化型LLMエージェントを活用した分子最適化のためのマルチエージェントフレームワークであるMT-Molを導入します。私たちのシステムは、構造記述子、電子およびトポロジー的特徴、フラグメントベースの官能基、分子表現、およびその他の化学的特性という5つの異なるドメインに分類された包括的なRDKitツールを組み込んでいます。各カテゴリは、タスクに関連するツールを抽出し、解釈可能で化学的に根拠のあるフィードバックを可能にする責任を負う専門アナリストエージェントによって管理されます。MT-Molは、アナリストエージェント、分子生成科学者、推論出力検証者、およびレビューエージェント間の相互作用を通じて、ツールに沿った段階的な推論による分子を生成します。その結果、私たちのフレームワークが、PMO-1Kベンチマークにおいて23タスク中17タスクで最先端の性能を示すことを示します。

&lt;img src="https://arxiv.org/html/2505.20820v1/x1.png"/&gt;&lt;p&gt;Hyomin Kim KAIST &amp;Yunhui Jang KAIST &amp;Sungsoo Ahn KAIST&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.20820v1</guid><pubDate>Tue, 27 May 2025 07:27:30 +0000</pubDate></item><item><title>Dynamical Data for More Efficient and Generalizable Learning: A Case Study in Disordered Elastic Networks</title><link>http://arxiv.org/abs/2505.21125v1</link><description>機械学習モデルはしばしば大規模なデータセットを必要とし、訓練分布を超えて汎化することが難しい。これらの制限は、網羅的なデータセットの生成が非現実的であり、目標が訓練ドメイン外の新しい解決策を発見することである科学および工学の分野において、重大な課題となる。本研究では、グラフニューラルネットワークに基づくシミュレータを通じて動的データを使用し、二次元無秩序弾性ネットワークの一軸圧縮の文脈において、効率的なシステム-特性学習と分布外予測を可能にすることを検討する。シミュレータは、少数の訓練例から基礎となる物理的ダイナミクスを学習し、未知のネットワークの時間的進化を正確に再現できることがわかった。特に、シミュレータは、このタスクのために明示的に訓練されていなかったにもかかわらず、ポアソン比やそのひずみ依存性などの創発特性を正確に予測することができる。さらに、システム温度、ひずみ振幅、そして最も重要なことに、訓練範囲を超えるポアソン比の変動に対して良好に汎化する。これらの発見は、機械学習モデルを訓練するために動的データを使用することが、特にデータが不足している環境において、材料および分子設計のためのよりデータ効率的で汎化可能なアプローチをサポートできることを示唆している。

&lt;img src="https://arxiv.org/html/2505.21125v1/extracted/6483999/figures/Fig1_model_architecture.jpg"/&gt;&lt;p&gt;Dan Mendels, Ron Levie, Salman N. Salman, Sergey A. Shteingolts&lt;/p&gt;&lt;p&gt;Faculty of Mathematics, Technion – Israel
Institute of Technology, Haifa 32000, Israel
The Wolfson Department of Chemical Engineering, Technion – Israel
Institute of Technology, Haifa 32000, Israel&lt;/p&gt;</description><guid isPermaLink="false">2505.21125v1</guid><pubDate>Tue, 27 May 2025 12:44:17 +0000</pubDate></item><item><title>Interpretable machine learned predictions of adsorption energies at the metal--oxide interface</title><link>http://arxiv.org/abs/2505.21428v1</link><description>$\mathrm{CO_2}$を有用な化合物に変換することは、大気中の$\mathrm{CO_2}$排出量を貯蔵し再利用するための取り組みの重要な部分です。ここでは、いわゆる逆触媒、つまり金属表面に担持された遷移金属酸化物クラスター上での$\mathrm{CO_2}$水素化に焦点を当てます。このような候補触媒材料の計算スクリーニングの従来のアプローチでは、密度汎関数理論（DFT）に依存して、かなりの計算コストをかけて正確な吸着エネルギーを取得します。本稿では、金属-酸化物界面での吸着エネルギーを取得するための機械学習（ML）加速ワークフローを紹介します。クラスター上の可能な結合部位を列挙し、DFTを使用して、多様な局所吸着質環境を持つこれらのサブセットをサンプリングします。このデータセットを使用して、金属-酸化物界面での吸着を制御する電子的および構造的要因を明らかにする目的で、解釈可能なMLモデルとブラックボックスMLモデルを調査します。さらに、調査されたMLモデルは、元のトレーニングデータセット外の構造における吸着エネルギーの低コスト予測に使用できます。ここで提示するワークフローは、金属-酸化物界面での吸着エネルギーの傾向に関する洞察とともに、活性部位の特定、複雑な触媒材料上での反応のミクロ速度論的モデリングに必要なパラメーターの予測、およびデータ駆動型触媒設計の加速に役立ちます。

&lt;img src="https://arxiv.org/html/2505.21428v1/x1.png"/&gt;&lt;p&gt;Julie de Neergaard Ravn, Luuk H. E. Kempen, Marius Juul Nielsen, Mie Andersen, Raffaele Cheula&lt;/p&gt;&lt;p&gt;Center for Interstellar Catalysis, Department of Physics and Astronomy, Aarhus University, Aarhus C, Denmark&lt;/p&gt;</description><guid isPermaLink="false">2505.21428v1</guid><pubDate>Tue, 27 May 2025 16:57:24 +0000</pubDate></item></channel></rss>