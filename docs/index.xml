<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>今日のarXiv-AI4Science</title><link>https://hommage-ebi.github.io/article-rss-proxy/</link><description>今日のarXiv-AI4Science</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Fri, 23 May 2025 03:18:27 +0000</lastBuildDate><item><title>other arxiv papers 2025-05-23</title><link>https://arxiv.org/2025-05-23</link><description>&lt;ol&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16173v1"&gt;Exact Expansion Formalism for Transport Properties of Heterogeneous Materials Characterized by Arbitrary Continuous Random Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16370v1"&gt;Diffraction Stress Factors Calculated Using a Maximum Entropy Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16382v1"&gt;Electronic and Vibrational Properties of Layered Boron Nitride Polymorphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16537v1"&gt;Kinetically controlling surface atom arrangements in thermally robust, amorphous high-entropy alloy nanoparticles by solvent selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16544v1"&gt;Phonon-limited carrier transport in the Weyl semimetal TaAs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16580v1"&gt;Orbital-resolved anisotropic electron pockets in electron-doped SrTiO3 observed by ARPES&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16870v1"&gt;First-principles study of metal-biphenylene interfaces: structural, electronic, and catalytic properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16926v1"&gt;Photo-induced electronic excitations drive polymerization of carbon monoxide: A first-principles study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16238v1"&gt;Magnetic Charge State Controlled Spin-Wave Dynamics in Nanoscale Three-Dimensional Artificial Spin Ice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16432v1"&gt;High-pressure high-temperature solution growth, structural, and superconducting properties of Fe-substituted MgB2 single crystals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16666v1"&gt;Morphological stability of Au-metal nanosatellites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16779v1"&gt;Phase engineering of MoS$_2$ monolayers: A pathway to enhanced lithium-polysulfide battery performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16871v1"&gt;Improvement of H$_2$O$_2$ electrogeneration using a Vulcan XC72 carbon-based electrocatalyst modified with Ce-doped Nb$_2$O$_5$&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16887v1"&gt;Hydrogen peroxide electrogeneration from O2 electroreduction: a review focusing on carbon electrocatalysts and environmental applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16183v1"&gt;Ultrafast charge-transfer dynamics in Ca$_2$CuO$_2$Cl$_2$ from time-resolved optical reflectivity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16657v1"&gt;Effect of spark plasma sintering on the superconducting properties of Sm-based oxypnictide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16677v1"&gt;Universal estimates for the density of states for aperiodic block subwavelength resonator systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16742v1"&gt;Pressure dependence of the interfacial polarization and negative activation volume for dielectric relaxation in heterogeneous systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16799v1"&gt;Synthesis of Y$_3$Fe$_4$H$_{20}$ as a new prototype structure for ternary superhydrides recoverable at ambient pressure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16427v1"&gt;Lineshapes in Pump-Probe Spectroscopy of Polaritons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16489v1"&gt;Spin-polarized alkali-metal trimers revisited&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16262v1"&gt;Vibrational Quantum States of Methanol&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16273v1"&gt;Symmetries and selection rules in photoelectron chiral dichroism from tailored light&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16597v1"&gt;Silver Electrodeposition from Ag/AgCl Electrodes: Implications for Nanoscience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16296v1"&gt;A finite element solver for a thermodynamically consistent electrolyte model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16393v1"&gt;Bottom-up Analysis of Ro-Vibrational Helical Dichroism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16989v1"&gt;Spin adaptation of the cumulant expansions of reduced density matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16906v1"&gt;Higher order Jacobi method for solving a system of linear equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16992v1"&gt;PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16147v1"&gt;Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16199v1"&gt;Velocity Completion Task and Method for Event-based Player Positional Data in Soccer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16221v1"&gt;LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16225v1"&gt;MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16288v1"&gt;No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16312v1"&gt;EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16448v1"&gt;Internal Bias in Reasoning Models leads to Overthinking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16459v1"&gt;MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16475v1"&gt;ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16507v1"&gt;Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16646v1"&gt;SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16771v1"&gt;Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16781v1"&gt;Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16787v1"&gt;Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16827v1"&gt;GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16877v1"&gt;Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16899v1"&gt;Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16135v1"&gt;Sudoku-Bench: Evaluating creative reasoning with Sudoku variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16172v1"&gt;Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16175v1"&gt;QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16176v1"&gt;Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16181v1"&gt;Understanding Generative AI Capabilities in Everyday Image Editing Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16187v1"&gt;EasyInsert: A Data-Efficient and Generalizable Insertion Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16192v1"&gt;VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16223v1"&gt;MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16234v1"&gt;LIFEBench: Evaluating Length Instruction Following in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16249v1"&gt;Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16259v1"&gt;Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16290v1"&gt;Multimodal Generative AI for Story Point Estimation in Software Development&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16306v1"&gt;Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16314v1"&gt;NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16315v1"&gt;Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16332v1"&gt;Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16335v1"&gt;FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16351v1"&gt;Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16362v1"&gt;Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16368v1"&gt;SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16372v1"&gt;Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16376v1"&gt;DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16377v1"&gt;VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16388v1"&gt;Serious Games: Human-AI Interaction, Evolution, and Coevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16392v1"&gt;Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16416v1"&gt;Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16419v1"&gt;Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16425v1"&gt;$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16429v1"&gt;Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16455v1"&gt;Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16460v1"&gt;University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16466v1"&gt;Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16482v1"&gt;Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16483v1"&gt;Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16491v1"&gt;LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16498v1"&gt;Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16512v1"&gt;Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16516v1"&gt;Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16518v1"&gt;CUB: Benchmarking Context Utilisation Techniques for Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16520v1"&gt;Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16522v1"&gt;Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16540v1"&gt;TextureSAM: Towards a Texture Aware Foundation Model for Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16547v1"&gt;Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16561v1"&gt;Auto-nnU-Net: Towards Automated Medical Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16573v1"&gt;From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16579v1"&gt;Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16581v1"&gt;How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16596v1"&gt;Safe Uncertainty-Aware Learning of Robotic Suturing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16619v1"&gt;Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16630v1"&gt;SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16640v1"&gt;BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16643v1"&gt;From Evaluation to Defense: Advancing Safety in Video Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16647v1"&gt;Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16648v1"&gt;Collaboration among Multiple Large Language Models for Medical Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16660v1"&gt;Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16664v1"&gt;End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16670v1"&gt;BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16679v1"&gt;Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16686v1"&gt;SPaRC: A Spatial Pathfinding Reasoning Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16690v1"&gt;Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16691v1"&gt;EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16694v1"&gt;Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16705v1"&gt;An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16710v1"&gt;Training Long-Context LLMs Efficiently via Chunk-wise Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16722v1"&gt;Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16735v1"&gt;Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16740v1"&gt;Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16752v1"&gt;Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16765v1"&gt;When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16773v1"&gt;Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16785v1"&gt;CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16791v1"&gt;Cohort-Based Active Modality Acquisition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16792v1"&gt;REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16798v1"&gt;SEED: Speaker Embedding Enhancement Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16801v1"&gt;A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16826v1"&gt;KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16836v1"&gt;Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16854v1"&gt;Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16860v1"&gt;GCAL: Adapting Graph Models to Evolving Domain Shifts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16881v1"&gt;CASTILLO: Characterizing Response Length Distributions of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16911v1"&gt;Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16915v1"&gt;DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16941v1"&gt;FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16947v1"&gt;MixAT: Combining Continuous and Discrete Adversarial Training for LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16957v1"&gt;Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16978v1"&gt;HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16998v1"&gt;Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17002v1"&gt;PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17012v1"&gt;SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16130v1"&gt;Scalable Graph Generative Modeling via Substructure Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16136v1"&gt;Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16146v1"&gt;Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16149v1"&gt;When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16186v1"&gt;SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16196v1"&gt;SEM: Enhancing Spatial Understanding for Robust Robot Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16208v1"&gt;Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16210v1"&gt;NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16211v1"&gt;AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16227v1"&gt;Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16256v1"&gt;DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16258v1"&gt;IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16270v1"&gt;Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16278v1"&gt;DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16307v1"&gt;PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16322v1"&gt;AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16325v1"&gt;CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16363v1"&gt;AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16394v1"&gt;Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16400v1"&gt;AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16415v1"&gt;Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16430v1"&gt;AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16452v1"&gt;CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16499v1"&gt;Smaller, Smarter, Closer: The Edge of Collaborative Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16505v1"&gt;Sparse Activation Editing for Reliable Instruction Following in Narratives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16508v1"&gt;Edge-First Language Model Inference: Models, Metrics, and Tradeoffs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16530v1"&gt;DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16567v1"&gt;Finetuning-Activated Backdoors in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16612v1"&gt;Steering Large Language Models for Machine Translation Personalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16637v1"&gt;SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16673v1"&gt;R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16724v1"&gt;Advancing Brainwave Modeling with a Codebook-Based Foundation Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16732v1"&gt;Sequential Monte Carlo for Policy Optimization in Continuous POMDPs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16743v1"&gt;TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16789v1"&gt;Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16813v1"&gt;Dynamic Reservoir Computing with Physical Neuromorphic Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16831v1"&gt;Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16832v1"&gt;From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16834v1"&gt;SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16845v1"&gt;Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16856v1"&gt;Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16875v1"&gt;T2I-ConBench: Text-to-Image Benchmark for Continual Post-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16886v1"&gt;Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16888v1"&gt;CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16927v1"&gt;Latent Principle Discovery for Language Model Self-Improvement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16928v1"&gt;Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16950v1"&gt;Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16965v1"&gt;BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16967v1"&gt;Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16985v1"&gt;Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16994v1"&gt;$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17010v1"&gt;Understanding Prompt Tuning and In-Context Learning via Meta-Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17016v1"&gt;Interactive Post-Training for Vision-Language-Action Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17017v1"&gt;Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17019v1"&gt;Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16195v1"&gt;SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16737v1"&gt;Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16968v1"&gt;CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17004v1"&gt;Guided Diffusion Sampling on Function Spaces with Applications to PDEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17022v1"&gt;GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16932v1"&gt;The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16126v1"&gt;Robust Invariant Representation Learning by Distribution Extrapolation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16159v1"&gt;Why Can Accurate Models Be Learned from Inaccurate Annotations?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16190v1"&gt;Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16217v1"&gt;Reward-Aware Proto-Representations in Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16226v1"&gt;Realistic Evaluation of TabPFN v2 in Open Environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16248v1"&gt;Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16260v1"&gt;Small-to-Large Generalization: Data Influences Models Consistently Across Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16265v1"&gt;Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16284v1"&gt;Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16308v1"&gt;CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16319v1"&gt;FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16333v1"&gt;Understanding Differential Transformer Unchains Pretrained Self-Attentions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16341v1"&gt;A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16386v1"&gt;Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16401v1"&gt;Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16446v1"&gt;Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16493v1"&gt;Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16494v1"&gt;Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16527v1"&gt;Joint Relational Database Generation via Graph-Conditional Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16531v1"&gt;HOFT: Householder Orthogonal Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16549v1"&gt;Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16563v1"&gt;A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16577v1"&gt;Large Language Model-Empowered Interactive Load Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16583v1"&gt;Training on Plausible Counterfactuals Removes Spurious Correlations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16620v1"&gt;CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16636v1"&gt;Multivariate Latent Recalibration for Conditional Normalizing Flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16672v1"&gt;Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16675v1"&gt;On the Out-of-Distribution Generalization of Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16733v1"&gt;Forward-only Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16734v1"&gt;Maximum Total Correlation Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16736v1"&gt;Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16754v1"&gt;PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16755v1"&gt;Multi-Output Gaussian Processes for Graph-Structured Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16786v1"&gt;FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16833v1"&gt;Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16857v1"&gt;Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16872v1"&gt;A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16903v1"&gt;Unsupervised Prompting for Graph Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16918v1"&gt;Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16925v1"&gt;Risk-Averse Reinforcement Learning with Itakura-Saito Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16936v1"&gt;SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16952v1"&gt;A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16996v1"&gt;A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16131v1"&gt;Machine Learning the 6d Supergravity Landscape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16134v1"&gt;Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16138v1"&gt;Multimodal Online Federated Learning with Modality Missing in Internet of Things&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16145v1"&gt;Exponential Convergence of CAVI for Bayesian PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16148v1"&gt;NAN: A Training-Free Solution to Coefficient Estimation in Model Merging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16156v1"&gt;Integral Imprecise Probability Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16215v1"&gt;A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16251v1"&gt;Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16257v1"&gt;Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16291v1"&gt;Fairness under Competition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16305v1"&gt;Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16313v1"&gt;Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16320v1"&gt;Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16329v1"&gt;Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16360v1"&gt;Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16381v1"&gt;PaTH Attention: Position Encoding via Accumulating Householder Transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16411v1"&gt;Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16441v1"&gt;Ranked Entropy Minimization for Continual Test-Time Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16463v1"&gt;AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16471v1"&gt;Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16481v1"&gt;Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16524v1"&gt;CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16548v1"&gt;Incremental Sequence Classification with Temporal Consistency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16594v1"&gt;Temporal Object Captioning for Street Scene Videos from LiDAR Tracks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16635v1"&gt;WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16649v1"&gt;Stochastic Forward-Forward Learning through Representational Dimensionality Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16652v1"&gt;Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16714v1"&gt;Experimental robustness benchmark of quantum neural network on a superconducting quantum processor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16723v1"&gt;Robust LLM Fingerprinting via Domain-Specific Watermarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16725v1"&gt;Masked Conditioning for Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16748v1"&gt;Revenue Optimization with Price-Sensitive and Interdependent Demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16879v1"&gt;How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16893v1"&gt;Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16900v1"&gt;Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16923v1"&gt;TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16942v1"&gt;Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16946v1"&gt;NY Real Estate Racial Equity Analysis via Applied Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16953v1"&gt;ICYM2I: The illusion of multimodal informativeness under missingness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16959v1"&gt;Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16993v1"&gt;Native Segmentation Vision Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17013v1"&gt;When Are Concepts Erased From Diffusion Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16204v1"&gt;Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16242v1"&gt;Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16244v1"&gt;Generalized Power Priors for Improved Bayesian Inference with Historical Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16263v1"&gt;All You Need is "Leet": Evading Hate-speech Detection AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16347v1"&gt;Graph Attention Network for Optimal User Association in Wireless Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16353v1"&gt;Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16403v1"&gt;Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16638v1"&gt;Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16680v1"&gt;Learning Genomic Structure from $k$-mers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16713v1"&gt;Sharp concentration of uniform generalization errors in binary linear classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16741v1"&gt;Meta-reinforcement learning with minimum attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16821v1"&gt;LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16829v1"&gt;Contextual Learning for Stochastic Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16850v1"&gt;ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16933v1"&gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17000v1"&gt;Critical Points of Random Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17003v1"&gt;Sufficient conditions for offline reactivation in recurrent neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16716v1"&gt;The Computational Complexity of Counting Linear Regions in ReLU Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16125v1"&gt;KoBALT: Korean Benchmark For Advanced Linguistic Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16128v1"&gt;Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16129v1"&gt;LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16142v1"&gt;Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16160v1"&gt;EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16162v1"&gt;KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16164v1"&gt;Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16170v1"&gt;When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16178v1"&gt;Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16188v1"&gt;SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16189v1"&gt;The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16216v1"&gt;Memorization or Reasoning? Exploring the Idiom Understanding of LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16241v1"&gt;Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16245v1"&gt;Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16252v1"&gt;Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16277v1"&gt;Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16281v1"&gt;HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16297v1"&gt;ToDi: Token-wise Distillation via Fine-Grained Divergence Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16303v1"&gt;INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16348v1"&gt;Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16385v1"&gt;Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16408v1"&gt;From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16418v1"&gt;Exploring the Relationship Between Diversity and Quality in Ad Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16467v1"&gt;Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16514v1"&gt;AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16526v1"&gt;EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16538v1"&gt;Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16552v1"&gt;Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16570v1"&gt;URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16576v1"&gt;EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16591v1"&gt;Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16610v1"&gt;From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16703v1"&gt;Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16774v1"&gt;IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16800v1"&gt;Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16814v1"&gt;Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16838v1"&gt;R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16847v1"&gt;Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16855v1"&gt;Nested Named Entity Recognition as Single-Pass Sequence Labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16868v1"&gt;Comparative analysis of subword tokenization approaches for Indian languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16869v1"&gt;MPO: Multilingual Safety Alignment via Reward Gap Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16894v1"&gt;Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16922v1"&gt;UNCLE: Uncertainty Expressions in Long-Form Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16931v1"&gt;PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16934v1"&gt;In-Context Watermarks for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16956v1"&gt;On Multilingual Encoder Language Model Compression for Low-Resource Languages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16973v1"&gt;VeriFastScore: Speeding up long-form factuality evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16983v1"&gt;LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16995v1"&gt;DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16180v1"&gt;Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16193v1"&gt;An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16212v1"&gt;Large Language Models based ASR Error Correction for Child Conversations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16220v1"&gt;Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16222v1"&gt;Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16406v1"&gt;On the reliability of feature attribution methods for speech classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16559v1"&gt;CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16592v1"&gt;What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16624v1"&gt;Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16631v1"&gt;MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16806v1"&gt;Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16964v1"&gt;MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.17015v1"&gt;Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16470v1"&gt;Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2505.16972v1"&gt;From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><guid isPermaLink="false">other-papers-2025-05-23</guid><pubDate>Fri, 23 May 2025 12:14:28 +0900</pubDate></item><item><title>SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development</title><link>http://arxiv.org/abs/2505.16975v1</link><description>大規模言語モデル（LLM）は、コード補完、バグ修正、ドキュメント生成など、多様なソフトウェアエンジニアリングタスクにおいて強力な能力を示しています。しかし、大規模で既存のコードベースに対して新しい機能を開発する、非常に普及している現実世界のタスクである機能駆動開発（FDD）は、未だ十分に探求されていません。そこで、現実世界の機能開発タスクにおいて自律的なコーディングシステムを評価および訓練するために設計された、初の大規模データセット（14,000のトレーニングサンプルと500のテストサンプル）であるSWE-Devを紹介します。検証可能で多様なトレーニングを保証するために、SWE-Devは、実行可能な環境と開発者が作成した実行可能なユニットテストをすべてのインスタンスに独自に提供します。このコレクションは、教師ありファインチューニング（SFT）のための高品質なデータを提供するだけでなく、実行可能なユニットテストからの正確な報酬信号を提供することにより、強化学習（RL）も可能にします。SWE-Devに対する広範な評価では、17のチャットボットLLM、10の推論モデル、および10のマルチエージェントシステム（MAS）を対象としており、FDDが現在のAIにとって非常に困難なフロンティアであることが明らかになっています（例：Claude-3.7-Sonnetは、難しいテスト分割でわずか22.45％のPass@3を達成）。重要なことに、SWE-Devがモデル改善のための効果的なプラットフォームとして機能することを実証します。トレーニングセットでのファインチューニングにより、\textit{難しい}分割でGPT-4oに匹敵する7Bモデルが実現し、その高品質なトレーニングデータの価値を強調しています。コードはこちらで入手できます：\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}。

&lt;img src="https://arxiv.org/html/2505.16975v1/x1.png"/&gt;&lt;p&gt;Astronautics, Beijing University of Aeronautics, Cheng Wang, S Shanghai Jiao Tong University, Soochow University, Tiktok, University of Michigan, Xianghe Pang, Yaxin Du, Yifan Zhou, Yu Qian, Yuzhu Cai&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16975v1</guid><pubDate>Thu, 22 May 2025 17:51:49 +0000</pubDate></item><item><title>Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2505.16782v1</link><description>大規模言語モデル（LLM）は、思考の連鎖（CoT）プロンプトによって、複雑な推論タスクにおいて目覚ましい性能を達成してきました。しかし、従来のCoTは、自然言語で明示的に表現された推論ステップに依存しており、非効率性をもたらし、抽象的な推論への適用を制限しています。これに対処するため、潜在空間内で推論が行われる潜在CoT推論への研究関心が高まっています。言語から推論を切り離すことで、潜在推論はより豊かな認知表現と、より柔軟で高速な推論を約束します。研究者たちは、この有望な分野において、トレーニング方法論、構造的革新、内部推論メカニズムなど、さまざまな方向性を探求してきました。本論文では、この推論パラダイムの包括的な概要と分析を提供します。まず、トークン単位の戦略、内部メカニズム、分析、およびアプリケーションという4つの視点から統一された分類法を提案します。次に、代表的な手法について、その設計パターン、強み、および未解決の課題を強調しながら、詳細な議論と比較分析を行います。LLM推論におけるこの新たな方向性を発展させるための構造化された基盤を提供することを目指しています。関連論文は、https://github.com/EIT-NLP/Awesome-Latent-CoT で定期的に更新されます。

&lt;img src="https://arxiv.org/html/2505.16782v1/x1.png"/&gt;&lt;p&gt;Anhao Zhao, China, Department of Computing, Eastern Institute of Technology, Heming Xia, Ningbo, The Hong Kong Polytechnic University Ningbo Digital Twin Institute, Xinghao Chen, Xuan Lu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16782v1</guid><pubDate>Thu, 22 May 2025 15:26:51 +0000</pubDate></item><item><title>A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP</title><link>http://arxiv.org/abs/2505.16661v1</link><description>我々は、20億の日本語医薬トークンと80億の英語生物医学トークンを用いた継続的な事前学習によって開発された、医薬分野向けの日本語ドメイン特化型言語モデルを発表します。厳密な評価を可能にするため、薬剤師国家試験に基づくYakugakuQA、異言語の同義語および用語の正規化をテストするNayoseQA、そして対となる文間の整合性推論を評価するために設計された新しいタスクであるSogoCheckという、3つの新しいベンチマークを導入します。我々は、オープンソースの医療LLMと、GPT-4oを含む商用モデルに対して、我々のモデルを評価します。結果は、我々のドメイン特化型モデルが既存のオープンモデルを上回り、特に用語が多く知識に基づいたタスクにおいて、商用モデルと競争力のある性能を達成することを示しています。興味深いことに、GPT-4oでさえSogoCheckでは低い性能を示しており、文間の整合性推論が依然として未解決の課題であることを示唆しています。我々のベンチマークスイートは、事実の想起、語彙のバリエーション、論理的な整合性を網羅し、医薬NLPのためのより広範な診断レンズを提供します。本研究は、日本のドメイン特化型アプリケーション向けの、実用的で安全かつ費用対効果の高い言語モデルを構築する可能性を示し、医薬およびヘルスケアNLPにおける将来の研究のための再利用可能な評価リソースを提供します。我々のモデル、コード、およびデータセットは、https://github.com/EQUES-Inc/pharma-LLM-eval で公開されています。

&lt;img src="https://arxiv.org/html/2505.16661v1/extracted/6461158/overview.png"/&gt;&lt;p&gt;EQUES Inc., The University of Tokyo, University of Tsukuba, issey.sukeda@eques.co.jp&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16661v1</guid><pubDate>Thu, 22 May 2025 13:27:37 +0000</pubDate></item><item><title>ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts</title><link>http://arxiv.org/abs/2505.16566v1</link><description>大規模言語モデル（LLM）のドメイン固有の知識を評価するための従来のベンチマークは、複雑な学術タスクを処理するためのスケーラビリティに欠けています。この問題に対処するため、我々は\texttt{ScholarBench}を導入します。これは、深い専門知識と複雑な学術問題解決を中心としたベンチマークであり、LLMの学術的な推論能力を評価し、3段階のプロセスを経て構築されます。\texttt{ScholarBench}は、学術文献から派生した、より専門的で論理的に複雑なコンテキストを対象とし、5つの異なる問題タイプを包含します。従来のベンチマークとは異なり、\texttt{ScholarBench}は、8つの異なる研究分野にわたるLLMの抽象化、理解、および推論能力を評価します。高品質な評価データを確保するために、カテゴリ固有の例の属性を定義し、各ドメインの特徴的な研究方法論および談話構造に沿った質問を設計します。さらに、このベンチマークは英語-韓国語のバイリンガルデータセットとして機能し、両言語におけるLLMの言語能力の同時評価を促進します。ベンチマークは韓国語で5,031個、英語で5,309個の例で構成されており、o3-miniのような最先端モデルでさえ、平均評価スコアがわずか0.543にとどまり、このベンチマークの難易度を示しています。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16566v1</guid><pubDate>Thu, 22 May 2025 11:59:06 +0000</pubDate></item><item><title>Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization</title><link>http://arxiv.org/abs/2505.16349v1</link><description>科学論文の指数関数的な増加により、研究者が最新情報を把握し、効果的に知識を統合することがますます困難になっています。本論文では、Retrieval-Augmented Generation（RAG）を用いた科学分野における複数文書要約（MDS）のためのモジュール式パイプラインであるXSumを紹介します。このパイプラインは、質問生成モジュールとエディターモジュールの2つの主要なコンポーネントで構成されています。質問生成モジュールは、入力論文に適応した質問を動的に生成し、関連性の高い正確な情報の検索を保証します。エディターモジュールは、検索されたコンテンツを、適切な引用のための学術基準に準拠した、首尾一貫した構造化された要約に統合します。SurveySumデータセットで評価した結果、XSumは優れた性能を示し、既存の手法と比較して、CheckEval、G-Eval、Ref-F1などの指標で大幅な改善を達成しました。本研究は、幅広い分野での応用が期待できる、透明性が高く適応可能な科学的要約のためのフレームワークを提供します。コードはhttps://github.com/webis-de/scolia25-xsumで入手できます。

&lt;img src="https://arxiv.org/html/2505.16349v1/x1.png"/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Bauhaus-Universität Weimar
Kassel University, hessian.AI, ScaDS.AI
Leipzig University, Fraunhofer ISI Leipzig&lt;/p&gt;</description><guid isPermaLink="false">2505.16349v1</guid><pubDate>Thu, 22 May 2025 08:00:59 +0000</pubDate></item><item><title>Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA</title><link>http://arxiv.org/abs/2505.16293v1</link><description>マルチホップ質問応答のための反復型RAGは、長文のコンテキストや無関係な情報の蓄積という課題に直面します。これは、モデルが検索されたコンテンツを処理し、推論する能力を妨げ、パフォーマンスを制限します。最近の手法は、検索された情報の圧縮に焦点を当てていますが、単一ラウンドのRAGに限定されたり、ファインチューニングが必要だったり、反復型RAGにおけるスケーラビリティが不足していたりします。これらの課題に対処するために、私たちはNotes Writingという手法を提案します。これは、各ステップで検索されたドキュメントから簡潔で関連性の高いノートを生成し、それによってノイズを減らし、本質的な情報のみを保持します。これにより、大規模言語モデル（LLM）の実効的なコンテキスト長が間接的に増加し、より大量の入力テキストを処理しながら、より効果的に推論および計画を立てることが可能になります。Notes Writingはフレームワークに依存せず、さまざまな反復型RAG手法と統合できます。2つのモデルと4つの評価データセットにわたって、3つの反復型RAG手法でその有効性を示します。Notes Writingは、出力トークンの最小限の増加で、全体で平均15.6パーセントポイントの改善をもたらします。

&lt;img src="https://arxiv.org/html/2505.16293v1/x1.png"/&gt;&lt;p&gt;Rishabh Maheshwary Masoud Hashemi Khyati Mahajan ServiceNow ServiceNow Research @servicenow.com&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16293v1</guid><pubDate>Thu, 22 May 2025 06:45:05 +0000</pubDate></item><item><title>Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.16237v1</link><description>大規模言語モデル（LLM）は目覚ましい能力を示していますが、依然としてハルシネーションや古い情報といった問題に苦慮しています。検索拡張生成（RAG）は、情報検索（IR）システムを用いてLLMの出力を外部知識に根拠づけることで、これらの問題に対処します。この基盤の上に構築されたグラフベースのRAGシステムは、知識エンティティ間の関係を保持し、より包括的なコンテキストを提供するサブグラフを検索することで、さらに一歩進んでいます。しかし、グラフRAGは2つの課題に直面しています。（1）関連情報を検索する際に、無関係なノードが導入され（特に密なグラフデータベースでは、検索が通常隣接ノードにまで及ぶため）、効率を阻害する過度に長い入力につながります。（2）LLMによる生成時に、グラフと言語の表現のギャップが、グラフ構造を最大限に活用して理解を深める能力を制限します。これらの制限に対処するために、我々はAlign-GRAGという、検索後の段階における新しい推論誘導型二重アライメントフレームワークを提案します。これはまず、ノードとエッジを検索してサブグラフを構築します。次に、グラフエンコーダをLLMで要約された推論と共同で最適化するアライナーを提案します。これは、KLダイバージェンス損失とコントラスト損失を活用して、グラフノードと表現の二重アライメントを実現し、無関係な知識の効率的なプルーニングを促進し、統一された意味空間を確立します。ジェネレーターは、アライメントされたグラフデータをLLMと統合して、一貫性のある正確な回答を生成します。常識推論、シーングラフ理解、知識グラフ推論を含む3つのタスクにわたるGraphQAベンチマークでの実験により、我々の手法の有効性が検証されました。コードは受理され次第公開予定です。

&lt;img src="https://arxiv.org/html/2505.16237v1/x1.png"/&gt;&lt;p&gt;City University of Hong Kong, Derong Xu, Huawei Noah’s Ark Lab, Maolin Wang, Pengyue Jia, Qidong Liu, Technology of China, University of Science, Xiaopeng Li, Yingyi Zhang&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16237v1</guid><pubDate>Thu, 22 May 2025 05:15:27 +0000</pubDate></item><item><title>MuseRAG: Idea Originality Scoring At Scale</title><link>http://arxiv.org/abs/2505.16232v1</link><description>創造的なアイデアの独創性を客観的かつ表面妥当性のある方法で評価するには、各アイデアが母集団内でどれほど稀であるかを測定することです。これは創造性研究で長年用いられてきたアプローチですが、大規模な自動化は困難でした。アイデアの言い換えを手作業で分類して応答頻度を集計するのは、手間がかかり、エラーが発生しやすく、大規模なコーパスでは脆弱です。そこで、頻度ベースの独創性スコアリングのための、完全に自動化された心理測定的に検証されたパイプラインを導入します。私たちの手法であるMuseRAGは、大規模言語モデル（LLM）と外部で調整された検索拡張生成（RAG）フレームワークを組み合わせたものです。新しいアイデアが与えられると、システムは意味的に類似した既存のアイデアのバケットを検索し、LLMにゼロショットプロンプトを与えて、新しいアイデアが既存のバケットに属するか、新しいバケットを形成するかを判断させます。その結果得られるバケットにより、頻度ベースの独創性メトリックの計算が可能になります。5つのデータセット（N=1143、n_ideas=16294）全体で、MuseRAGはアイデアのクラスタリング構造と解像度（AMI = 0.59）、および参加者レベルのスコアリング（r = 0.89）において、人間のアノテーターと一致し、強力な収束的および外的妥当性を示しています。私たちの研究は、創造性研究を支援するために、意図に敏感で、人間と整合した独創性スコアリングを大規模に実現します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16232v1</guid><pubDate>Thu, 22 May 2025 05:05:25 +0000</pubDate></item><item><title>Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free</title><link>http://arxiv.org/abs/2505.16644v1</link><description>我々は、確率的力学系の初期および最終状態のアンサンブル測定と、力学に関する何らかの事前知識が与えられたとき、データと整合的なシステムの「最も可能性の高い」進化を再構築することを目的とするシュレーディンガー橋問題を考察する。既存の研究のほとんどはブラウン運動を参照力学として仮定しており、暗黙のうちにポテンシャル駆動の力学に限定されている。我々は、この領域から逸脱し、汎用的なドリフト行列 $\mathbf{A} \in \mathbb{R}^{d \times d}$ を持つ多変量オルンシュタイン-ウーレンベック過程によって記述される参照過程を考察する。$\mathbf{A}$ が非対称である場合、これは非保存力が働く非平衡系に対応する。これは、本質的に非平衡状態にある生物学的システムへの応用にとって重要である。ガウス周辺分布の場合、静的および動的シュレーディンガー橋の解を特徴付ける明示的な式を導出する。一般的な周辺分布に対しては、シュレーディンガー橋を学習するための、流れとスコアマッチングに基づくシミュレーションフリーのアルゴリズムである mvOU-OTFM を提案する。合成および実際の単一細胞データに基づく一連の問題への適用において、mvOU-OTFM は、競合する手法と比較してより高い精度を達成し、かつトレーニングが大幅に高速であることを示す。

&lt;img src="https://arxiv.org/html/2505.16644v1/x1.png"/&gt;&lt;p&gt;Stephen Y. Zhang University of Melbourne &amp;Michael P. H. Stumpf University of Melbourne&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16644v1</guid><pubDate>Thu, 22 May 2025 13:17:30 +0000</pubDate></item><item><title>Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions</title><link>http://arxiv.org/abs/2505.16311v1</link><description>生成AI（GenAI）モデルにおける最近の進歩により、最新のユーザーコンテキストに適応するパーソナライズされたコンテンツの生成が可能になった。パーソナライズされた意思決定システムはしばしばバンディット問題としてモデル化されるが、GenAIの統合は、古典的な逐次学習問題に新たな構造をもたらす。GenAIを活用した介入では、エージェントがクエリを選択するが、環境は生成モデルから引き出された確率的な応答を経験する。標準的なバンディット法は、この構造、つまりアクションが確率的な観測された処置を通じてのみ報酬に影響を与えるという構造を明示的に考慮していない。我々は、大規模言語モデルで生成されたテキストを用いたモバイルヘルス介入を動機付けのケーススタディとして、このアクション/処置の分割のために設計されたバンディットアプローチである、ジェネレーター媒介バンディット-トンプソンサンプリング（GAMBITTS）を導入する。GAMBITTSは、処置と報酬の生成プロセスの両方を明示的にモデル化し、配信された処置の情報を使用して、標準的な方法と比較してポリシー学習を加速する。我々は、処置と報酬における不確実性の源泉を分解することにより、GAMBITTSのレグレット境界を確立し、標準的なバンディットアプローチよりも強力な保証を達成する条件を特定する。シミュレーション研究では、GAMBITTSは観測された処置を活用して期待報酬をより正確に推定することにより、一貫して従来のアルゴリズムを上回る。

&lt;img src="https://arxiv.org/html/2505.16311v1/extracted/6444282/Figures/1d_correct_model_no_legebd.png"/&gt;&lt;p&gt;Ambuj Tewari Department of Statistics, Ann Arbor, Gabriel Durham, Kihyuk Hong, MI (USA), Marc Brooks, University of Michigan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16311v1</guid><pubDate>Thu, 22 May 2025 07:06:51 +0000</pubDate></item><item><title>UFT: Unifying Supervised and Reinforcement Fine-Tuning</title><link>http://arxiv.org/abs/2505.16984v1</link><description>事後学習は、大規模言語モデル（LLM）の推論能力を高める上でその重要性を示してきました。主な事後学習手法は、教師ありファインチューニング（SFT）と強化学習ファインチューニング（RFT）に分類できます。SFTは効率的で小規模な言語モデルに適していますが、過学習を引き起こし、より大規模なモデルの推論能力を制限する可能性があります。対照的に、RFTは一般的に優れた汎化性能をもたらしますが、ベースモデルの強さに大きく依存します。SFTとRFTの限界に対処するため、我々は、SFTとRFTを単一の統合されたプロセスに統合する、新しい事後学習パラダイムであるUnified Fine-Tuning（UFT）を提案します。UFTは、モデルが効果的に解を探索しながら、有益な教師信号を取り込むことを可能にし、既存の手法に内在する記憶と思考の間のギャップを埋めます。特に、UFTはモデルサイズに関係なく、一般的にSFTとRFTの両方を上回ります。さらに、UFTがRFTに固有の指数関数的なサンプル複雑性のボトルネックを打破することを理論的に証明し、統合された学習が長期間の推論タスクにおける収束を指数関数的に加速できることを初めて示します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16984v1</guid><pubDate>Thu, 22 May 2025 17:53:57 +0000</pubDate></item><item><title>Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks</title><link>http://arxiv.org/abs/2505.16901v1</link><description>大規模言語モデル（LLM）の最近の進歩は、関数レベルのコード生成において有望性を示していますが、リポジトリレベルのソフトウェアエンジニアリングタスクは依然として困難です。現在のソリューションは主にプロプライエタリなLLMエージェントに依存しており、予測不可能性をもたらし、アクセス性を制限し、データプライバシーとモデルのカスタマイズに関する懸念を引き起こしています。本論文では、オープンソースLLMがエージェントベースのアプローチを必要とせずに、リポジトリレベルのタスクに効果的に対処できるかどうかを調査します。セマンティック情報と構造的依存関係を通じて、LLMがコードベース内の関数とファイルを理解できるようにすることで、これが可能であることを示します。この目的のために、リポジトリのコードグラフ構造をLLMのアテンションメカニズムに統合し、特殊なアダプターを使用してノード属性をLLMの入力空間にマッピングする、Code Graph Models（CGM）を導入します。エージェントレスなグラフRAGフレームワークと組み合わせることで、我々のアプローチは、オープンソースのQwen2.5-72Bモデルを使用して、SWE-bench Liteベンチマークで43.00%の解決率を達成します。このパフォーマンスは、オープンウェイトモデルの中で1位、オープンソースシステムを持つ手法の中で2位、全体で8位にランクインし、以前の最高のオープンソースモデルベースの手法を12.33%上回っています。

&lt;img src="https://arxiv.org/html/2505.16901v1/x1.png"/&gt;&lt;p&gt;Bingchang Liu, Haipeng Zhang, Hang Yu, Hongen Peng, Hongyuan Tao, Jianguo Li, Linchao Zhu, Peng Di, Rui Wang, Xukun Zhu, Ying Zhang, Yingguang Yang, Zhaogui Xu, Zhenhao Tang, Ziyin Zhang&lt;/p&gt;&lt;p&gt;Ant Group, Hangzhou, China
Shanghai Jiaotong University, Shanghai, China
ShanghaiTech University, Shanghai, China
Zhejiang University, Hangzhou, China&lt;/p&gt;</description><guid isPermaLink="false">2505.16901v1</guid><pubDate>Thu, 22 May 2025 17:00:55 +0000</pubDate></item><item><title>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning</title><link>http://arxiv.org/abs/2505.16421v1</link><description>強化学習（RL）は、大規模言語モデル（LLM）の性能向上において目覚ましい成功を収めていますが、その主な焦点は数学の問題解決のような単一ターンのタスクに当てられてきました。動的なウェブインターフェースを介した長期的な意思決定の複雑さから、複数ターンのインタラクションを行う効果的なウェブエージェントの訓練は依然として困難です。本研究では、ウェブエージェントを訓練するためのシンプルかつ効果的なエンドツーエンドのマルチターンRLフレームワークであるWebAgent-R1を提案します。これは、タスクの成功に応じた二値報酬のみに基づいて、多様な軌跡を非同期的に生成することにより、ウェブ環境とのオンラインインタラクションから直接学習します。WebArena-Liteベンチマークでの実験により、WebAgent-R1の有効性が実証され、Qwen-2.5-3Bのタスク成功率を6.1%から33.9%に、Llama-3.1-8Bのタスク成功率を8.5%から44.8%に向上させ、既存の最先端の手法やOpenAI o3のような強力なプロプライエタリモデルを大幅に上回りました。詳細な分析により、思考に基づいたプロンプト戦略の有効性と、ウェブタスクにおけるインタラクションの増加によるテスト時のスケーリングが明らかになりました。さらに、WebAgent-R1-ZeroとWebAgent-R1-CoTという2つのバリアントを導入し、異なるRL初期化ポリシーを調査することで、ウォームアップ訓練段階（すなわち、模倣学習）の重要性を強調し、ウェブエージェントに長い思考連鎖（CoT）推論を組み込むことに関する洞察を提供します。

&lt;img src="https://arxiv.org/html/2505.16421v1/x1.png"/&gt;&lt;p&gt;Liang Qiu University of Virginia Amazon Georgia Institute of Technology, Qin Lu, Weizhi Zhang, Wenlin Yao, Yao Liu, Zhepei Wei&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16421v1</guid><pubDate>Thu, 22 May 2025 09:07:43 +0000</pubDate></item><item><title>Improving Chemical Understanding of LLMs via SMILES Parsing</title><link>http://arxiv.org/abs/2505.16340v1</link><description>大規模言語モデル（LLM）は、特に分子科学において、科学的発見のための強力なツールとしてますます認識されています。これらのモデルの基本的な要件は、分子構造を正確に理解する能力であり、これは通常SMILES表現でエンコードされます。しかし、現在のLLMはSMILESの解釈に苦労しており、分子リングの数を数えるといった基本的なタスクでさえ失敗します。この制限に対処するため、我々はCLEANMOLという新しいフレームワークを導入します。これは、SMILESの解析を、グラフレベルでの分子理解を促進するように明示的に設計された、一連の明確で決定論的なタスクとして定式化します。これらのタスクは、部分グラフのマッチングからグローバルグラフのマッチングまで及び、分子構造特性に沿った構造化された教師あり学習を提供します。我々は、適応的な難易度スコアリングを用いた分子事前学習データセットを構築し、これらのタスクでオープンソースのLLMを事前学習させます。我々の結果は、CLEANMOLが構造理解を向上させるだけでなく、Mol-Instructionsベンチマークで最高の性能を達成するか、ベースラインと競合することを示しています。

&lt;img src="https://arxiv.org/html/2505.16340v1/x1.png"/&gt;&lt;p&gt;Yunhui Jang KAIST &amp;Jaehyung Kim Yonsei University &amp;Sungsoo Ahn KAIST&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16340v1</guid><pubDate>Thu, 22 May 2025 07:54:39 +0000</pubDate></item><item><title>ChemMLLM: Chemical Multimodal Large Language Model</title><link>http://arxiv.org/abs/2505.16326v1</link><description>マルチモーダル大規模言語モデル（MLLM）は近年、多くのアプリケーションで目覚ましい進歩を遂げています。しかし、クロスモーダルな理解と生成を扱える化学MLLMは、まだ十分に研究されていません。このギャップを埋めるため、本論文では、分子の理解と生成のための統一された化学マルチモーダル大規模言語モデルであるChemMLLMを提案します。また、テキスト、分子SMILES文字列、画像にまたがる5つのマルチモーダルタスクを設計し、データセットをキュレーションしました。これらのタスクにおいて、ChemMLLMを一般的な主要MLLMおよび化学LLMと比較評価しました。実験結果は、ChemMLLMが評価されたすべてのタスクにおいて優れた性能を達成することを示しています。例えば、分子画像最適化タスクでは、ChemMLLMは最高のベースライン（GPT-4o）を118.9％上回っています（特性改善4.27対1.95）。コードはhttps://github.com/bbsbz/ChemMLLM.gitで公開されています。

&lt;img src="https://arxiv.org/html/2505.16326v1/x1.png"/&gt;&lt;p&gt;Dongzhan Zhou, Peng Xia, Qian Tan, Technology of China Shanghai Artificial Intelligence Laboratory Nanjing University, University of Science, Wanhao Liu&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16326v1</guid><pubDate>Thu, 22 May 2025 07:32:17 +0000</pubDate></item><item><title>R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning</title><link>http://arxiv.org/abs/2505.17005v1</link><description>大規模言語モデル（LLM）は強力ですが、静的な知識のためにハルシネーションを起こしやすいです。検索拡張生成（RAG）は外部情報を注入することで役立ちますが、現在の手法はしばしば高コストで、汎化性能が低く、モデルの内部知識を無視します。本論文では、LLMが内部および外部の知識源を適応的に活用するように訓練するために設計された新しいフレームワークであるR1-Searcher++を紹介します。R1-Searcher++は、予備的なフォーマット学習のための初期SFTコールドスタートフェーズと、動的知識獲得のためのRLという2段階のトレーニング戦略を採用しています。RL段階では、探索を促すための結果監督、内部知識の利用を促すための報酬メカニズム、および検索された情報を継続的に同化してモデルの内部知識を豊かにする記憶メカニズムを統合しています。内部知識と外部検索エンジンを活用することで、モデルは継続的にその能力を向上させ、効率的な検索拡張推論を可能にします。私たちの実験では、R1-Searcher++が以前のRAGおよび推論手法よりも優れており、効率的な検索を達成することが示されています。コードはhttps://github.com/RUCAIBox/R1-Searcher-plusで入手できます。

&lt;img src="https://arxiv.org/html/2505.17005v1/x1.png"/&gt;&lt;p&gt;Gaoling School of Artificial Intelligence, Renmin University of China. DataCanvas Alaya NeW. Beijing Institute of Technology.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.17005v1</guid><pubDate>Thu, 22 May 2025 17:58:26 +0000</pubDate></item><item><title>X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs</title><link>http://arxiv.org/abs/2505.16997v1</link><description>LLMベースのマルチエージェントシステム（MAS）は、複数の専門化されたエージェント間の協調を可能にすることで、単一のLLMの能力を拡張します。しかし、既存のMASフレームワークのほとんどは、単一のLLMにすべてのエージェントを駆動させることに依存しており、システムの知能をそのモデルの限界に制約しています。本論文では、エージェントが多様なLLMによって駆動され、システムの潜在能力を多様なLLMの集合知に高める、異種LLM駆動型MAS（X-MAS）のパラダイムを探求します。様々なドメインおよびMAS関連機能における様々なLLMの性能を評価するために設計された包括的なテストベッドであるX-MAS-Benchを紹介します。広範な実証研究として、5つのドメイン（21のテストセットを含む）と5つの機能にわたって27のLLMを評価し、各ドメインと機能の組み合わせに最適なモデル選択を特定するために170万回以上の評価を実施します。これらの発見に基づいて、同種LLM駆動型MASから異種LLM駆動型MASへの移行が、構造的な再設計を必要とせずにシステム性能を大幅に向上させることができることを示します。具体的には、チャットボットのみのMASシナリオでは、異種構成によりMATHデータセットで最大8.4％の性能向上が得られます。チャットボットと推論者の混合シナリオでは、異種MASはAIMEデータセットで驚異的な47％の性能向上を達成できます。私たちの結果は、MASにおける異種LLMの変革の可能性を強調し、スケーラブルで協調的なAIシステムを推進するための有望な道筋を示しています。

&lt;img src="https://arxiv.org/html/2505.16997v1/x1.png"/&gt;&lt;p&gt;Shanghai Jiao Tong University University of Oxford The University of Sydney Shanghai AI Laboratory Equal Contributions Corresponding Author&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16997v1</guid><pubDate>Thu, 22 May 2025 17:56:39 +0000</pubDate></item><item><title>MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems</title><link>http://arxiv.org/abs/2505.16988v1</link><description>LLM（大規模言語モデル）に基づくマルチエージェントシステム（MAS）は、単一のLLMを強化し、実用的なアプリケーションにおける複雑で多様なタスクに対処する上で大きな可能性を示しています。しかし、目覚ましい進歩にもかかわらず、既存の手法を統合する統一されたコードベースが不足しており、その結果、冗長な再実装、不公平な比較、研究者にとって高い参入障壁が生じています。これらの課題に対処するため、LLMベースのMASのための統一的で包括的、かつ研究に優しいコードベースであるMASLabを紹介します。(1) MASLabは、複数のドメインにわたる20以上の確立された手法を統合し、それぞれの手法は、公式の実装とのステップごとの出力を比較することで厳密に検証されています。(2) MASLabは、手法間の公平な比較のための様々なベンチマークを備えた統一された環境を提供し、一貫した入力と標準化された評価プロトコルを保証します。(3) MASLabは、共有された合理化された構造内で手法を実装し、理解と拡張の障壁を下げます。MASLabを基盤として、10以上のベンチマークと8つのモデルを網羅する広範な実験を行い、研究者にMAS手法の現在の状況に関する明確かつ包括的な見解を提供します。MASLabは、この分野の最新の動向を追跡し、より広範なオープンソースコミュニティからの貢献を歓迎し、進化し続けます。

&lt;img src="https://arxiv.org/html/2505.16988v1/x1.png"/&gt;&lt;p&gt;Shanghai Jiao Tong University Shanghai AI Laboratory University of Oxford Princeton University Meta University of Michigan The University of Sydney Beihang University Nanyang Technological University Nanjing University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16988v1</guid><pubDate>Thu, 22 May 2025 17:54:38 +0000</pubDate></item><item><title>NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification</title><link>http://arxiv.org/abs/2505.16938v1</link><description>人工知能（AI）は、科学研究のパラダイムの変革を加速させており、研究効率を高めるだけでなく、イノベーションを推進しています。本稿では、様々な科学研究分野にわたる自律的科学研究（ASR）を実施するための、統一された閉ループ型マルチエージェントフレームワークであるNovelSeekを紹介します。これにより、研究者はこれらの分野における複雑な問題を、これまでにないスピードと精度で解決できます。NovelSeekは、3つの主要な利点を強調しています。1) スケーラビリティ：NovelSeekは、12の科学研究タスクにわたってその汎用性を示しており、ベースラインコードのパフォーマンスを向上させる革新的なアイデアを生み出すことができます。2) インタラクティビティ：NovelSeekは、自動化されたエンドツーエンドのプロセスにおいて、人間の専門家からのフィードバックとマルチエージェントの相互作用のためのインターフェースを提供し、ドメイン専門家の知識をシームレスに統合することを可能にします。3) 効率性：NovelSeekは、人間の努力と比較して大幅に少ない時間コストで、いくつかの科学分野で有望なパフォーマンス向上を達成しました。例えば、反応収率予測では、わずか12時間で27.6%から35.4%に向上し、エンハンサー活性予測では、わずか4時間の処理で精度が0.52から0.79に上昇し、2Dセマンティックセグメンテーションでは、わずか30時間で精度が78.8%から81.0%に向上しました。

&lt;img src="https://arxiv.org/html/2505.16938v1/x3.png"/&gt;&lt;p&gt;NovelSeek Team, Shanghai Artificial Intelligence Laboratory https://alpha-innovator.github.io/NovelSeek-project-page https://github.com/Alpha-Innovator/NovelSeek https://huggingface.co/U4R/NovelSeek&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16938v1</guid><pubDate>Thu, 22 May 2025 17:27:43 +0000</pubDate></item><item><title>Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning</title><link>http://arxiv.org/abs/2505.16410v1</link><description>近年、大規模言語モデル（LLM）は、大規模な強化学習（RL）を通じて目覚ましい推論能力を示しています。しかし、LLMにおける効果的なマルチツール協調推論を強化するためにRLアルゴリズムを活用することは、依然として未解決の課題です。本論文では、LLMが段階的な推論中に複数の外部ツールを自律的に呼び出すことを可能にするように設計された、RLベースのフレームワークであるTool-Starを紹介します。Tool-Starは6種類のツールを統合し、データ合成とトレーニングの両方において体系的な設計を取り入れています。ツール使用データの不足に対処するため、ツール統合プロンプトとヒントベースのサンプリングを組み合わせ、ツール使用軌跡を自動的かつスケーラブルに生成する、一般的なツール統合推論データ合成パイプラインを提案します。その後の品質正規化と難易度認識分類プロセスにより、低品質のサンプルが除外され、データセットが簡単なものから難しいものへと整理されます。さらに、(1) コールドスタートファインチューニング（ツール呼び出しフィードバックを通じてLLMが推論パターンを探索するように誘導）と、(2) 階層的な報酬設計を備えたマルチツール自己批判RLアルゴリズム（報酬の理解を強化し、効果的なツール連携を促進）という、2段階のトレーニングフレームワークを提案し、マルチツール協調推論を強化します。10を超える困難な推論ベンチマークでの実験的分析は、Tool-Starの有効性と効率を強調しています。コードはhttps://github.com/dongguanting/Tool-Starで入手できます。

&lt;img src="https://arxiv.org/html/2505.16410v1/x1.png"/&gt;&lt;p&gt;Guanting Dong, Hongjin Qian, Jiajie Jin, Xiaoxi Li, Yifei Chen, Yutao Zhu Renmin University of China BAAI Kuaishou Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16410v1</guid><pubDate>Thu, 22 May 2025 09:00:19 +0000</pubDate></item><item><title>SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers</title><link>http://arxiv.org/abs/2505.16330v1</link><description>新規性は学術論文の核となる要素であり、新規性の評価には複数の視点があります。既存の手法は単語やエンティティの組み合わせに焦点を当てることが多く、得られる洞察は限られています。論文の新規性に関連する内容は通常、序論、方法論、結果など、異なる主要なセクションに分散しています。したがって、論文の新規性を評価するための最適なセクションの組み合わせを検討することは、自動化された新規性評価を進歩させる上で重要です。本論文では、学術論文のさまざまなセクションの組み合わせを入力として使用し、言語モデルを駆動して新規性スコアを予測します。次に、結果を分析して、新規性スコアの予測に最適なセクションの組み合わせを決定します。まず、自然言語処理技術を用いて学術論文のセクション構造を特定し、序論、方法、結果、考察（IMRaD）に分類します。その後、これらのセクションのさまざまな組み合わせ（例：序論と方法）を、事前学習済み言語モデル（PLM）および大規模言語モデル（LLM）の入力として使用し、人間の専門家レビューアによって提供された新規性スコアを正解ラベルとして使用して、予測結果を取得します。結果は、序論、結果、考察を使用することが論文の新規性を評価するのに最も適切であり、全文を使用しても有意な結果は得られないことを示しています。さらに、PLMとLLMの結果に基づいて、序論と結果が新規性スコア予測のタスクにとって最も重要なセクションであると思われます。本論文のコードとデータセットは、https://github.com/njust-winchy/SC4ANM でアクセスできます。

&lt;img src="https://arxiv.org/html/2505.16330v1/x1.png"/&gt;&lt;p&gt;Chengzhi Zhang, Tong Bao, Wenqing Wu, Yi Zhao&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16330v1</guid><pubDate>Thu, 22 May 2025 07:34:59 +0000</pubDate></item><item><title>T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning</title><link>http://arxiv.org/abs/2505.16986v1</link><description>大規模言語モデル（LLM）は、複雑な問題を解決できる知的エージェントとして目覚ましい能力を示してきました。しかし、APIやツール呼び出し間の依存関係を伴うシナリオ、特に複数ターンの会話における効果的な計画立案は、依然として大きな課題です。これに対処するため、我々はT1という、ツール拡張された、マルチドメイン、マルチターンの会話データセットを導入します。これは、多様なドメインにわたるツール間の依存関係を捉え、管理するために特別に設計されています。T1は、短期および長期の記憶のための統合されたキャッシュメカニズムの助けを借りて、9つの異なるドメイン（4つのシングルドメインと5つのマルチドメイン）にわたるエージェントのツール使用を調整する能力を厳密に評価することを可能にし、再計算するかキャッシュされた結果を再利用するかを決定するなど、動的な再計画をサポートします。ツール使用と計画に関する研究を促進するだけでなく、T1はオープンソース言語モデルのパフォーマンスを評価するためのベンチマークとしても機能します。T1-Agentによって実現された結果を提示し、複雑なツール依存シナリオにおける計画と推論の能力を強調します。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16986v1</guid><pubDate>Thu, 22 May 2025 17:54:32 +0000</pubDate></item><item><title>Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine</title><link>http://arxiv.org/abs/2505.16982v1</link><description>大規模言語モデル（LLM）は生物医学において有望だが、真の因果関係の理解を欠き、相関関係に依存している。本論文では、マルチモーダルデータ（テキスト、画像、ゲノミクスなど）を統合し、介入に基づく推論を行い、原因と結果を推論する因果LLMエージェントを構想する。これを実現するには、安全で制御可能なエージェントフレームワークの設計、因果評価のための厳密なベンチマークの開発、異種データソースの統合、LLMと構造化知識（KG）および正式な因果推論ツールとの相乗的な組み合わせという主要な課題を克服する必要がある。このようなエージェントは、自動化された仮説生成とシミュレーションによる創薬の加速、患者固有の因果モデルによる個別化医療の実現など、変革的な機会を切り開く可能性がある。この研究アジェンダは、因果関係の概念と基盤モデルを結びつけ、生物医学の進歩のための信頼できるAIパートナーを開発するために、学際的な取り組みを促進することを目的としている。

&lt;img src="https://arxiv.org/html/2505.16982v1/extracted/6468486/challenges.png"/&gt;&lt;p&gt;Adib Bazgir University of Missouri-Columbia Columbia, Amir Habibdoust Lafmajani University of Missouri-Columbia Columbia, MO 65211, USA, Yuwen Zhang University of Missouri-Columbia Columbia&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16982v1</guid><pubDate>Thu, 22 May 2025 17:52:59 +0000</pubDate></item><item><title>AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios</title><link>http://arxiv.org/abs/2505.16944v1</link><description>大規模言語モデル（LLM）は、現実世界の自律エージェント応用において高度な能力を示してきました。LLMベースのエージェントを開発し、実用的な要求に対応しようとする研究努力が拡大しており、新たな課題が生じています。それは、自律エージェントのシナリオでは、拡張されたシステムプロンプトや詳細なツール仕様など、複雑な制約を伴う長文の指示が頻繁に発生することです。このような指示の遵守は自律エージェント応用にとって非常に重要ですが、LLMがそれらを確実に実行できるかどうかは、まだ十分に調査されていません。本論文では、自律エージェントのシナリオにおけるLLMの指示遵守能力を体系的に評価するための最初のベンチマークであるAgentIFを紹介します。AgentIFは、次の3つの主要な特徴を備えています。（1）現実的：50の現実世界の自律エージェント応用から構築されています。（2）長文：平均1,723語、最大15,630語です。（3）複雑：指示あたり平均11.9個の制約があり、ツール仕様や条件制約など、多様な制約タイプを網羅しています。AgentIFを構築するために、産業用アプリケーションエージェントおよびオープンソースの自律エージェントシステムから、50の自律エージェントタスクにわたる707件の人手でアノテーションされた指示を収集しました。各指示について、関連する制約と、コードベースの評価、LLMベースの評価、ハイブリッドコード-LLM評価を含む、対応する評価指標をアノテーションしました。AgentIFを使用して、既存の高度なLLMを体系的に評価します。現在のモデルは、特に複雑な制約構造やツール仕様の処理において、一般的に性能が低いことが観察されました。さらに、指示の長さとメタ制約に関するエラー分析と分析実験を行い、既存のLLMの失敗モードに関するいくつかの知見を提供します。今後の研究を促進するために、コードとデータを公開しました。

&lt;img src="https://arxiv.org/html/2505.16944v1/x1.png"/&gt;&lt;p&gt;Amy Xin, Hao Peng, Tsinghua University Zhipu AI, Xiaozhi Wang, Youfeng Liu, Yunjia Qi&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16944v1</guid><pubDate>Thu, 22 May 2025 17:31:10 +0000</pubDate></item><item><title>Structure-Aligned Protein Language Model</title><link>http://arxiv.org/abs/2505.16896v1</link><description>タンパク質配列の大規模データベースで事前学習されたタンパク質言語モデル（pLMs）は、様々な下流タスクで優れているものの、多くの生物学的応用にとって不可欠な構造的知識を欠いています。これに対処するため、事前学習済みのタンパク質グラフニューラルネットワーク（pGNNs）からの構造的洞察を、潜在レベルの対照学習タスクを通じてpLMsに統合します。このタスクは、複数のタンパク質にわたってpLMsからの残基表現をpGNNsからの残基表現と整合させ、タンパク質間の構造的知識でpLMsを強化します。さらに、pLMsが構造トークンを予測するように最適化することで、タンパク質内の構造的知識を注入する物理レベルのタスクを組み込みます。提案されたデュアルタスクフレームワークは、タンパク質間およびタンパク質内の構造的知識の両方を効果的にpLMsに組み込みます。PDBにおけるタンパク質構造の品質のばらつきを考慮して、残基損失選択モジュールを導入します。これは、高品質の構造で訓練された小さなモデルを使用して、pLMが学習するための信頼性が高く、かつ困難な残基損失を選択します。最先端のESM2およびAMPLIFYに構造アライメント法を適用した結果、ESM2の接触予測における12.7%の向上を含む、幅広いタスクで顕著なパフォーマンス向上が得られました。データ、コード、および結果として得られたSaESM2およびSaAMPLIFYモデルは、Hugging Faceで公開されます。

&lt;img src="https://arxiv.org/html/2505.16896v1/extracted/6467786/Figure/cover_figure.png"/&gt;&lt;p&gt;Can (Sam) Chen, Christopher James Langmead, David Heurtel-Depeiges, Quentin Fournier, Robert M. Vernon, Yoshua Bengio&lt;/p&gt;&lt;p&gt;Amgen
Chandar Research Lab
Mila – Quebec AI Institute
Polytechnique Montréal
Université de Montréal&lt;/p&gt;</description><guid isPermaLink="false">2505.16896v1</guid><pubDate>Thu, 22 May 2025 16:56:12 +0000</pubDate></item><item><title>Learning Flexible Forward Trajectories for Masked Molecular Diffusion</title><link>http://arxiv.org/abs/2505.16790v1</link><description>マスクされた拡散モデル（MDM）は、離散データのモデリングにおいて目覚ましい進歩を遂げましたが、分子生成におけるその可能性はまだ十分に探求されていません。本研究では、その可能性を探求し、標準的なMDMを単純に適用すると性能が著しく低下するという驚くべき結果を紹介します。この問題の重大な原因は、状態衝突問題であると特定しました。これは、異なる分子の順方向拡散が共通の状態に崩壊し、単峰性の予測を伴う典型的な逆拡散プロセスでは学習できない再構成ターゲットの混合をもたらすものです。これを軽減するために、Masked Element-wise Learnable Diffusion（MELD）を提案します。これは、要素ごとの破壊軌道を調整して、異なる分子グラフ間の衝突を回避します。これは、個々のグラフ要素（すなわち、原子と結合）に異なる破壊率を割り当てるパラメータ化されたノイズスケジューリングネットワークを通じて実現されます。多様な分子ベンチマークに関する広範な実験により、MELDは要素に依存しないノイズスケジューリングと比較して、全体的な生成品質を著しく向上させ、ZINC250KにおけるバニラMDMの化学的妥当性を15％から93％に向上させることが明らかになりました。さらに、条件付き生成タスクにおいて、最先端の特性アライメントを達成します。

&lt;img src="https://arxiv.org/html/2505.16790v1/x1.png"/&gt;&lt;p&gt;Hyunjin Seo, Polymerize, Sihyun Yu, SungSoo Ahn Korea Advanced Institute of Science, Taewon Kim, Technology (KAIST)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16790v1</guid><pubDate>Thu, 22 May 2025 15:30:17 +0000</pubDate></item><item><title>O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering</title><link>http://arxiv.org/abs/2505.16582v1</link><description>大規模言語モデル（LLM）は、その進歩にもかかわらず、基本的に静的なパラメータ知識によって制限されており、オープンなドメインの最新情報を必要とするタスクでのパフォーマンスを妨げています。LLMが外部知識環境と相互作用できるようにすることは有望な解決策ですが、現在の取り組みは主にクローズドエンドの問題に対処しています。標準的な答えがない、または非ユニークで多様な答えを提供するのが特徴であるオープンエンドの質問は、未だ十分に探求されていません。このギャップを埋めるために、我々はO$^2$-Searcherを提案します。これは、強化学習を活用して、オープンなドメインにおけるオープンエンドとクローズドエンドの両方の質問に効果的に取り組む新しい検索エージェントです。O$^2$-Searcherは、動的な知識獲得のために効率的なローカルシミュレーション検索環境を活用し、外部世界の知識をモデルの高度な推論プロセスから効果的に分離します。綿密に設計された報酬関数を備えた統一されたトレーニングメカニズムを採用し、エージェントが問題の種類を特定し、さまざまな回答生成戦略を適応できるようにします。さらに、複雑なオープンエンドタスクのパフォーマンスを評価するために、関連するWebページキャッシュを備えた300の手動でキュレーションされた、マルチドメインのオープンエンドの質問を特徴とする高品質なベンチマークであるO$^2$-QAを構築しました。広範な実験により、O$^2$-Searcherは、わずか3Bのモデルを使用するだけで、O$^2$-QAで主要なLLMエージェントを大幅に上回ることが示されています。また、同様のサイズのモデルに対して、さまざまなクローズドエンドQAベンチマークでSOTAの結果を達成し、はるかに大きなモデルと同等のパフォーマンスを発揮します。

&lt;img src="https://arxiv.org/html/2505.16582v1/extracted/6467017/figs/teaser.png"/&gt;&lt;p&gt;Jianbiao Mei Tao Hu Daocheng Fu Licheng Wen Xuemeng Yang Zhejiang University Shanghai Artificial Intelligence Laboratory University of Science, Technology of China Fudan University State Key Laboratory of Industrial Control Technology&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16582v1</guid><pubDate>Thu, 22 May 2025 12:17:13 +0000</pubDate></item><item><title>How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance</title><link>http://arxiv.org/abs/2505.16276v1</link><description>大規模言語モデル（LLM）を知識グラフエンジニアリング（KGE）のサポートに利用する場合、適切なモデルを探す際に最初に注目するのはそのサイズです。スケーリング則によれば、一般的にモデルが大きいほど能力が高いことが示されています。しかし、実際にはリソースコストも重要な要素であるため、モデルの性能とコストの比率を考慮することが理にかなっています。LLM-KG-Benchフレームワークは、KGEタスクにおけるLLMの比較を可能にし、KGおよびKGクエリの理解と生成能力を評価します。26のオープンな最先端LLMを網羅するLLM-KG-Benchの実行で作成されたデータセットに基づいて、KGEタスクに特有のモデルサイズのスケーリング則を調査します。分析では、ベンチマークスコアが異なるモデルサイズカテゴリ間でどのように変化するかを評価します。さらに、単一のモデルおよびモデルファミリーの一般的なスコアの推移が、そのサイズとどのように相関するかを調べます。分析の結果、いくつかの例外を除き、モデルサイズのスケーリング則は、選択されたKGEタスクにも概ね適用されることが明らかになりました。ただし、場合によっては、プラトー効果または天井効果が発生し、つまり、タスクのパフォーマンスが、あるモデルとその次に大きいモデルの間であまり変化しませんでした。このような場合、高い費用対効果を達成するために、より小さなモデルを検討することができます。同じファミリーのモデルに関しては、より大きなモデルが、同じファミリーのより小さなモデルよりもパフォーマンスが低い場合がありました。これらの効果は局所的にのみ発生しました。したがって、同じファミリーの次に小さいモデルと大きいモデルもテストすることをお勧めします。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16276v1</guid><pubDate>Thu, 22 May 2025 06:21:40 +0000</pubDate></item><item><title>Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design</title><link>http://arxiv.org/abs/2505.16979v1</link><description>単一エージェントLLMは、有限のコンテキスト、役割の過負荷、脆弱なドメイン転送といった厳しい限界に直面する。従来のマルチエージェントによる修正は、これらの問題を緩和するものの、不適切な分解、曖昧な契約、検証のオーバーヘッドといった新たな問題を引き起こし、利点を損なう。そこで、我々はKnow-The-Ropes (KtR)を提案する。これは、ドメインの事前知識をアルゴリズムの設計図階層に変換するフレームワークであり、タスクは型付けされたコントローラーを介したサブタスクに再帰的に分割され、各サブタスクはゼロショットまたは最小限のブースト（例：chain-of-thought、マイクロチューニング、自己チェック）で解決される。ノーフリーランチ定理に基づき、KtRは普遍的なプロンプトを追い求める代わりに、規律ある分解を行う。ナップサック問題（3〜8アイテム）において、3つのGPT-4o-miniエージェントは、単一のボトルネックエージェントを修正した後、サイズ5のインスタンスで精度をゼロショットの3％から95％に向上させる。より困難なタスク割り当て問題（6〜15ジョブ）では、6エージェントのo3-mini設計図が、サイズ10まで100％、サイズ13〜15で84％の精度を達成するのに対し、ゼロショットでは11％である。したがって、アルゴリズムを意識した分解とターゲットを絞った拡張により、控えめなモデルを信頼できる協力者に変えることができる。より巨大なモノリスは必要ない。

&lt;img src="https://arxiv.org/html/2505.16979v1/x1.png"/&gt;&lt;p&gt;Zhenkun Li University of South Florida &amp;Lingyao Li University of South Florida Shuhang Lin Rutgers University &amp;Yongfeng Zhang Rutgers University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16979v1</guid><pubDate>Thu, 22 May 2025 17:52:33 +0000</pubDate></item><item><title>MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models</title><link>http://arxiv.org/abs/2505.16700v1</link><description>大規模言語モデル（LLM）が、受動的なテキスト生成器から、ツール操作が可能な能動的な推論エージェントへと進化するにつれて、モデルコンテキストプロトコル（MCP）が、動的なツール発見とオーケストレーションのための標準化されたフレームワークとして登場しました。業界での広範な採用にもかかわらず、既存の評価方法論では、この新しいパラダイムにおけるツール利用能力を適切に評価できていません。本論文では、MCPフレームワークにおけるLLMのパフォーマンスを評価するために特別に設計された、初の包括的なベンチマークであるMCP-RADARを紹介します。これは、回答の正確性、ツール選択の効率、計算資源の効率、パラメータ構築の正確性、実行速度を測定する、斬新な5次元アプローチを採用しています。主観的な人間による評価や二値の成功指標に依存する従来のベンチマークとは異なり、MCP-RADARは、ソフトウェアエンジニアリング、数学的推論、一般的な問題解決を含む複数のタスクドメインにわたって、客観的で定量化可能な測定を使用します。主要な商用およびオープンソースのLLMの評価により、正確性、効率、速度の間に大きなトレードオフが存在する、特徴的な能力プロファイルが明らかになり、従来の単一指標によるパフォーマンスランキングに疑問を投げかけます。さらに、開発者がモデルの互換性と有効性を最大限に高めるためにツールを最適化するための貴重なガイダンスを提供します。標準化されたアプローチのためMCPに焦点を当てていますが、当社の方法論はすべてのLLMエージェントツール統合フレームワークに適用可能であり、LLM開発者とツール作成者の両方がLLMとツールの相互作用エコシステム全体を最適化するための貴重な洞察を提供します。評価で使用した実装、構成、およびデータセットは、https://anonymous.4open.science/r/MCPRadar-B143 で公開されています。

&lt;img src="https://arxiv.org/html/2505.16700v1/x1.png"/&gt;&lt;p&gt;China, China Juan Zhai University of Massachusetts at Amherst Amherst, China Siyi Xie Xi’an Jiaotong University Xi’an, USA Chao Shen Xi’an Jiaotong University Xi’an, USA Shqing Ma University of Massachusetts at Amherst Amherst, Xuanqi Gao Xi’an Jiaotong University Xi’an&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16700v1</guid><pubDate>Thu, 22 May 2025 14:02:37 +0000</pubDate></item><item><title>ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming</title><link>http://arxiv.org/abs/2505.16667v1</link><description>近年の研究では、競技プログラミングにおける人間とLLMの協調の価値がますます強調され、多くの経験的な手法が提案されていますが、既存の研究の断片的な性質や、アプリケーション固有の多様な人間のフィードバックの使用により、包括的な理解は依然として得られていません。そこで、私たちの研究は3つの目的を果たします。第一に、プログラミングプロセス全体を統合した、初の人間によるフィードバックの分類法を提示し、きめ細かい評価を促進します。第二に、人間とLLMの協調のために特別に設計された、新しいプログラミングデータセットELABORATIONSETを導入します。これは、大規模なシミュレーションされた人間のフィードバックを可能にし、費用対効果の高い実際の人間のインタラクション研究を促進するために、綿密にアノテーションされています。第三に、人間とLLMの競技プログラミングの徹底的な評価を促進するための新しいベンチマークELABORATIONを導入します。ELABORATIONにより、既存の手法の強みと弱みを特定し、将来の改善のための基盤を築きます。私たちのコードとデータセットは、https://github.com/SCUNLP/ELABORATION で入手できます。

&lt;img src="https://arxiv.org/html/2505.16667v1/extracted/6467325/pic/taxonomy.png"/&gt;&lt;p&gt;Chen Huang, China △ △ {\triangle} △ Vanderbilt University xinwei_yang@stu.scu.edu.cn  {scu.zhangtong, Industry Intelligence, Jiashuai Zhang, Ministry of Education, Technology ♡ ♡ {\heartsuit} ♡ Engineering Research Center of Machine Learning, Xinwei Yang, Zhaofeng Liu, huangc.scu}@gmail.com wenqianglei@scu.edu.cn, ♠ ♠ {\spadesuit} ♠ Sichuan University ♢ ♢ {\diamondsuit} ♢ Tianjin University of Science&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16667v1</guid><pubDate>Thu, 22 May 2025 13:32:39 +0000</pubDate></item><item><title>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</title><link>http://arxiv.org/abs/2505.16477v1</link><description>近年のノーベル賞が科学へのAIの貢献を認めたように、大規模言語モデル（LLM）は生産性を向上させ、科学的手法を再構築することで、科学研究を変革しています。LLMは現在、実験計画、データ分析、ワークフローに関与しており、特に化学や生物学の分野で顕著です。しかし、ハルシネーションや信頼性といった課題は依然として残っています。本稿では、大規模言語モデル（LLM）が科学的手法をどのように再定義しているかを概観し、仮説検証から発見まで、科学的サイクルのさまざまな段階における潜在的な応用を探ります。LLMが関連性のある効果的な創造的エンジンおよび生産性向上ツールとして機能するためには、明確な評価指標を用いて、人間の科学的目標との連携と協調の下、科学的プロセスのすべての段階への深い統合を追求すべきであると結論付けます。AI主導の科学への移行は、創造性、監督、責任に関する倫理的な問題を提起します。慎重な指導により、LLMは創造的なエンジンへと進化し、科学分野全体で責任を持って効果的に革新的なブレークスルーを推進する可能性があります。しかし、科学界はまた、LLMに科学をどの程度任せるかを決定しなければなりません。たとえ、ほとんどの場合、現在では不当な「推論」との関連付けが、人間だけでは探求されない可能性のある仮説や解決策の領域を探求する可能性と引き換えに行われたとしてもです。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16477v1</guid><pubDate>Thu, 22 May 2025 10:05:48 +0000</pubDate></item><item><title>FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS</title><link>http://arxiv.org/abs/2505.16409v1</link><description>大規模推論モデル（LRM）は、多段階推論や適切な段階での検索エンジンの呼び出しにおいて、目覚ましい能力を示してきました。しかし、既存の検索拡張推論アプローチは、独立した検索モデルに依存しており、LRMの役割をいつ検索するか、どのようにクエリするかを決定することに限定しています。この分離は、ハードウェアと運用コストを増加させるだけでなく、表現のボトルネック（検索器の埋め込み空間が生成器の要件を満たすのに十分な表現力を持たない現象）により、検索プロセスにおけるエラーにつながります。これに対処するため、シーケンス・ツー・シーケンスのマッチングから、コーパス内の回答を含むパスを特定することに視点を移し、FREESON（Retriever-FREE Retrieval-Augmented ReaSONing）と呼ばれる新しいフレームワークを提案します。このフレームワークにより、LRMは生成器と検索器の両方として機能することで、関連する知識を自力で検索できます。これを実現するために、検索タスクに特化したMCTSアルゴリズムの変種であるCT-MCTS（Corpus-Traversing Monte Carlo Tree Search）を導入します。このアルゴリズムでは、LRMはコーパス内を回答を含む領域に向かって探索します。シングルホップおよびマルチホップの質問を含む5つのオープンなQAベンチマークでの結果は、FREESONが独立した検索器を持つ4つの多段階推論モデルと比較して、EMおよびF1で平均14.4%の改善を達成し、PopQAおよび2WikiMultihopQAでは最強のベースラインを3%上回り、同等の性能を発揮することを示しています。

&lt;img src="https://arxiv.org/html/2505.16409v1/x1.png"/&gt;&lt;p&gt;Chaeeun Kim LBOX &amp;Seungone Kim Carnegie Mellon University&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16409v1</guid><pubDate>Thu, 22 May 2025 09:00:08 +0000</pubDate></item><item><title>A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules</title><link>http://arxiv.org/abs/2505.16365v1</link><description>新しい分子化合物の開発は、健康から環境の持続可能性まで、喫緊の課題に対処するために不可欠です。しかし、新しい分子を発見するために分子空間を探求することは、その広大さゆえに困難です。本稿では、化学的に有効であることが保証された分子を生成できる、協調的かつ制約付きのグラフ拡散モデルであるCoCoGraphを紹介します。モデルに組み込まれた制約と協調的なメカニズムのおかげで、CoCoGraphは、最大で1桁少ないパラメータで、標準的なベンチマークにおいて最先端のアプローチを凌駕します。36の化学的特性の分析はまた、CoCoGraphが、現在のモデルよりも実際の分子に分布がより近い分子を生成することを示しています。モデルの効率性を活用して、820万個の合成的に生成された分子のデータベースを作成し、生成された分子の妥当性、およびCoCoGraphの潜在的なバイアスと限界をさらに評価するために、有機化学の専門家とのチューリングテストのようなテストを実施しました。

&lt;img src="https://arxiv.org/html/2505.16365v1/x1.png"/&gt;&lt;p&gt;Manuel Ruiz-Botella, Marta Sales-Pardo, Roger Guimerà&lt;/p&gt;&lt;p&gt;Department of Chemical Engineering, Universitat Rovira i Virgili, 43007 Tarragona, Catalonia
ICREA, 08007 Barcelona, Catalonia&lt;/p&gt;</description><guid isPermaLink="false">2505.16365v1</guid><pubDate>Thu, 22 May 2025 08:21:27 +0000</pubDate></item><item><title>Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space</title><link>http://arxiv.org/abs/2505.16301v1</link><description>分子動力学（MD）は、原子レベルの系の挙動を探求するための強力なツールですが、逐次的な数値積分に依存しているため、シミュレーション効率が制限されます。本稿では、力計算や積分をバイパスして、化学空間全体にわたるMD軌道を直接生成する基盤的なAIモデルであるMDtrajNet-1を紹介します。このアプローチは、機械学習原子間ポテンシャルで強化された従来のMDと比較して、シミュレーションを最大2桁高速化します。MDtrajNet-1は、共変ニューラルネットワークとTransformerベースのアーキテクチャを組み合わせることで、既知および未知のシステムの両方について、長期的な軌道を予測する上で高い精度と転移性を実現します。驚くべきことに、MDtrajNet-1によって生成された様々な分子系の軌道の誤差は、従来の第一原理MDの誤差に近いものです。このモデルの柔軟な設計は、異なる統計集団、境界条件、相互作用タイプなど、多様なアプリケーションシナリオをサポートします。従来のMDの固有の速度障壁を克服することで、MDtrajNet-1は、効率的でスケーラブルな原子シミュレーションにおける新たなフロンティアを開きます。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16301v1</guid><pubDate>Thu, 22 May 2025 06:56:19 +0000</pubDate></item><item><title>NeuralTSNE: A Python Package for the Dimensionality Reduction of Molecular Dynamics Data Using Neural Networks</title><link>http://arxiv.org/abs/2505.16476v1</link><description>教師なし機械学習は、近年、分子動力学（MD）の分野で大きな注目を集めています。特に、次元削減技術は、高次元のMDデータの膨大な量を解析し、MD軌跡にエンコードされた隠れた情報を得るために、定期的に用いられています。多くのそのような技術の中でも、t-分布型確率的近傍埋め込み法（t-SNE）は特に人気があります。ニューラルネットワークを用いるパラメトリック版のt-SNEは、あまり知られていませんが、標準的な実装と比較して、次元削減において優れた性能を発揮することが示されています。本稿では、パラメトリックt-SNEの実装であるPythonパッケージNeuralTSNEを紹介します。この実装は、PyTorchライブラリとPyTorch Lightningフレームワークを使用して行われており、モジュールとしてインポートすることも、コマンドラインから使用することもできます。NeuralTSNEは、MDデータの解析のための使いやすいツールであることを示します。

&lt;img src="https://arxiv.org/html/2505.16476v1/x2.png"/&gt;&lt;p&gt;Jakub Rydzewski, Mateusz Skarupski, Patryk Tajs&lt;/p&gt;&lt;p&gt;Institute of Physics,
Faculty of Physics, Astronomy and Informatics,
Nicolaus Copernicus University,
Grudziądzka 5, 87-100 Toruń, Poland&lt;/p&gt;</description><guid isPermaLink="false">2505.16476v1</guid><pubDate>Thu, 22 May 2025 10:05:04 +0000</pubDate></item><item><title>Accurate crystal field Hamiltonians of single-ion magnets at mean-field cost</title><link>http://arxiv.org/abs/2505.16905v1</link><description>有効結晶場ハミルトニアンは、単一イオン磁石の電子特性を記述する上で重要な鍵となるが、そのパラメータを第一原理計算から得ることは困難である。本稿では、低エネルギー多様体内のランダムに回転させた平均場状態の密度汎関数計算を通じて、有効結晶場ハミルトニアンを導出する簡単な手法を紹介する。5つのランタノイド系錯体におけるベンチマークにおいて、平均場のコストで、はるかに高価な多配置量子化学法による最先端技術に匹敵する有効結晶場ハミルトニアンを計算できることがわかった。さらに、実験的な低エネルギー・スペクトルと磁気特性を、これまでの試みを上回る精度で再現することができる。低コストであるため、本手法は、調整された物理特性と低エネルギー・スペクトルを持つ単一イオン磁石の計算設計における重要な要素となる。

&lt;img src="https://arxiv.org/html/2505.16905v1/extracted/6468154/SOC_CF_Energy_diagram.png"/&gt;&lt;p&gt;Chenghan Li, Garnet Kin-Lic Chan, Hai-Ping Cheng, Linqing Peng, Shuanglong Liu, Xiao Chen, Xing Zhang&lt;/p&gt;&lt;p&gt;Department of Physics, Northeastern University, Boston, Massachusetts 02115, USA
Division of Chemistry and Chemical Engineering, California Institute of Technology, Pasadena CA 91125, USA&lt;/p&gt;</description><guid isPermaLink="false">2505.16905v1</guid><pubDate>Thu, 22 May 2025 17:06:25 +0000</pubDate></item><item><title>Tailored Vapor Deposition Unlocks Large-Grain, Wafer-Scale Epitaxial Growth of 2D Magnetic CrCl3</title><link>http://arxiv.org/abs/2505.16627v1</link><description>二次元磁性材料（2D-MM）は、基礎研究、スピントロニクス、量子センシングにとって刺激的な研究の場です。しかし、スケーラブルな気相成長法を用いた大粒径・大面積合成は、依然として未解決の課題です。本研究では、物理気相輸送法を用いて、マイカ基板上に半導体性2D-MM CrCl3薄膜をセンチメートルスケールで成長させるための、調整された手法を開発しました。光管理、超高キャリアガス流量、前駆体フラックス、酸素/水分除去に関する技術革新によって可能になった、制御された合成プロトコルは、ウェハースケール成長にとって不可欠です。光学、化学量論、構造、および磁気特性評価により、結晶性の相純粋な2D-MM CrCl3が確認されました。基板温度は、数層から数十ナノメートルの範囲で薄膜の厚さを調整します。さらに、選択的領域成長と大面積転写が実証されました。基板依存性の成長特性は、密度汎関数理論と最先端の機械学習原子間ポテンシャルに基づく原子スケールシミュレーションによって説明されます。このスケーラブルな気相成長法は、いくつかの2D-MMの成長に適用でき、低い成長温度（〜500℃）は、ハイブリッドヘテロ構造の作成を可能にします。

&lt;img src=""/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16627v1</guid><pubDate>Thu, 22 May 2025 13:00:23 +0000</pubDate></item><item><title>Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey</title><link>http://arxiv.org/abs/2505.16379v1</link><description>材料は現代社会の基盤であり、エネルギー、エレクトロニクス、ヘルスケア、輸送、インフラにおける進歩を支えています。調整された特性を持つ新しい材料を発見し設計する能力は、最も差し迫った地球規模の課題を解決するために不可欠です。近年、高品質な材料データの入手可能性の向上と、人工知能（AI）の急速な進歩が組み合わさることで、材料発見を加速させる新たな機会が開かれました。データ駆動型の生成モデルは、事前に定義された特性要件を満たす新しい材料を直接作成することにより、材料設計のための強力なツールを提供します。関連研究の増加にもかかわらず、この分野における最新かつ体系的な調査が著しく不足しています。このギャップを埋めるために、本論文では、AI駆動型材料生成における最近の進展について包括的な概要を提供します。まず、さまざまな種類の材料を整理し、結晶性材料の複数の表現を示します。次に、現在のAI駆動型材料生成アプローチの詳細な要約と分類を提供します。さらに、一般的な評価指標について議論し、オープンソースコードとベンチマークデータセットをまとめます。最後に、この急速に成長している分野における将来の方向性と課題について結論を述べます。関連資料は、https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation で見つけることができます。

&lt;img src="https://arxiv.org/html/2505.16379v1/x1.png"/&gt;&lt;p&gt;Bin Cao, Ding Wang, Dingshuo Chen, Jeffrey Xu Yu, Jia Li, Liang Wang, Qiang Liu, Rui Jiao, Tong-yi Zhang, Yang Liu, Yu Rong, Zhixun Li&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><guid isPermaLink="false">2505.16379v1</guid><pubDate>Thu, 22 May 2025 08:33:21 +0000</pubDate></item></channel></rss>